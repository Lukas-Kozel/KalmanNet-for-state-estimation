{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/luky/skola/KalmanNet-main/data/data.mat\n",
      "Project root added: /home/luky/skola/KalmanNet-main\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 1\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[3.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import TAN\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [362.96917724609375, -12.126676559448242, 0.16327548027038574]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=128 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 3: Seq=300 | Batch=64 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_clean_motion'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-4,             \n",
    "        'batch_size': 128\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3ad555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_BayesianKalmanNet_Hybrid(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    warmup_iterations=0, weight_decay_=1e-5,\n",
    "    lambda_mse=100.0  # <--- NOV√ù PARAMETR: Kotva pro MSE\n",
    "):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Scheduler: Pokud se loss zasekne, sn√≠≈æ√≠me LR (pom√°h√° stabilizovat konvergenci)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    score_at_best = {\"val_nll\": 0.0, \"val_mse\": 0.0}\n",
    "    best_iter_count = 0\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Hybrid Training: Loss = NLL + {lambda_mse} * MSE\")\n",
    "    print(f\"    Logging period: {logging_period} iterations\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true (Ground Truth) !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            # --- Training ---\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                            raise ValueError(f\"NaN in x_filtered_t at sample {j}, step {t}\")\n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance (ƒçist√Ω rozptyl s√≠tƒõ)\n",
    "            # P≈ôiƒç√≠t√°me 1e-9 jen proti dƒõlen√≠ nulou, nen√≠ to \"noise floor\"\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-4 \n",
    "            \n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean()\n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET HYBRIDN√ç LOSS ---\n",
    "            \n",
    "            # A) MSE ƒå√°st (P≈ôesnost)\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            \n",
    "            # B) NLL ƒå√°st (Konzistence)\n",
    "            # 0.5 * (log(var) + (target - pred)^2 / var)\n",
    "            cov_diag_clamped = torch.clamp(cov_diag_sequence, min=1e-4, max=1e6)\n",
    "            error_sq = (x_hat_sequence - target_sequence) ** 2\n",
    "            nll_term = 0.5 * (torch.log(cov_diag_clamped) + error_sq / cov_diag_clamped)\n",
    "            nll_loss = nll_term.mean()\n",
    "            mean_var = cov_diag_sequence.mean()\n",
    "            var_penalty = torch.relu(mean_var - 100.0) * 0.01\n",
    "            \n",
    "            # C) Celkov√° Loss (Hybrid)\n",
    "            # Zde je ta magie: I kdy≈æ NLL chce ut√©ct s varianc√≠, lambda_mse * mse ho dr≈æ√≠ zp√°tky\n",
    "            weighted_mse = lambda_mse * mse_loss\n",
    "            loss = nll_loss + weighted_mse + regularization_loss * 1.0 + var_penalty\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # --- DIAGNOSTIC LOGGING (Gradients) ---\n",
    "            # Zaznamen√°me statistiky gradient≈Ø p≈ôed o≈ô√≠znut√≠m (clippingem)\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                total_norm = 0.0\n",
    "                max_grad = 0.0\n",
    "                min_grad = float('inf')\n",
    "                nan_grad_detected = False\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2).item()\n",
    "                        total_norm += param_norm ** 2\n",
    "                        p_max = p.grad.data.abs().max().item()\n",
    "                        p_min = p.grad.data.abs().min().item()\n",
    "                        if p_max > max_grad: max_grad = p_max\n",
    "                        if p_min < min_grad: min_grad = p_min\n",
    "                        if torch.isnan(p.grad).any():\n",
    "                            nan_grad_detected = True\n",
    "                total_norm = total_norm ** 0.5\n",
    "                \n",
    "                if nan_grad_detected:\n",
    "                     print(f\"!!! WARNING: NaN gradient detected at iter {train_iter_count} !!!\")\n",
    "\n",
    "            if clip_grad > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- Logging ---\n",
    "            diff = x_hat_sequence - target_sequence\n",
    "            mean_error = diff.abs().mean().item()\n",
    "            min_variance = cov_diag_sequence.min().item()\n",
    "            max_variance = cov_diag_sequence.max().item()\n",
    "            mean_variance = cov_diag_sequence.mean().item()\n",
    "\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Zjist√≠me dropout pravdƒõpodobnosti (jen pro info)\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                \n",
    "                print(f\"--- Iter [{train_iter_count}/{total_train_iter}] ---\")\n",
    "                print(f\"    Total Loss:     {loss.item():.4f}\")\n",
    "                print(f\"    MSE (Raw):      {mse_loss.item():.6f}\")\n",
    "                print(f\"    MSE (Weighted): {weighted_mse.item():.4f} (lambda={lambda_mse})\")\n",
    "                print(f\"    NLL Component:  {nll_loss.item():.4f}\")\n",
    "                print(f\"    Reg Loss:       {regularization_loss.item():.6f}\")\n",
    "                print(f\"    Variance stats: Min={min_variance:.2e}, Max={max_variance:.2e}, Mean={mean_variance:.2e}\")\n",
    "                print(f\"    Mean Error L1:  {mean_error:.4f}\")\n",
    "                print(f\"    Grad Norm:      {total_norm:.4f} (Max abs grad: {max_grad:.4f})\")\n",
    "                print(f\"    Dropout probs:  p1={p1:.4f}, p2={p2:.4f}\")\n",
    "                \n",
    "                # Check pro \"Variance collapse\"\n",
    "                if mean_variance < 1e-8:\n",
    "                    print(\"    !!! WARNING: Variance is extremely low (Collapse risk) !!!\")\n",
    "\n",
    "            # --- Validation step ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                # Step scheduleru podle tr√©novac√≠ loss (nebo validace, pokud bys to p≈ôedƒõlal)\n",
    "                scheduler.step(loss)\n",
    "                \n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                all_val_x_true_cpu, all_val_x_hat_cpu, all_val_P_hat_cpu = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val_batch, y_meas_val_batch in val_loader:\n",
    "                        val_batch_size, val_seq_len, _ = x_true_val_batch.shape\n",
    "                        x_true_val_batch = x_true_val_batch.to(device)\n",
    "                        y_meas_val_batch = y_meas_val_batch.to(device)\n",
    "                        val_ensemble_trajectories = []\n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=val_batch_size, initial_state=x_true_val_batch[:, 0, :])\n",
    "                            val_current_x_hats = []\n",
    "                            for t in range(1, val_seq_len):\n",
    "                                y_t_val = y_meas_val_batch[:, t, :]\n",
    "                                x_filtered_t, _ = model.step(y_t_val)\n",
    "                                val_current_x_hats.append(x_filtered_t)\n",
    "                            val_ensemble_trajectories.append(torch.stack(val_current_x_hats, dim=1))\n",
    "                        \n",
    "                        # Agregace validace\n",
    "                        val_ensemble = torch.stack(val_ensemble_trajectories, dim=0)\n",
    "                        val_preds_seq = val_ensemble.mean(dim=0)\n",
    "                        \n",
    "                        val_target_seq = x_true_val_batch[:, 1:, :]\n",
    "                        val_mse_list.append(F.mse_loss(val_preds_seq, val_target_seq).item())\n",
    "                        \n",
    "                        # P≈ô√≠prava pro ANEES\n",
    "                        initial_state_val = x_true_val_batch[:, 0, :].unsqueeze(1)\n",
    "                        full_x_hat = torch.cat([initial_state_val, val_preds_seq], dim=1)\n",
    "                        \n",
    "                        # Epistemick√° variance\n",
    "                        val_covs_diag = val_ensemble.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        # Vytvo≈ôen√≠ diagon√°ln√≠ch matic P\n",
    "                        # (Zjednodu≈°en√° konstrukce pro ANEES calc)\n",
    "                        # Pro p≈ôesn√© ANEES bychom mƒõli dƒõlat outer product, \n",
    "                        # ale diagon√°la z var() je dobr√° aproximace pro BKN\n",
    "                        val_covs_full = torch.zeros(val_batch_size, val_seq_len-1, 4, 4, device=device)\n",
    "                        for b in range(val_batch_size):\n",
    "                            for t in range(val_seq_len-1):\n",
    "                                val_covs_full[b, t] = torch.diag(val_covs_diag[b, t])\n",
    "\n",
    "                        P0 = model.system_model.P0.unsqueeze(0).repeat(val_batch_size, 1, 1).unsqueeze(1)\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true_cpu.append(x_true_val_batch.cpu())\n",
    "                        all_val_x_hat_cpu.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat_cpu.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                final_x_true_list = torch.cat(all_val_x_true_cpu, dim=0)\n",
    "                final_x_hat_list = torch.cat(all_val_x_hat_cpu, dim=0)\n",
    "                final_P_hat_list = torch.cat(all_val_P_hat_cpu, dim=0)\n",
    "                \n",
    "                # V√Ωpoƒçet ANEES\n",
    "                try:\n",
    "                    avg_val_anees = trainer.calculate_anees_vectorized(final_x_true_list, final_x_hat_list, final_P_hat_list)\n",
    "                except Exception as e:\n",
    "                    print(f\"  !!! Error calculating ANEES: {e}\")\n",
    "                    avg_val_anees = float('nan')\n",
    "                \n",
    "                print(f\"  Average MSE: {avg_val_mse:.4f}, Average ANEES: {avg_val_anees:.4f}\")\n",
    "                \n",
    "                # Ukl√°d√°n√≠ modelu:\n",
    "                if not np.isnan(avg_val_anees) and avg_val_anees < best_val_anees and avg_val_anees > 0:\n",
    "                    print(f\"  >>> New best VALIDATION ANEES! Saving model. (Old: {best_val_anees:.4f} -> New: {avg_val_anees:.4f}) <<<\")\n",
    "                    best_val_anees = avg_val_anees\n",
    "                    best_iter_count = train_iter_count\n",
    "                    score_at_best['val_mse'] = avg_val_mse\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                print(\"-\" * 50)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count} with ANEES {best_val_anees:.4f}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(\"No best model was saved; returning last state.\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_anees\": best_val_anees,\n",
    "        \"best_val_nll\": score_at_best['val_nll'],\n",
    "        \"best_val_mse\": score_at_best['val_mse'],\n",
    "        \"best_iter\": best_iter_count,\n",
    "        \"final_model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0fe051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_BayesianKalmanNet_TBPTT(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad=1.0,\n",
    "    J_samples=5, tbptt_steps=20, # D√©lka okna (krok≈Ø), po kter√Ωch dƒõl√°me update\n",
    "    validation_period=50, logging_period=10,\n",
    "    weight_decay_=1e-5, lambda_mse=1000.0\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=100, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START TBPTT Training (Window={tbptt_steps}): Loss = NLL + {lambda_mse} * MSE\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            batch_size, seq_len, dim_x = x_true_batch.shape\n",
    "            \n",
    "            # === VEKTORIZOVAN√ù ENSEMBLE ===\n",
    "            # M√≠sto cyklu p≈ôes J udƒõl√°me \"Super Batch\": [Batch * J, Seq, Dim]\n",
    "            # T√≠m p√°dem model zpracuje v≈°echny vzorky nar√°z paralelnƒõ.\n",
    "            x_true_super = x_true_batch.repeat_interleave(J_samples, dim=0)\n",
    "            y_meas_super = y_meas_batch.repeat_interleave(J_samples, dim=0)\n",
    "            super_batch_size = x_true_super.shape[0]\n",
    "\n",
    "            # 1. Reset na zaƒç√°tku sekvence (t=0)\n",
    "            # Inicializujeme pro v≈°echny vzorky najednou\n",
    "            model.reset(batch_size=super_batch_size, initial_state=x_true_super[:, 0, :])\n",
    "\n",
    "            # 2. TBPTT Smyƒçka p≈ôes okna\n",
    "            # Kr√°j√≠me sekvenci na kousky d√©lky 'tbptt_steps'\n",
    "            for t_start in range(1, seq_len, tbptt_steps):\n",
    "                t_end = min(t_start + tbptt_steps, seq_len)\n",
    "                current_window_len = t_end - t_start\n",
    "                if current_window_len <= 0: continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Ulo≈æi≈°tƒõ pro predikce v oknƒõ\n",
    "                window_x_preds = []\n",
    "                window_regs = []\n",
    "\n",
    "                # A) Forward pass oknem\n",
    "                for t in range(t_start, t_end):\n",
    "                    y_t = y_meas_super[:, t, :] # [Batch*J, 3]\n",
    "                    x_est, reg = model.step(y_t)\n",
    "                    \n",
    "                    window_x_preds.append(x_est)\n",
    "                    window_regs.append(reg)\n",
    "                \n",
    "                # B) Zpracov√°n√≠ v√Ωsledk≈Ø okna\n",
    "                # Stackneme ƒças: [Batch*J, Window, 4]\n",
    "                preds_super = torch.stack(window_x_preds, dim=1)\n",
    "                regs_super = torch.stack(window_regs) # [Window, Batch*J]\n",
    "                \n",
    "                # Rozbal√≠me zpƒõt na [Batch, J, Window, 4] pro statistiku\n",
    "                preds_reshaped = preds_super.view(batch_size, J_samples, current_window_len, dim_x)\n",
    "                \n",
    "                # Statistiky Ensemble (p≈ôes dimenzi J=1)\n",
    "                x_hat_seq = preds_reshaped.mean(dim=1) # [Batch, Window, 4]\n",
    "                cov_diag_seq = preds_reshaped.var(dim=1) + 1e-4 # [Batch, Window, 4]\n",
    "                \n",
    "                # Ground Truth pro toto okno\n",
    "                target_seq = x_true_batch[:, t_start:t_end, :]\n",
    "\n",
    "                # C) V√Ωpoƒçet Loss (pouze pro toto okno!)\n",
    "                mse_loss = F.mse_loss(x_hat_seq, target_seq)\n",
    "                \n",
    "                cov_clamped = torch.clamp(cov_diag_seq, min=1e-4, max=1e6)\n",
    "                error_sq = (x_hat_seq - target_seq) ** 2\n",
    "                nll_term = 0.5 * (torch.log(cov_clamped) + error_sq / cov_clamped)\n",
    "                nll_loss = nll_term.mean()\n",
    "                \n",
    "                # Normalizovan√° regularizace na okno\n",
    "                reg_loss = regs_super.mean() / current_window_len \n",
    "                \n",
    "                # Variance penalty (voliteln√©, proti alibismu)\n",
    "                var_penalty = torch.relu(cov_diag_seq.mean() - 100.0) * 0.01\n",
    "\n",
    "                loss = nll_loss + (lambda_mse * mse_loss) + reg_loss + var_penalty\n",
    "                \n",
    "                # D) Backward & Update\n",
    "                loss.backward()\n",
    "                if clip_grad > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # E) !!! DETACH HIDDEN STATE !!!\n",
    "                # Toto je podstata TBPTT. Utneme gradienty, ale nech√°me hodnoty stav≈Ø\n",
    "                # pro dal≈°√≠ okno.\n",
    "                model.detach_hidden()\n",
    "                \n",
    "                train_iter_count += 1 # Poƒç√≠t√°me ka≈æd√Ω update (okno) jako iteraci\n",
    "\n",
    "                # --- Logging (uvnit≈ô okna, aby to bylo ƒçast√©) ---\n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    print(f\"Iter {train_iter_count} (TBPTT): Loss {loss.item():.4f} | MSE {mse_loss.item():.2f} | VarMean {cov_diag_seq.mean().item():.1f}\")\n",
    "\n",
    "            # --- Validation step (Na konci batchi, ne okna) ---\n",
    "            if train_iter_count % validation_period == 0:  # Zjednodu≈°en√° podm√≠nka\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                # Pro validaci mus√≠me bohu≈æel iterovat po staru nebo taky vektorizovanƒõ\n",
    "                # Zde zjednodu≈°enƒõ celou sekvenci nar√°z (bez TBPTT, jen inference)\n",
    "                with torch.no_grad():\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                        b_val, s_val, _ = x_val.shape\n",
    "                        \n",
    "                        # Vektorizovan√Ω validace\n",
    "                        x_val_super = x_val.repeat_interleave(J_samples, dim=0)\n",
    "                        y_val_super = y_val.repeat_interleave(J_samples, dim=0)\n",
    "                        \n",
    "                        model.reset(batch_size=b_val*J_samples, initial_state=x_val_super[:,0,:])\n",
    "                        preds = []\n",
    "                        for t in range(1, s_val):\n",
    "                            est, _ = model.step(y_val_super[:, t, :])\n",
    "                            preds.append(est)\n",
    "                        \n",
    "                        preds_stack = torch.stack(preds, dim=1).view(b_val, J_samples, s_val-1, 4)\n",
    "                        mean_preds = preds_stack.mean(dim=1)\n",
    "                        val_mse_list.append(F.mse_loss(mean_preds, x_val[:, 1:, :]).item())\n",
    "                \n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                print(f\"\\n--- VALIDATION: MSE {avg_val_mse:.4f} ---\")\n",
    "                \n",
    "                if avg_val_mse < 200000 and avg_val_mse > 0: # Simple guard\n",
    "                     best_model_state = deepcopy(model.state_dict())\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "    print(\"TBPTT Training completed.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE BKN MODELU ===\n",
      "INFO: Aplikuji upravenou inicializaci pro BKN.\n",
      "DEBUG: V√Ωstupn√≠ vrstva inicializov√°na konzervativnƒõ (interval -0.1 a≈æ 0.1).\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 1: SeqLen 10 | LR 0.001 | TBPTT: False\n",
      "============================================================\n",
      "   -> Using Standard Hybrid Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luky/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ START Hybrid Training: Loss = NLL + 1000.0 * MSE\n",
      "    Logging period: 10 iterations\n",
      "--- Iter [10/400] ---\n",
      "    Total Loss:     79713.0469\n",
      "    MSE (Raw):      79.708778\n",
      "    MSE (Weighted): 79708.7812 (lambda=1000.0)\n",
      "    NLL Component:  4.2493\n",
      "    Reg Loss:       0.014900\n",
      "    Variance stats: Min=1.00e-04, Max=4.43e+03, Mean=2.40e+01\n",
      "    Mean Error L1:  5.1866\n",
      "    Grad Norm:      1396394.8080 (Max abs grad: 360432.9062)\n",
      "    Dropout probs:  p1=0.1565, p2=0.2235\n",
      "--- Iter [20/400] ---\n",
      "    Total Loss:     27753.3789\n",
      "    MSE (Raw):      27.749788\n",
      "    MSE (Weighted): 27749.7891 (lambda=1000.0)\n",
      "    NLL Component:  3.5739\n",
      "    Reg Loss:       0.014878\n",
      "    Variance stats: Min=1.00e-04, Max=3.85e+03, Mean=1.68e+01\n",
      "    Mean Error L1:  3.4901\n",
      "    Grad Norm:      59908.8808 (Max abs grad: 13878.5283)\n",
      "    Dropout probs:  p1=0.1557, p2=0.2222\n",
      "\n",
      "--- Validation at iteration 20 ---\n",
      "  Average MSE: 24.0134, Average ANEES: 16.4585\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: inf -> New: 16.4585) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [30/400] ---\n",
      "    Total Loss:     19833.6309\n",
      "    MSE (Raw):      19.830732\n",
      "    MSE (Weighted): 19830.7324 (lambda=1000.0)\n",
      "    NLL Component:  2.8821\n",
      "    Reg Loss:       0.014857\n",
      "    Variance stats: Min=1.57e-02, Max=5.43e+02, Mean=1.11e+01\n",
      "    Mean Error L1:  3.0505\n",
      "    Grad Norm:      19889.1270 (Max abs grad: 4396.2466)\n",
      "    Dropout probs:  p1=0.1552, p2=0.2205\n",
      "--- Iter [40/400] ---\n",
      "    Total Loss:     26382.3008\n",
      "    MSE (Raw):      26.379425\n",
      "    MSE (Weighted): 26379.4258 (lambda=1000.0)\n",
      "    NLL Component:  2.8595\n",
      "    Reg Loss:       0.014825\n",
      "    Variance stats: Min=1.00e-04, Max=6.28e+03, Mean=1.60e+01\n",
      "    Mean Error L1:  3.1687\n",
      "    Grad Norm:      11419.2547 (Max abs grad: 2735.8298)\n",
      "    Dropout probs:  p1=0.1540, p2=0.2187\n",
      "\n",
      "--- Validation at iteration 40 ---\n",
      "  Average MSE: 18.7483, Average ANEES: 13.7011\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 16.4585 -> New: 13.7011) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [50/400] ---\n",
      "    Total Loss:     20592.2227\n",
      "    MSE (Raw):      20.589748\n",
      "    MSE (Weighted): 20589.7480 (lambda=1000.0)\n",
      "    NLL Component:  2.4591\n",
      "    Reg Loss:       0.014795\n",
      "    Variance stats: Min=1.66e-02, Max=7.53e+03, Mean=1.56e+01\n",
      "    Mean Error L1:  2.9016\n",
      "    Grad Norm:      10520.5010 (Max abs grad: 2032.0535)\n",
      "    Dropout probs:  p1=0.1526, p2=0.2170\n",
      "--- Iter [60/400] ---\n",
      "    Total Loss:     19781.0000\n",
      "    MSE (Raw):      19.778442\n",
      "    MSE (Weighted): 19778.4414 (lambda=1000.0)\n",
      "    NLL Component:  2.5429\n",
      "    Reg Loss:       0.014767\n",
      "    Variance stats: Min=1.00e-04, Max=3.17e+03, Mean=1.42e+01\n",
      "    Mean Error L1:  2.9941\n",
      "    Grad Norm:      21077.0889 (Max abs grad: 4926.9458)\n",
      "    Dropout probs:  p1=0.1512, p2=0.2154\n",
      "\n",
      "--- Validation at iteration 60 ---\n",
      "  Average MSE: 17.9352, Average ANEES: 14.5064\n",
      "--------------------------------------------------\n",
      "--- Iter [70/400] ---\n",
      "    Total Loss:     23838.1250\n",
      "    MSE (Raw):      23.835470\n",
      "    MSE (Weighted): 23835.4707 (lambda=1000.0)\n",
      "    NLL Component:  2.6396\n",
      "    Reg Loss:       0.014742\n",
      "    Variance stats: Min=8.78e-03, Max=8.37e+03, Mean=1.64e+01\n",
      "    Mean Error L1:  3.0553\n",
      "    Grad Norm:      15732.8187 (Max abs grad: 4022.0298)\n",
      "    Dropout probs:  p1=0.1498, p2=0.2136\n",
      "--- Iter [80/400] ---\n",
      "    Total Loss:     18704.4961\n",
      "    MSE (Raw):      18.701904\n",
      "    MSE (Weighted): 18701.9043 (lambda=1000.0)\n",
      "    NLL Component:  2.5768\n",
      "    Reg Loss:       0.014715\n",
      "    Variance stats: Min=1.00e-04, Max=8.15e+03, Mean=1.66e+01\n",
      "    Mean Error L1:  2.7860\n",
      "    Grad Norm:      7483.6755 (Max abs grad: 1175.8981)\n",
      "    Dropout probs:  p1=0.1483, p2=0.2118\n",
      "\n",
      "--- Validation at iteration 80 ---\n",
      "  Average MSE: 15.9474, Average ANEES: 14.2290\n",
      "--------------------------------------------------\n",
      "--- Iter [90/400] ---\n",
      "    Total Loss:     18823.8047\n",
      "    MSE (Raw):      18.820934\n",
      "    MSE (Weighted): 18820.9336 (lambda=1000.0)\n",
      "    NLL Component:  2.8551\n",
      "    Reg Loss:       0.014693\n",
      "    Variance stats: Min=1.00e-04, Max=4.57e+03, Mean=1.58e+01\n",
      "    Mean Error L1:  2.9438\n",
      "    Grad Norm:      7256.7357 (Max abs grad: 1788.8981)\n",
      "    Dropout probs:  p1=0.1469, p2=0.2098\n",
      "--- Iter [100/400] ---\n",
      "    Total Loss:     15391.8633\n",
      "    MSE (Raw):      15.388890\n",
      "    MSE (Weighted): 15388.8906 (lambda=1000.0)\n",
      "    NLL Component:  2.9577\n",
      "    Reg Loss:       0.014669\n",
      "    Variance stats: Min=1.00e-04, Max=8.51e+03, Mean=1.43e+01\n",
      "    Mean Error L1:  2.6062\n",
      "    Grad Norm:      13245.6938 (Max abs grad: 3243.8518)\n",
      "    Dropout probs:  p1=0.1456, p2=0.2080\n",
      "\n",
      "--- Validation at iteration 100 ---\n",
      "  Average MSE: 17.6387, Average ANEES: 14.4911\n",
      "--------------------------------------------------\n",
      "--- Iter [110/400] ---\n",
      "    Total Loss:     16852.1172\n",
      "    MSE (Raw):      16.849026\n",
      "    MSE (Weighted): 16849.0254 (lambda=1000.0)\n",
      "    NLL Component:  3.0772\n",
      "    Reg Loss:       0.014646\n",
      "    Variance stats: Min=1.00e-04, Max=7.49e+03, Mean=1.34e+01\n",
      "    Mean Error L1:  2.6916\n",
      "    Grad Norm:      9575.8419 (Max abs grad: 2570.5674)\n",
      "    Dropout probs:  p1=0.1441, p2=0.2064\n",
      "--- Iter [120/400] ---\n",
      "    Total Loss:     14839.5820\n",
      "    MSE (Raw):      14.836443\n",
      "    MSE (Weighted): 14836.4434 (lambda=1000.0)\n",
      "    NLL Component:  3.1238\n",
      "    Reg Loss:       0.014624\n",
      "    Variance stats: Min=1.00e-04, Max=4.97e+02, Mean=1.11e+01\n",
      "    Mean Error L1:  2.6258\n",
      "    Grad Norm:      8314.5807 (Max abs grad: 2571.5874)\n",
      "    Dropout probs:  p1=0.1428, p2=0.2051\n",
      "\n",
      "--- Validation at iteration 120 ---\n",
      "  Average MSE: 17.8620, Average ANEES: 15.2440\n",
      "--------------------------------------------------\n",
      "--- Iter [130/400] ---\n",
      "    Total Loss:     14171.1670\n",
      "    MSE (Raw):      14.168306\n",
      "    MSE (Weighted): 14168.3066 (lambda=1000.0)\n",
      "    NLL Component:  2.8462\n",
      "    Reg Loss:       0.014604\n",
      "    Variance stats: Min=1.00e-04, Max=2.72e+02, Mean=1.08e+01\n",
      "    Mean Error L1:  2.6095\n",
      "    Grad Norm:      11842.4401 (Max abs grad: 2710.4863)\n",
      "    Dropout probs:  p1=0.1415, p2=0.2040\n",
      "--- Iter [140/400] ---\n",
      "    Total Loss:     13399.0098\n",
      "    MSE (Raw):      13.396346\n",
      "    MSE (Weighted): 13396.3457 (lambda=1000.0)\n",
      "    NLL Component:  2.6492\n",
      "    Reg Loss:       0.014580\n",
      "    Variance stats: Min=1.00e-04, Max=5.08e+02, Mean=1.12e+01\n",
      "    Mean Error L1:  2.5554\n",
      "    Grad Norm:      9350.1887 (Max abs grad: 1827.2139)\n",
      "    Dropout probs:  p1=0.1401, p2=0.2028\n",
      "\n",
      "--- Validation at iteration 140 ---\n",
      "  Average MSE: 16.3643, Average ANEES: 14.7674\n",
      "--------------------------------------------------\n",
      "--- Iter [150/400] ---\n",
      "    Total Loss:     14493.8076\n",
      "    MSE (Raw):      14.491289\n",
      "    MSE (Weighted): 14491.2891 (lambda=1000.0)\n",
      "    NLL Component:  2.5037\n",
      "    Reg Loss:       0.014561\n",
      "    Variance stats: Min=1.00e-04, Max=2.26e+02, Mean=1.10e+01\n",
      "    Mean Error L1:  2.6374\n",
      "    Grad Norm:      11943.4115 (Max abs grad: 3809.6550)\n",
      "    Dropout probs:  p1=0.1389, p2=0.2013\n",
      "--- Iter [160/400] ---\n",
      "    Total Loss:     13173.3936\n",
      "    MSE (Raw):      13.170592\n",
      "    MSE (Weighted): 13170.5928 (lambda=1000.0)\n",
      "    NLL Component:  2.7861\n",
      "    Reg Loss:       0.014541\n",
      "    Variance stats: Min=1.00e-04, Max=3.14e+03, Mean=1.13e+01\n",
      "    Mean Error L1:  2.5250\n",
      "    Grad Norm:      7359.5550 (Max abs grad: 1929.8505)\n",
      "    Dropout probs:  p1=0.1375, p2=0.1998\n",
      "\n",
      "--- Validation at iteration 160 ---\n",
      "  Average MSE: 15.1331, Average ANEES: 14.4142\n",
      "--------------------------------------------------\n",
      "--- Iter [170/400] ---\n",
      "    Total Loss:     14401.9756\n",
      "    MSE (Raw):      14.399144\n",
      "    MSE (Weighted): 14399.1445 (lambda=1000.0)\n",
      "    NLL Component:  2.8161\n",
      "    Reg Loss:       0.014521\n",
      "    Variance stats: Min=1.00e-04, Max=2.75e+02, Mean=1.16e+01\n",
      "    Mean Error L1:  2.5725\n",
      "    Grad Norm:      9621.1996 (Max abs grad: 2886.5398)\n",
      "    Dropout probs:  p1=0.1363, p2=0.1982\n",
      "--- Iter [180/400] ---\n",
      "    Total Loss:     13402.3896\n",
      "    MSE (Raw):      13.399200\n",
      "    MSE (Weighted): 13399.2002 (lambda=1000.0)\n",
      "    NLL Component:  3.1753\n",
      "    Reg Loss:       0.014499\n",
      "    Variance stats: Min=1.00e-04, Max=1.81e+02, Mean=1.05e+01\n",
      "    Mean Error L1:  2.5215\n",
      "    Grad Norm:      10315.2938 (Max abs grad: 3464.5579)\n",
      "    Dropout probs:  p1=0.1348, p2=0.1967\n",
      "\n",
      "--- Validation at iteration 180 ---\n",
      "  Average MSE: 18.4049, Average ANEES: 15.1933\n",
      "--------------------------------------------------\n",
      "--- Iter [190/400] ---\n",
      "    Total Loss:     13959.3848\n",
      "    MSE (Raw):      13.956774\n",
      "    MSE (Weighted): 13956.7734 (lambda=1000.0)\n",
      "    NLL Component:  2.5968\n",
      "    Reg Loss:       0.014479\n",
      "    Variance stats: Min=1.00e-04, Max=5.54e+03, Mean=1.26e+01\n",
      "    Mean Error L1:  2.5499\n",
      "    Grad Norm:      8289.1628 (Max abs grad: 2990.6970)\n",
      "    Dropout probs:  p1=0.1335, p2=0.1953\n",
      "--- Iter [200/400] ---\n",
      "    Total Loss:     13426.1582\n",
      "    MSE (Raw):      13.423140\n",
      "    MSE (Weighted): 13423.1396 (lambda=1000.0)\n",
      "    NLL Component:  3.0040\n",
      "    Reg Loss:       0.014464\n",
      "    Variance stats: Min=1.00e-04, Max=2.99e+02, Mean=1.03e+01\n",
      "    Mean Error L1:  2.5630\n",
      "    Grad Norm:      21274.6695 (Max abs grad: 7564.5400)\n",
      "    Dropout probs:  p1=0.1323, p2=0.1940\n",
      "\n",
      "--- Validation at iteration 200 ---\n",
      "  Average MSE: 14.2444, Average ANEES: 16.6979\n",
      "--------------------------------------------------\n",
      "--- Iter [210/400] ---\n",
      "    Total Loss:     12927.4150\n",
      "    MSE (Raw):      12.924703\n",
      "    MSE (Weighted): 12924.7031 (lambda=1000.0)\n",
      "    NLL Component:  2.6973\n",
      "    Reg Loss:       0.014449\n",
      "    Variance stats: Min=1.00e-04, Max=6.95e+03, Mean=1.47e+01\n",
      "    Mean Error L1:  2.4598\n",
      "    Grad Norm:      9598.1722 (Max abs grad: 2659.1123)\n",
      "    Dropout probs:  p1=0.1312, p2=0.1929\n",
      "--- Iter [220/400] ---\n",
      "    Total Loss:     13206.1592\n",
      "    MSE (Raw):      13.203495\n",
      "    MSE (Weighted): 13203.4951 (lambda=1000.0)\n",
      "    NLL Component:  2.6496\n",
      "    Reg Loss:       0.014430\n",
      "    Variance stats: Min=1.00e-04, Max=2.76e+02, Mean=9.83e+00\n",
      "    Mean Error L1:  2.5126\n",
      "    Grad Norm:      7417.4867 (Max abs grad: 1818.8846)\n",
      "    Dropout probs:  p1=0.1300, p2=0.1917\n",
      "\n",
      "--- Validation at iteration 220 ---\n",
      "  Average MSE: 14.5767, Average ANEES: 15.5916\n",
      "--------------------------------------------------\n",
      "--- Iter [230/400] ---\n",
      "    Total Loss:     12300.8691\n",
      "    MSE (Raw):      12.298008\n",
      "    MSE (Weighted): 12298.0078 (lambda=1000.0)\n",
      "    NLL Component:  2.8466\n",
      "    Reg Loss:       0.014414\n",
      "    Variance stats: Min=1.00e-04, Max=7.74e+03, Mean=1.37e+01\n",
      "    Mean Error L1:  2.4271\n",
      "    Grad Norm:      6357.0282 (Max abs grad: 1205.8590)\n",
      "    Dropout probs:  p1=0.1288, p2=0.1903\n",
      "--- Iter [240/400] ---\n",
      "    Total Loss:     12677.4727\n",
      "    MSE (Raw):      12.673985\n",
      "    MSE (Weighted): 12673.9844 (lambda=1000.0)\n",
      "    NLL Component:  3.4736\n",
      "    Reg Loss:       0.014399\n",
      "    Variance stats: Min=1.00e-04, Max=1.70e+02, Mean=9.86e+00\n",
      "    Mean Error L1:  2.4530\n",
      "    Grad Norm:      8154.7967 (Max abs grad: 2479.8403)\n",
      "    Dropout probs:  p1=0.1278, p2=0.1888\n",
      "\n",
      "--- Validation at iteration 240 ---\n",
      "  Average MSE: 17.8291, Average ANEES: 16.2719\n",
      "--------------------------------------------------\n",
      "--- Iter [250/400] ---\n",
      "    Total Loss:     14128.6182\n",
      "    MSE (Raw):      14.125781\n",
      "    MSE (Weighted): 14125.7812 (lambda=1000.0)\n",
      "    NLL Component:  2.8224\n",
      "    Reg Loss:       0.014382\n",
      "    Variance stats: Min=1.00e-04, Max=4.70e+03, Mean=1.25e+01\n",
      "    Mean Error L1:  2.5141\n",
      "    Grad Norm:      6145.9268 (Max abs grad: 1279.6864)\n",
      "    Dropout probs:  p1=0.1265, p2=0.1875\n",
      "--- Iter [260/400] ---\n",
      "    Total Loss:     13118.4570\n",
      "    MSE (Raw):      13.115458\n",
      "    MSE (Weighted): 13115.4580 (lambda=1000.0)\n",
      "    NLL Component:  2.9846\n",
      "    Reg Loss:       0.014364\n",
      "    Variance stats: Min=1.00e-04, Max=2.00e+02, Mean=1.08e+01\n",
      "    Mean Error L1:  2.4807\n",
      "    Grad Norm:      7667.4874 (Max abs grad: 2774.6650)\n",
      "    Dropout probs:  p1=0.1253, p2=0.1861\n",
      "\n",
      "--- Validation at iteration 260 ---\n",
      "  Average MSE: 14.7206, Average ANEES: 16.5819\n",
      "--------------------------------------------------\n",
      "--- Iter [270/400] ---\n",
      "    Total Loss:     11475.6094\n",
      "    MSE (Raw):      11.472740\n",
      "    MSE (Weighted): 11472.7402 (lambda=1000.0)\n",
      "    NLL Component:  2.8544\n",
      "    Reg Loss:       0.014346\n",
      "    Variance stats: Min=1.00e-04, Max=2.87e+02, Mean=1.01e+01\n",
      "    Mean Error L1:  2.3590\n",
      "    Grad Norm:      9789.4558 (Max abs grad: 2175.6082)\n",
      "    Dropout probs:  p1=0.1241, p2=0.1847\n",
      "--- Iter [280/400] ---\n",
      "    Total Loss:     12518.1875\n",
      "    MSE (Raw):      12.515021\n",
      "    MSE (Weighted): 12515.0215 (lambda=1000.0)\n",
      "    NLL Component:  3.1514\n",
      "    Reg Loss:       0.014331\n",
      "    Variance stats: Min=1.00e-04, Max=3.89e+03, Mean=1.22e+01\n",
      "    Mean Error L1:  2.4295\n",
      "    Grad Norm:      6376.0583 (Max abs grad: 1595.1523)\n",
      "    Dropout probs:  p1=0.1230, p2=0.1832\n",
      "\n",
      "--- Validation at iteration 280 ---\n",
      "  Average MSE: 14.6995, Average ANEES: 18.2219\n",
      "--------------------------------------------------\n",
      "--- Iter [290/400] ---\n",
      "    Total Loss:     12115.5820\n",
      "    MSE (Raw):      12.112439\n",
      "    MSE (Weighted): 12112.4395 (lambda=1000.0)\n",
      "    NLL Component:  3.1276\n",
      "    Reg Loss:       0.014318\n",
      "    Variance stats: Min=1.00e-04, Max=2.22e+03, Mean=1.11e+01\n",
      "    Mean Error L1:  2.4285\n",
      "    Grad Norm:      6673.1153 (Max abs grad: 1353.1552)\n",
      "    Dropout probs:  p1=0.1219, p2=0.1821\n",
      "--- Iter [300/400] ---\n",
      "    Total Loss:     13181.0635\n",
      "    MSE (Raw):      13.177917\n",
      "    MSE (Weighted): 13177.9180 (lambda=1000.0)\n",
      "    NLL Component:  3.1304\n",
      "    Reg Loss:       0.014302\n",
      "    Variance stats: Min=1.00e-04, Max=1.77e+02, Mean=9.15e+00\n",
      "    Mean Error L1:  2.5179\n",
      "    Grad Norm:      6851.3938 (Max abs grad: 1624.6389)\n",
      "    Dropout probs:  p1=0.1209, p2=0.1809\n",
      "\n",
      "--- Validation at iteration 300 ---\n",
      "  Average MSE: 17.7519, Average ANEES: 19.2910\n",
      "--------------------------------------------------\n",
      "--- Iter [310/400] ---\n",
      "    Total Loss:     11393.7383\n",
      "    MSE (Raw):      11.390851\n",
      "    MSE (Weighted): 11390.8506 (lambda=1000.0)\n",
      "    NLL Component:  2.8727\n",
      "    Reg Loss:       0.014284\n",
      "    Variance stats: Min=1.00e-04, Max=4.83e+02, Mean=9.54e+00\n",
      "    Mean Error L1:  2.3712\n",
      "    Grad Norm:      7022.4932 (Max abs grad: 1760.2786)\n",
      "    Dropout probs:  p1=0.1198, p2=0.1795\n",
      "--- Iter [320/400] ---\n",
      "    Total Loss:     12133.2148\n",
      "    MSE (Raw):      12.129329\n",
      "    MSE (Weighted): 12129.3291 (lambda=1000.0)\n",
      "    NLL Component:  3.8709\n",
      "    Reg Loss:       0.014265\n",
      "    Variance stats: Min=1.00e-04, Max=2.41e+02, Mean=9.47e+00\n",
      "    Mean Error L1:  2.4081\n",
      "    Grad Norm:      13810.4664 (Max abs grad: 4663.8955)\n",
      "    Dropout probs:  p1=0.1187, p2=0.1782\n",
      "\n",
      "--- Validation at iteration 320 ---\n",
      "  Average MSE: 14.9253, Average ANEES: 18.5216\n",
      "--------------------------------------------------\n",
      "--- Iter [330/400] ---\n",
      "    Total Loss:     11191.1914\n",
      "    MSE (Raw):      11.187818\n",
      "    MSE (Weighted): 11187.8174 (lambda=1000.0)\n",
      "    NLL Component:  3.3590\n",
      "    Reg Loss:       0.014252\n",
      "    Variance stats: Min=1.00e-04, Max=2.25e+02, Mean=9.10e+00\n",
      "    Mean Error L1:  2.3266\n",
      "    Grad Norm:      7705.8374 (Max abs grad: 1598.9264)\n",
      "    Dropout probs:  p1=0.1176, p2=0.1769\n",
      "--- Iter [340/400] ---\n",
      "    Total Loss:     11097.4844\n",
      "    MSE (Raw):      11.094318\n",
      "    MSE (Weighted): 11094.3184 (lambda=1000.0)\n",
      "    NLL Component:  3.1511\n",
      "    Reg Loss:       0.014238\n",
      "    Variance stats: Min=1.00e-04, Max=2.16e+02, Mean=9.67e+00\n",
      "    Mean Error L1:  2.3467\n",
      "    Grad Norm:      5546.4389 (Max abs grad: 1536.4086)\n",
      "    Dropout probs:  p1=0.1166, p2=0.1758\n",
      "\n",
      "--- Validation at iteration 340 ---\n",
      "  Average MSE: 14.1292, Average ANEES: 20.6465\n",
      "--------------------------------------------------\n",
      "--- Iter [350/400] ---\n",
      "    Total Loss:     12083.5391\n",
      "    MSE (Raw):      12.079864\n",
      "    MSE (Weighted): 12079.8633 (lambda=1000.0)\n",
      "    NLL Component:  3.6612\n",
      "    Reg Loss:       0.014226\n",
      "    Variance stats: Min=1.00e-04, Max=8.62e+03, Mean=1.18e+01\n",
      "    Mean Error L1:  2.4126\n",
      "    Grad Norm:      12915.9462 (Max abs grad: 4814.7310)\n",
      "    Dropout probs:  p1=0.1156, p2=0.1749\n",
      "--- Iter [360/400] ---\n",
      "    Total Loss:     11153.6387\n",
      "    MSE (Raw):      11.150588\n",
      "    MSE (Weighted): 11150.5879 (lambda=1000.0)\n",
      "    NLL Component:  3.0365\n",
      "    Reg Loss:       0.014211\n",
      "    Variance stats: Min=1.00e-04, Max=1.40e+02, Mean=7.89e+00\n",
      "    Mean Error L1:  2.3070\n",
      "    Grad Norm:      7818.6665 (Max abs grad: 1601.3447)\n",
      "    Dropout probs:  p1=0.1146, p2=0.1738\n",
      "\n",
      "--- Validation at iteration 360 ---\n",
      "  Average MSE: 14.9830, Average ANEES: 21.0656\n",
      "--------------------------------------------------\n",
      "--- Iter [370/400] ---\n",
      "    Total Loss:     11255.6641\n",
      "    MSE (Raw):      11.251904\n",
      "    MSE (Weighted): 11251.9043 (lambda=1000.0)\n",
      "    NLL Component:  3.7454\n",
      "    Reg Loss:       0.014195\n",
      "    Variance stats: Min=1.00e-04, Max=2.04e+02, Mean=8.26e+00\n",
      "    Mean Error L1:  2.3196\n",
      "    Grad Norm:      7179.6381 (Max abs grad: 2461.5068)\n",
      "    Dropout probs:  p1=0.1136, p2=0.1726\n",
      "--- Iter [380/400] ---\n",
      "    Total Loss:     10905.2002\n",
      "    MSE (Raw):      10.901498\n",
      "    MSE (Weighted): 10901.4980 (lambda=1000.0)\n",
      "    NLL Component:  3.6874\n",
      "    Reg Loss:       0.014182\n",
      "    Variance stats: Min=1.00e-04, Max=5.58e+02, Mean=1.02e+01\n",
      "    Mean Error L1:  2.3234\n",
      "    Grad Norm:      7736.6977 (Max abs grad: 2219.0767)\n",
      "    Dropout probs:  p1=0.1124, p2=0.1712\n",
      "\n",
      "--- Validation at iteration 380 ---\n",
      "  Average MSE: 15.7706, Average ANEES: 20.2834\n",
      "--------------------------------------------------\n",
      "--- Iter [390/400] ---\n",
      "    Total Loss:     11857.4795\n",
      "    MSE (Raw):      11.854321\n",
      "    MSE (Weighted): 11854.3213 (lambda=1000.0)\n",
      "    NLL Component:  3.1433\n",
      "    Reg Loss:       0.014170\n",
      "    Variance stats: Min=1.00e-04, Max=2.63e+02, Mean=8.89e+00\n",
      "    Mean Error L1:  2.3793\n",
      "    Grad Norm:      6820.2168 (Max abs grad: 2323.6045)\n",
      "    Dropout probs:  p1=0.1116, p2=0.1701\n",
      "--- Iter [400/400] ---\n",
      "    Total Loss:     10925.9893\n",
      "    MSE (Raw):      10.922118\n",
      "    MSE (Weighted): 10922.1182 (lambda=1000.0)\n",
      "    NLL Component:  3.8562\n",
      "    Reg Loss:       0.014161\n",
      "    Variance stats: Min=1.00e-04, Max=1.79e+02, Mean=9.00e+00\n",
      "    Mean Error L1:  2.3345\n",
      "    Grad Norm:      8097.5476 (Max abs grad: 2274.7910)\n",
      "    Dropout probs:  p1=0.1108, p2=0.1691\n",
      "\n",
      "--- Validation at iteration 400 ---\n",
      "  Average MSE: 17.4864, Average ANEES: 21.6267\n",
      "--------------------------------------------------\n",
      "\n",
      "Training completed.\n",
      "Loading best model from iteration 40 with ANEES 13.7011\n",
      "‚úÖ F√°ze 1 dokonƒçena. Model ulo≈æen do: bkn_curriculum_phase1_len10.pth\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 2: SeqLen 100 | LR 0.0001 | TBPTT: True\n",
      "============================================================\n",
      "   -> Using TBPTT with window size 20\n",
      "üöÄ START TBPTT Training (Window=20): Loss = NLL + 1000.0 * MSE\n",
      "Iter 10 (TBPTT): Loss 552554.2500 | MSE 552.54 | VarMean 1249.4\n",
      "Iter 20 (TBPTT): Loss 2393399.0000 | MSE 2393.31 | VarMean 8511.0\n",
      "Iter 30 (TBPTT): Loss 1021727.2500 | MSE 1021.68 | VarMean 4303.4\n",
      "Iter 40 (TBPTT): Loss 1641752.3750 | MSE 1641.69 | VarMean 6239.5\n",
      "Iter 50 (TBPTT): Loss 812312.7500 | MSE 812.27 | VarMean 3984.8\n",
      "\n",
      "--- VALIDATION: MSE 327.4020 ---\n",
      "Iter 60 (TBPTT): Loss 403877.4375 | MSE 403.87 | VarMean 424.0\n",
      "Iter 70 (TBPTT): Loss 1961050.5000 | MSE 1960.98 | VarMean 6975.6\n",
      "Iter 80 (TBPTT): Loss 826751.8125 | MSE 826.70 | VarMean 4511.6\n",
      "Iter 90 (TBPTT): Loss 305940.3125 | MSE 305.94 | VarMean 268.1\n",
      "Iter 100 (TBPTT): Loss 3636950.0000 | MSE 3636.87 | VarMean 8113.2\n",
      "\n",
      "--- VALIDATION: MSE 335.3464 ---\n",
      "Iter 110 (TBPTT): Loss 968019.5625 | MSE 967.97 | VarMean 5102.8\n",
      "Iter 120 (TBPTT): Loss 1420991.3750 | MSE 1420.95 | VarMean 4422.2\n",
      "Iter 130 (TBPTT): Loss 343156.6562 | MSE 343.15 | VarMean 690.6\n",
      "Iter 140 (TBPTT): Loss 2148836.5000 | MSE 2148.78 | VarMean 5284.1\n",
      "Iter 150 (TBPTT): Loss 1447047.2500 | MSE 1446.97 | VarMean 7248.8\n",
      "\n",
      "--- VALIDATION: MSE 314.7920 ---\n",
      "Iter 160 (TBPTT): Loss 959130.5000 | MSE 959.08 | VarMean 5337.5\n",
      "Iter 170 (TBPTT): Loss 13822329.0000 | MSE 13822.23 | VarMean 10069.3\n",
      "Iter 180 (TBPTT): Loss 763917.7500 | MSE 763.86 | VarMean 5365.7\n",
      "Iter 190 (TBPTT): Loss 372967.5312 | MSE 372.96 | VarMean 256.0\n",
      "Iter 200 (TBPTT): Loss 301267.1250 | MSE 301.26 | VarMean 235.8\n",
      "\n",
      "--- VALIDATION: MSE 268.5120 ---\n",
      "Iter 210 (TBPTT): Loss 351971.4062 | MSE 351.97 | VarMean 244.9\n",
      "Iter 220 (TBPTT): Loss 31201198.0000 | MSE 31201.15 | VarMean 4546.4\n",
      "Iter 230 (TBPTT): Loss 385820.8438 | MSE 385.82 | VarMean 274.6\n",
      "Iter 240 (TBPTT): Loss 1450638.5000 | MSE 1450.55 | VarMean 8509.9\n",
      "Iter 250 (TBPTT): Loss 2256121.5000 | MSE 2256.00 | VarMean 12029.0\n",
      "\n",
      "--- VALIDATION: MSE 199.0141 ---\n",
      "Iter 260 (TBPTT): Loss 318802.2500 | MSE 318.80 | VarMean 336.2\n",
      "Iter 270 (TBPTT): Loss 5016540.5000 | MSE 5016.18 | VarMean 36030.4\n",
      "Iter 280 (TBPTT): Loss 590199.7500 | MSE 590.17 | VarMean 2562.7\n",
      "Iter 290 (TBPTT): Loss 381821.9688 | MSE 381.81 | VarMean 764.0\n",
      "Iter 300 (TBPTT): Loss 2228845.7500 | MSE 2228.76 | VarMean 8479.8\n",
      "\n",
      "--- VALIDATION: MSE 310.5242 ---\n",
      "Iter 310 (TBPTT): Loss 6630090.0000 | MSE 6629.97 | VarMean 11995.6\n",
      "Iter 320 (TBPTT): Loss 611919.3750 | MSE 611.88 | VarMean 3429.5\n",
      "Iter 330 (TBPTT): Loss 4016895.2500 | MSE 4016.80 | VarMean 9594.2\n",
      "Iter 340 (TBPTT): Loss 340191.5312 | MSE 340.19 | VarMean 312.3\n",
      "Iter 350 (TBPTT): Loss 369158.1875 | MSE 369.15 | VarMean 356.8\n",
      "\n",
      "--- VALIDATION: MSE 243.0211 ---\n",
      "Iter 360 (TBPTT): Loss 3414620.0000 | MSE 3414.38 | VarMean 24084.8\n",
      "Iter 370 (TBPTT): Loss 681209.1875 | MSE 681.18 | VarMean 3033.9\n",
      "Iter 380 (TBPTT): Loss 857327.3125 | MSE 857.28 | VarMean 4471.7\n",
      "Iter 390 (TBPTT): Loss 308624.9688 | MSE 308.62 | VarMean 295.5\n",
      "Iter 400 (TBPTT): Loss 285677.4062 | MSE 285.67 | VarMean 245.9\n",
      "\n",
      "--- VALIDATION: MSE 367.0017 ---\n",
      "Iter 410 (TBPTT): Loss 1110169.8750 | MSE 1110.10 | VarMean 6769.1\n",
      "Iter 420 (TBPTT): Loss 312310.6875 | MSE 312.31 | VarMean 269.3\n",
      "Iter 430 (TBPTT): Loss 908962.3125 | MSE 908.90 | VarMean 5730.9\n",
      "Iter 440 (TBPTT): Loss 2579301.2500 | MSE 2579.20 | VarMean 10290.1\n",
      "Iter 450 (TBPTT): Loss 844584.5625 | MSE 844.53 | VarMean 4907.2\n",
      "\n",
      "--- VALIDATION: MSE 286.9834 ---\n",
      "Iter 460 (TBPTT): Loss 25099776.0000 | MSE 25099.70 | VarMean 7691.9\n",
      "Iter 470 (TBPTT): Loss 316660.6875 | MSE 316.66 | VarMean 243.8\n",
      "Iter 480 (TBPTT): Loss 1176684.6250 | MSE 1176.63 | VarMean 5414.0\n",
      "Iter 490 (TBPTT): Loss 4557801.5000 | MSE 4557.68 | VarMean 11571.2\n",
      "Iter 500 (TBPTT): Loss 338964.8125 | MSE 338.96 | VarMean 276.9\n",
      "\n",
      "--- VALIDATION: MSE 206.2167 ---\n",
      "Iter 510 (TBPTT): Loss 330127.9688 | MSE 330.12 | VarMean 261.8\n",
      "Iter 520 (TBPTT): Loss 1969913.7500 | MSE 1969.84 | VarMean 7369.5\n",
      "Iter 530 (TBPTT): Loss 395570.8125 | MSE 395.56 | VarMean 829.4\n",
      "Iter 540 (TBPTT): Loss 387724.2812 | MSE 387.72 | VarMean 268.4\n",
      "Iter 550 (TBPTT): Loss 791121.6875 | MSE 791.09 | VarMean 2900.1\n",
      "\n",
      "--- VALIDATION: MSE 377.2581 ---\n",
      "Iter 560 (TBPTT): Loss 1134907.8750 | MSE 1134.87 | VarMean 3988.7\n",
      "Iter 570 (TBPTT): Loss 600371.4375 | MSE 600.34 | VarMean 2712.5\n",
      "Iter 580 (TBPTT): Loss 247724.4844 | MSE 247.72 | VarMean 236.9\n",
      "Iter 590 (TBPTT): Loss 529134.5625 | MSE 529.11 | VarMean 1782.8\n",
      "Iter 600 (TBPTT): Loss 711186.7500 | MSE 711.15 | VarMean 3835.1\n",
      "\n",
      "--- VALIDATION: MSE 330.9141 ---\n",
      "Iter 610 (TBPTT): Loss 409596.5000 | MSE 409.59 | VarMean 711.2\n",
      "Iter 620 (TBPTT): Loss 297106.7500 | MSE 297.10 | VarMean 241.3\n",
      "Iter 630 (TBPTT): Loss 508484.0625 | MSE 508.48 | VarMean 251.4\n",
      "Iter 640 (TBPTT): Loss 2563759.7500 | MSE 2563.70 | VarMean 5458.1\n",
      "Iter 650 (TBPTT): Loss 5780371.5000 | MSE 5780.25 | VarMean 11969.4\n",
      "\n",
      "--- VALIDATION: MSE 333.1042 ---\n",
      "Iter 660 (TBPTT): Loss 950916.6250 | MSE 950.86 | VarMean 5601.8\n",
      "Iter 670 (TBPTT): Loss 1235152.3750 | MSE 1235.10 | VarMean 4671.9\n",
      "Iter 680 (TBPTT): Loss 320494.4375 | MSE 320.49 | VarMean 260.6\n",
      "Iter 690 (TBPTT): Loss 1283740.2500 | MSE 1283.69 | VarMean 4711.8\n",
      "Iter 700 (TBPTT): Loss 303314.9688 | MSE 303.31 | VarMean 247.6\n",
      "\n",
      "--- VALIDATION: MSE 285.3058 ---\n",
      "Iter 710 (TBPTT): Loss 890507.0000 | MSE 890.46 | VarMean 4655.6\n",
      "Iter 720 (TBPTT): Loss 459367.2812 | MSE 459.36 | VarMean 317.7\n",
      "Iter 730 (TBPTT): Loss 669289.7500 | MSE 669.25 | VarMean 4100.5\n",
      "Iter 740 (TBPTT): Loss 38324972.0000 | MSE 38324.94 | VarMean 2221.5\n",
      "Iter 750 (TBPTT): Loss 470420.6875 | MSE 470.40 | VarMean 1880.7\n",
      "\n",
      "--- VALIDATION: MSE 198.0672 ---\n",
      "Iter 760 (TBPTT): Loss 282381.5312 | MSE 282.38 | VarMean 219.7\n",
      "Iter 770 (TBPTT): Loss 566570.9375 | MSE 566.54 | VarMean 2596.4\n",
      "Iter 780 (TBPTT): Loss 527427.3750 | MSE 527.41 | VarMean 1325.1\n",
      "Iter 790 (TBPTT): Loss 4926111.5000 | MSE 4925.95 | VarMean 16368.5\n",
      "Iter 800 (TBPTT): Loss 384059.8750 | MSE 384.05 | VarMean 747.6\n",
      "\n",
      "--- VALIDATION: MSE 230.9452 ---\n",
      "Iter 810 (TBPTT): Loss 341761.4375 | MSE 341.76 | VarMean 220.9\n",
      "Iter 820 (TBPTT): Loss 965360.0625 | MSE 965.30 | VarMean 6011.7\n",
      "Iter 830 (TBPTT): Loss 321216.6562 | MSE 321.21 | VarMean 264.4\n",
      "Iter 840 (TBPTT): Loss 2744546.0000 | MSE 2744.40 | VarMean 13986.5\n",
      "Iter 850 (TBPTT): Loss 1207714.8750 | MSE 1207.69 | VarMean 2380.0\n",
      "\n",
      "--- VALIDATION: MSE 301.7170 ---\n",
      "Iter 860 (TBPTT): Loss 374377.9062 | MSE 374.37 | VarMean 282.1\n",
      "Iter 870 (TBPTT): Loss 1395799.7500 | MSE 1395.77 | VarMean 2638.8\n",
      "Iter 880 (TBPTT): Loss 717858.4375 | MSE 717.82 | VarMean 3876.3\n",
      "Iter 890 (TBPTT): Loss 1392206.5000 | MSE 1392.16 | VarMean 4591.1\n",
      "Iter 900 (TBPTT): Loss 430925.1875 | MSE 430.92 | VarMean 223.7\n",
      "\n",
      "--- VALIDATION: MSE 189.7002 ---\n",
      "Iter 910 (TBPTT): Loss 6999408.5000 | MSE 6999.28 | VarMean 12749.0\n",
      "Iter 920 (TBPTT): Loss 742935.6250 | MSE 742.89 | VarMean 4563.6\n",
      "Iter 930 (TBPTT): Loss 10316508.0000 | MSE 10316.39 | VarMean 11915.2\n",
      "Iter 940 (TBPTT): Loss 877387.6875 | MSE 877.34 | VarMean 4544.1\n",
      "Iter 950 (TBPTT): Loss 5507711.5000 | MSE 5507.59 | VarMean 11798.2\n",
      "\n",
      "--- VALIDATION: MSE 319.9916 ---\n",
      "Iter 960 (TBPTT): Loss 2111564.5000 | MSE 2111.46 | VarMean 10545.0\n",
      "Iter 970 (TBPTT): Loss 400015.7500 | MSE 400.01 | VarMean 673.7\n",
      "Iter 980 (TBPTT): Loss 798257.8125 | MSE 798.23 | VarMean 2100.1\n",
      "Iter 990 (TBPTT): Loss 307851.0938 | MSE 307.85 | VarMean 232.8\n",
      "Iter 1000 (TBPTT): Loss 377868.2500 | MSE 377.86 | VarMean 257.6\n",
      "\n",
      "--- VALIDATION: MSE 209.7110 ---\n",
      "Iter 1010 (TBPTT): Loss 282348.9688 | MSE 282.34 | VarMean 252.6\n",
      "Iter 1020 (TBPTT): Loss 1116932.7500 | MSE 1116.88 | VarMean 4799.9\n",
      "Iter 1030 (TBPTT): Loss 345436.4688 | MSE 345.43 | VarMean 247.8\n",
      "Iter 1040 (TBPTT): Loss 727569.2500 | MSE 727.53 | VarMean 3694.8\n",
      "Iter 1050 (TBPTT): Loss 320906.0938 | MSE 320.90 | VarMean 231.4\n",
      "\n",
      "--- VALIDATION: MSE 233.0207 ---\n",
      "Iter 1060 (TBPTT): Loss 5085966.5000 | MSE 5085.81 | VarMean 15095.8\n",
      "Iter 1070 (TBPTT): Loss 401003.9375 | MSE 401.00 | VarMean 609.1\n",
      "Iter 1080 (TBPTT): Loss 223258.4844 | MSE 223.25 | VarMean 267.5\n",
      "Iter 1090 (TBPTT): Loss 1833821.6250 | MSE 1833.75 | VarMean 7333.8\n",
      "Iter 1100 (TBPTT): Loss 444089.7812 | MSE 444.08 | VarMean 1071.1\n",
      "\n",
      "--- VALIDATION: MSE 339.8908 ---\n",
      "Iter 1110 (TBPTT): Loss 336410.6250 | MSE 336.41 | VarMean 342.6\n",
      "Iter 1120 (TBPTT): Loss 438759.7812 | MSE 438.75 | VarMean 473.5\n",
      "Iter 1130 (TBPTT): Loss 1054836.0000 | MSE 1054.82 | VarMean 1442.8\n",
      "Iter 1140 (TBPTT): Loss 288769.1250 | MSE 288.77 | VarMean 232.7\n",
      "Iter 1150 (TBPTT): Loss 31832688.0000 | MSE 31832.61 | VarMean 8088.3\n",
      "\n",
      "--- VALIDATION: MSE 212.0635 ---\n",
      "Iter 1160 (TBPTT): Loss 502505.2188 | MSE 502.49 | VarMean 920.9\n",
      "Iter 1170 (TBPTT): Loss 264621.3438 | MSE 264.62 | VarMean 263.6\n",
      "Iter 1180 (TBPTT): Loss 2926525.7500 | MSE 2926.41 | VarMean 11508.4\n",
      "Iter 1190 (TBPTT): Loss 2773594.2500 | MSE 2773.50 | VarMean 8752.4\n",
      "Iter 1200 (TBPTT): Loss 429043.0312 | MSE 429.03 | VarMean 1535.9\n",
      "\n",
      "--- VALIDATION: MSE 233.0761 ---\n",
      "Iter 1210 (TBPTT): Loss 14423639.0000 | MSE 14423.50 | VarMean 13989.0\n",
      "Iter 1220 (TBPTT): Loss 7904905.0000 | MSE 7904.76 | VarMean 14266.5\n",
      "Iter 1230 (TBPTT): Loss 802491.3125 | MSE 802.44 | VarMean 4507.9\n",
      "Iter 1240 (TBPTT): Loss 1282921.2500 | MSE 1282.86 | VarMean 5994.0\n",
      "Iter 1250 (TBPTT): Loss 1528893.0000 | MSE 1528.82 | VarMean 7090.4\n",
      "\n",
      "--- VALIDATION: MSE 275.9172 ---\n",
      "Iter 1260 (TBPTT): Loss 2798240.2500 | MSE 2797.99 | VarMean 24937.3\n",
      "Iter 1270 (TBPTT): Loss 3734978.2500 | MSE 3734.90 | VarMean 7379.0\n",
      "Iter 1280 (TBPTT): Loss 651671.5625 | MSE 651.64 | VarMean 3282.3\n",
      "Iter 1290 (TBPTT): Loss 7473194.0000 | MSE 7473.00 | VarMean 18871.2\n",
      "Iter 1300 (TBPTT): Loss 720401.6875 | MSE 720.35 | VarMean 4535.5\n",
      "\n",
      "--- VALIDATION: MSE 196.9521 ---\n",
      "Iter 1310 (TBPTT): Loss 319795.2812 | MSE 319.79 | VarMean 296.4\n",
      "Iter 1320 (TBPTT): Loss 488972.5938 | MSE 488.96 | VarMean 869.6\n",
      "Iter 1330 (TBPTT): Loss 307582.7188 | MSE 307.58 | VarMean 409.7\n",
      "Iter 1340 (TBPTT): Loss 274497.1562 | MSE 274.49 | VarMean 326.5\n",
      "Iter 1350 (TBPTT): Loss 309298.2812 | MSE 309.29 | VarMean 240.4\n",
      "\n",
      "--- VALIDATION: MSE 236.5400 ---\n",
      "Iter 1360 (TBPTT): Loss 1048758.2500 | MSE 1048.70 | VarMean 6108.1\n",
      "Iter 1370 (TBPTT): Loss 851414.5625 | MSE 851.36 | VarMean 4946.5\n",
      "Iter 1380 (TBPTT): Loss 4455099.5000 | MSE 4454.97 | VarMean 12572.6\n",
      "Iter 1390 (TBPTT): Loss 243852.3906 | MSE 243.85 | VarMean 228.8\n",
      "Iter 1400 (TBPTT): Loss 303045.2500 | MSE 303.04 | VarMean 366.3\n",
      "\n",
      "--- VALIDATION: MSE 169.8823 ---\n",
      "Iter 1410 (TBPTT): Loss 14588321.0000 | MSE 14588.17 | VarMean 14399.1\n",
      "Iter 1420 (TBPTT): Loss 289695.5625 | MSE 289.69 | VarMean 227.7\n",
      "Iter 1430 (TBPTT): Loss 501011.8438 | MSE 500.99 | VarMean 2276.8\n",
      "Iter 1440 (TBPTT): Loss 254129.9688 | MSE 254.13 | VarMean 250.1\n",
      "Iter 1450 (TBPTT): Loss 11898080.0000 | MSE 11897.87 | VarMean 20990.1\n",
      "\n",
      "--- VALIDATION: MSE 230.4638 ---\n",
      "Iter 1460 (TBPTT): Loss 409166.6875 | MSE 409.16 | VarMean 940.8\n",
      "Iter 1470 (TBPTT): Loss 4745670.5000 | MSE 4745.54 | VarMean 12776.3\n",
      "Iter 1480 (TBPTT): Loss 733481.7500 | MSE 733.44 | VarMean 4154.3\n",
      "Iter 1490 (TBPTT): Loss 601269.7500 | MSE 601.24 | VarMean 3238.6\n",
      "Iter 1500 (TBPTT): Loss 1723837.0000 | MSE 1723.75 | VarMean 8650.9\n",
      "\n",
      "--- VALIDATION: MSE 390.7039 ---\n",
      "Iter 1510 (TBPTT): Loss 1662463.1250 | MSE 1662.38 | VarMean 8465.0\n",
      "Iter 1520 (TBPTT): Loss 832919.6250 | MSE 832.86 | VarMean 5838.3\n",
      "Iter 1530 (TBPTT): Loss 341981.0625 | MSE 341.98 | VarMean 187.4\n",
      "Iter 1540 (TBPTT): Loss 1195272.5000 | MSE 1195.19 | VarMean 8194.4\n",
      "Iter 1550 (TBPTT): Loss 5285099.5000 | MSE 5284.96 | VarMean 13405.3\n",
      "\n",
      "--- VALIDATION: MSE 272.1164 ---\n",
      "Iter 1560 (TBPTT): Loss 1551263.7500 | MSE 1551.21 | VarMean 5302.8\n",
      "Iter 1570 (TBPTT): Loss 7284440.5000 | MSE 7284.30 | VarMean 13717.1\n",
      "Iter 1580 (TBPTT): Loss 5999171.0000 | MSE 5999.10 | VarMean 7414.6\n",
      "Iter 1590 (TBPTT): Loss 279884.5625 | MSE 279.88 | VarMean 331.4\n",
      "Iter 1600 (TBPTT): Loss 321479.6875 | MSE 321.48 | VarMean 272.1\n",
      "\n",
      "--- VALIDATION: MSE 223.0699 ---\n",
      "Iter 1610 (TBPTT): Loss 2039132.3750 | MSE 2039.03 | VarMean 10030.7\n",
      "Iter 1620 (TBPTT): Loss 264539.8438 | MSE 264.54 | VarMean 282.1\n",
      "Iter 1630 (TBPTT): Loss 7448822.5000 | MSE 7448.76 | VarMean 6218.7\n",
      "Iter 1640 (TBPTT): Loss 393290.2812 | MSE 393.28 | VarMean 665.6\n",
      "Iter 1650 (TBPTT): Loss 639345.3125 | MSE 639.31 | VarMean 3264.9\n",
      "\n",
      "--- VALIDATION: MSE 261.7633 ---\n",
      "Iter 1660 (TBPTT): Loss 977137.6250 | MSE 977.12 | VarMean 1890.0\n",
      "Iter 1670 (TBPTT): Loss 570883.4375 | MSE 570.86 | VarMean 2650.6\n",
      "Iter 1680 (TBPTT): Loss 1939997.3750 | MSE 1939.93 | VarMean 7017.2\n",
      "Iter 1690 (TBPTT): Loss 536058.6875 | MSE 536.03 | VarMean 2408.8\n",
      "Iter 1700 (TBPTT): Loss 25974076.0000 | MSE 25973.97 | VarMean 10954.3\n",
      "\n",
      "--- VALIDATION: MSE 312.8585 ---\n",
      "Iter 1710 (TBPTT): Loss 1440409.7500 | MSE 1440.28 | VarMean 12644.2\n",
      "Iter 1720 (TBPTT): Loss 307869.9375 | MSE 307.87 | VarMean 232.7\n",
      "Iter 1730 (TBPTT): Loss 1342393.0000 | MSE 1342.29 | VarMean 10534.8\n",
      "Iter 1740 (TBPTT): Loss 897195.0000 | MSE 897.15 | VarMean 4597.9\n",
      "Iter 1750 (TBPTT): Loss 640388.1875 | MSE 640.37 | VarMean 1375.3\n",
      "\n",
      "--- VALIDATION: MSE 350.0649 ---\n",
      "Iter 1760 (TBPTT): Loss 3351701.7500 | MSE 3351.53 | VarMean 16905.0\n",
      "Iter 1770 (TBPTT): Loss 5659883.0000 | MSE 5659.81 | VarMean 6711.8\n",
      "Iter 1780 (TBPTT): Loss 472623.7812 | MSE 472.60 | VarMean 2120.1\n",
      "Iter 1790 (TBPTT): Loss 347337.1562 | MSE 347.33 | VarMean 486.7\n",
      "Iter 1800 (TBPTT): Loss 369154.4375 | MSE 369.15 | VarMean 253.4\n",
      "\n",
      "--- VALIDATION: MSE 193.1594 ---\n",
      "Iter 1810 (TBPTT): Loss 572823.8125 | MSE 572.79 | VarMean 3508.4\n",
      "Iter 1820 (TBPTT): Loss 834579.5625 | MSE 834.54 | VarMean 4145.3\n",
      "Iter 1830 (TBPTT): Loss 2299688.2500 | MSE 2299.62 | VarMean 6211.9\n",
      "Iter 1840 (TBPTT): Loss 38964164.0000 | MSE 38964.10 | VarMean 4150.1\n",
      "Iter 1850 (TBPTT): Loss 285165.3438 | MSE 285.16 | VarMean 245.1\n",
      "\n",
      "--- VALIDATION: MSE 211.2076 ---\n",
      "Iter 1860 (TBPTT): Loss 1176166.7500 | MSE 1176.13 | VarMean 3523.0\n",
      "Iter 1870 (TBPTT): Loss 1692333.3750 | MSE 1692.25 | VarMean 8336.3\n",
      "Iter 1880 (TBPTT): Loss 602654.6875 | MSE 602.63 | VarMean 2391.4\n",
      "Iter 1890 (TBPTT): Loss 254643.9844 | MSE 254.64 | VarMean 184.1\n",
      "Iter 1900 (TBPTT): Loss 501480.9688 | MSE 501.46 | VarMean 1458.4\n",
      "\n",
      "--- VALIDATION: MSE 292.6864 ---\n",
      "Iter 1910 (TBPTT): Loss 872875.6250 | MSE 872.82 | VarMean 5530.1\n",
      "Iter 1920 (TBPTT): Loss 707907.0000 | MSE 707.87 | VarMean 3993.5\n",
      "Iter 1930 (TBPTT): Loss 610451.1875 | MSE 610.42 | VarMean 3063.3\n",
      "Iter 1940 (TBPTT): Loss 558477.0625 | MSE 558.44 | VarMean 3737.0\n",
      "Iter 1950 (TBPTT): Loss 362000.5000 | MSE 361.99 | VarMean 967.3\n",
      "\n",
      "--- VALIDATION: MSE 283.8610 ---\n",
      "Iter 1960 (TBPTT): Loss 551532.0000 | MSE 551.50 | VarMean 2986.1\n",
      "Iter 1970 (TBPTT): Loss 1525993.5000 | MSE 1525.92 | VarMean 6679.4\n",
      "Iter 1980 (TBPTT): Loss 163735.9688 | MSE 163.73 | VarMean 248.4\n",
      "Iter 1990 (TBPTT): Loss 1650723.5000 | MSE 1650.69 | VarMean 3019.4\n",
      "Iter 2000 (TBPTT): Loss 419583.9375 | MSE 419.57 | VarMean 861.2\n",
      "\n",
      "--- VALIDATION: MSE 169.8683 ---\n",
      "Iter 2010 (TBPTT): Loss 549202.7500 | MSE 549.18 | VarMean 1868.3\n",
      "Iter 2020 (TBPTT): Loss 1522551.7500 | MSE 1522.44 | VarMean 10623.2\n",
      "Iter 2030 (TBPTT): Loss 868754.7500 | MSE 868.70 | VarMean 5122.7\n",
      "Iter 2040 (TBPTT): Loss 364519.3125 | MSE 364.51 | VarMean 685.4\n",
      "Iter 2050 (TBPTT): Loss 9561566.0000 | MSE 9561.46 | VarMean 10667.7\n",
      "\n",
      "--- VALIDATION: MSE 328.4648 ---\n",
      "Iter 2060 (TBPTT): Loss 450599.0000 | MSE 450.57 | VarMean 2876.2\n",
      "Iter 2070 (TBPTT): Loss 379329.4688 | MSE 379.33 | VarMean 218.5\n",
      "Iter 2080 (TBPTT): Loss 753528.5000 | MSE 753.48 | VarMean 4686.5\n",
      "Iter 2090 (TBPTT): Loss 3843851.2500 | MSE 3843.74 | VarMean 10743.2\n",
      "Iter 2100 (TBPTT): Loss 633931.3125 | MSE 633.90 | VarMean 2532.8\n",
      "\n",
      "--- VALIDATION: MSE 214.2359 ---\n",
      "Iter 2110 (TBPTT): Loss 825125.0000 | MSE 825.08 | VarMean 3854.2\n",
      "Iter 2120 (TBPTT): Loss 1996326.0000 | MSE 1996.26 | VarMean 6055.0\n",
      "Iter 2130 (TBPTT): Loss 305183.0625 | MSE 305.18 | VarMean 231.1\n",
      "Iter 2140 (TBPTT): Loss 572172.2500 | MSE 572.14 | VarMean 3048.7\n",
      "Iter 2150 (TBPTT): Loss 1041571.8125 | MSE 1041.51 | VarMean 6415.1\n",
      "\n",
      "--- VALIDATION: MSE 176.5694 ---\n",
      "Iter 2160 (TBPTT): Loss 1355085.6250 | MSE 1355.04 | VarMean 4328.2\n",
      "Iter 2170 (TBPTT): Loss 312839.6250 | MSE 312.84 | VarMean 235.7\n",
      "Iter 2180 (TBPTT): Loss 855056.4375 | MSE 854.99 | VarMean 6112.0\n",
      "Iter 2190 (TBPTT): Loss 1496459.5000 | MSE 1496.39 | VarMean 6432.2\n",
      "Iter 2200 (TBPTT): Loss 580362.6875 | MSE 580.34 | VarMean 2394.4\n",
      "\n",
      "--- VALIDATION: MSE 280.4399 ---\n",
      "Iter 2210 (TBPTT): Loss 809384.6250 | MSE 809.33 | VarMean 4779.0\n",
      "Iter 2220 (TBPTT): Loss 14594546.0000 | MSE 14594.43 | VarMean 11395.5\n",
      "Iter 2230 (TBPTT): Loss 6820642.5000 | MSE 6820.48 | VarMean 16285.6\n",
      "Iter 2240 (TBPTT): Loss 24897696.0000 | MSE 24897.62 | VarMean 7061.3\n",
      "Iter 2250 (TBPTT): Loss 2865295.2500 | MSE 2865.00 | VarMean 28970.8\n",
      "\n",
      "--- VALIDATION: MSE 194.0942 ---\n",
      "Iter 2260 (TBPTT): Loss 7221223.0000 | MSE 7221.07 | VarMean 14843.7\n",
      "Iter 2270 (TBPTT): Loss 678752.1250 | MSE 678.72 | VarMean 2939.4\n",
      "Iter 2280 (TBPTT): Loss 561026.0000 | MSE 561.01 | VarMean 1356.2\n",
      "Iter 2290 (TBPTT): Loss 600316.8125 | MSE 600.29 | VarMean 2609.9\n",
      "Iter 2300 (TBPTT): Loss 1109951.0000 | MSE 1109.88 | VarMean 6649.5\n",
      "\n",
      "--- VALIDATION: MSE 219.0217 ---\n",
      "Iter 2310 (TBPTT): Loss 440918.2500 | MSE 440.90 | VarMean 1289.2\n",
      "Iter 2320 (TBPTT): Loss 605547.3750 | MSE 605.51 | VarMean 3268.6\n",
      "Iter 2330 (TBPTT): Loss 38892900.0000 | MSE 38892.86 | VarMean 1408.1\n",
      "Iter 2340 (TBPTT): Loss 199515552.0000 | MSE 199515.28 | VarMean 27922.4\n",
      "Iter 2350 (TBPTT): Loss 730473.3750 | MSE 730.43 | VarMean 3890.1\n",
      "\n",
      "--- VALIDATION: MSE 201.6105 ---\n",
      "Iter 2360 (TBPTT): Loss 1617025.8750 | MSE 1616.96 | VarMean 6057.0\n",
      "Iter 2370 (TBPTT): Loss 661672.5000 | MSE 661.63 | VarMean 4185.5\n",
      "Iter 2380 (TBPTT): Loss 3054515.7500 | MSE 3054.44 | VarMean 7221.5\n",
      "Iter 2390 (TBPTT): Loss 1211961.0000 | MSE 1211.89 | VarMean 6590.0\n",
      "Iter 2400 (TBPTT): Loss 1937550.5000 | MSE 1937.51 | VarMean 3529.2\n",
      "\n",
      "--- VALIDATION: MSE 177.9649 ---\n",
      "Iter 2410 (TBPTT): Loss 29669626.0000 | MSE 29669.38 | VarMean 24485.4\n",
      "Iter 2420 (TBPTT): Loss 661334.0000 | MSE 661.31 | VarMean 2238.7\n",
      "Iter 2430 (TBPTT): Loss 815303.3125 | MSE 815.25 | VarMean 5501.9\n",
      "Iter 2440 (TBPTT): Loss 728886.6250 | MSE 728.85 | VarMean 3479.9\n",
      "Iter 2450 (TBPTT): Loss 772401.1875 | MSE 772.36 | VarMean 4181.3\n",
      "\n",
      "--- VALIDATION: MSE 170.9165 ---\n",
      "Iter 2460 (TBPTT): Loss 396138.9062 | MSE 396.13 | VarMean 240.2\n",
      "Iter 2470 (TBPTT): Loss 669199.3750 | MSE 669.16 | VarMean 4077.6\n",
      "Iter 2480 (TBPTT): Loss 365084.8125 | MSE 365.08 | VarMean 545.2\n",
      "Iter 2490 (TBPTT): Loss 1920941.5000 | MSE 1920.85 | VarMean 9255.5\n",
      "Iter 2500 (TBPTT): Loss 4816628.0000 | MSE 4816.54 | VarMean 8991.0\n",
      "\n",
      "--- VALIDATION: MSE 210.1155 ---\n",
      "Iter 2510 (TBPTT): Loss 564196.2500 | MSE 564.17 | VarMean 2735.8\n",
      "Iter 2520 (TBPTT): Loss 797171.6250 | MSE 797.12 | VarMean 4453.1\n",
      "Iter 2530 (TBPTT): Loss 281200.6250 | MSE 281.20 | VarMean 236.4\n",
      "Iter 2540 (TBPTT): Loss 20956672.0000 | MSE 20956.50 | VarMean 16890.3\n",
      "Iter 2550 (TBPTT): Loss 1668785.6250 | MSE 1668.71 | VarMean 7497.5\n",
      "\n",
      "--- VALIDATION: MSE 206.5886 ---\n",
      "Iter 2560 (TBPTT): Loss 592463.9375 | MSE 592.45 | VarMean 1095.9\n",
      "Iter 2570 (TBPTT): Loss 400574.8750 | MSE 400.57 | VarMean 253.7\n",
      "Iter 2580 (TBPTT): Loss 1891360.3750 | MSE 1891.26 | VarMean 9954.7\n",
      "Iter 2590 (TBPTT): Loss 497116.9062 | MSE 497.08 | VarMean 3095.9\n",
      "Iter 2600 (TBPTT): Loss 299903.7812 | MSE 299.90 | VarMean 228.1\n",
      "\n",
      "--- VALIDATION: MSE 214.2938 ---\n",
      "Iter 2610 (TBPTT): Loss 397025.5625 | MSE 397.02 | VarMean 243.7\n",
      "Iter 2620 (TBPTT): Loss 580073.5625 | MSE 580.04 | VarMean 3144.0\n",
      "Iter 2630 (TBPTT): Loss 571792.9375 | MSE 571.77 | VarMean 2142.8\n",
      "Iter 2640 (TBPTT): Loss 289670.1250 | MSE 289.67 | VarMean 229.1\n",
      "Iter 2650 (TBPTT): Loss 341195.1250 | MSE 341.19 | VarMean 743.9\n",
      "\n",
      "--- VALIDATION: MSE 286.5541 ---\n",
      "Iter 2660 (TBPTT): Loss 266894.6875 | MSE 266.89 | VarMean 218.7\n",
      "Iter 2670 (TBPTT): Loss 887665.7500 | MSE 887.62 | VarMean 4269.0\n",
      "Iter 2680 (TBPTT): Loss 10656137.0000 | MSE 10656.00 | VarMean 13327.6\n",
      "Iter 2690 (TBPTT): Loss 1355958.8750 | MSE 1355.90 | VarMean 6190.1\n",
      "Iter 2700 (TBPTT): Loss 243420.6875 | MSE 243.42 | VarMean 235.5\n",
      "\n",
      "--- VALIDATION: MSE 228.7373 ---\n",
      "Iter 2710 (TBPTT): Loss 705799.1250 | MSE 705.76 | VarMean 3960.3\n",
      "Iter 2720 (TBPTT): Loss 289183.3750 | MSE 289.18 | VarMean 223.0\n",
      "Iter 2730 (TBPTT): Loss 2408446.5000 | MSE 2408.35 | VarMean 9856.8\n",
      "Iter 2740 (TBPTT): Loss 282456.0938 | MSE 282.45 | VarMean 550.4\n",
      "Iter 2750 (TBPTT): Loss 3761936.7500 | MSE 3761.84 | VarMean 9472.5\n",
      "\n",
      "--- VALIDATION: MSE 182.6304 ---\n",
      "Iter 2760 (TBPTT): Loss 534699.2500 | MSE 534.67 | VarMean 2487.0\n",
      "Iter 2770 (TBPTT): Loss 1853013.1250 | MSE 1852.94 | VarMean 7465.7\n",
      "Iter 2780 (TBPTT): Loss 3078167.7500 | MSE 3078.10 | VarMean 6479.8\n",
      "Iter 2790 (TBPTT): Loss 264826.7500 | MSE 264.82 | VarMean 238.7\n",
      "Iter 2800 (TBPTT): Loss 817180.0625 | MSE 817.13 | VarMean 4394.0\n",
      "\n",
      "--- VALIDATION: MSE 230.1418 ---\n",
      "Iter 2810 (TBPTT): Loss 460941.0312 | MSE 460.92 | VarMean 1758.7\n",
      "Iter 2820 (TBPTT): Loss 311301.1562 | MSE 311.30 | VarMean 230.1\n",
      "Iter 2830 (TBPTT): Loss 745880.5625 | MSE 745.84 | VarMean 3930.9\n",
      "Iter 2840 (TBPTT): Loss 765227.3750 | MSE 765.18 | VarMean 4458.8\n",
      "Iter 2850 (TBPTT): Loss 2030622.0000 | MSE 2030.53 | VarMean 8872.5\n",
      "\n",
      "--- VALIDATION: MSE 185.1907 ---\n",
      "Iter 2860 (TBPTT): Loss 364933.9062 | MSE 364.92 | VarMean 1603.7\n",
      "Iter 2870 (TBPTT): Loss 2171626.5000 | MSE 2171.55 | VarMean 7063.8\n",
      "Iter 2880 (TBPTT): Loss 416953.0312 | MSE 416.94 | VarMean 1283.9\n",
      "Iter 2890 (TBPTT): Loss 445096.5312 | MSE 445.08 | VarMean 1816.5\n",
      "Iter 2900 (TBPTT): Loss 473455.0312 | MSE 473.43 | VarMean 2292.2\n",
      "\n",
      "--- VALIDATION: MSE 185.3667 ---\n",
      "Iter 2910 (TBPTT): Loss 17124960.0000 | MSE 17124.77 | VarMean 18680.9\n",
      "Iter 2920 (TBPTT): Loss 10567402.0000 | MSE 10567.22 | VarMean 17603.2\n",
      "Iter 2930 (TBPTT): Loss 954073.5000 | MSE 954.03 | VarMean 3778.7\n",
      "Iter 2940 (TBPTT): Loss 704433.5625 | MSE 704.40 | VarMean 3006.6\n",
      "Iter 2950 (TBPTT): Loss 431572.1250 | MSE 431.56 | VarMean 1458.5\n",
      "\n",
      "--- VALIDATION: MSE 295.7432 ---\n",
      "Iter 2960 (TBPTT): Loss 284871.4375 | MSE 284.87 | VarMean 240.4\n",
      "Iter 2970 (TBPTT): Loss 396723.5625 | MSE 396.72 | VarMean 247.9\n",
      "Iter 2980 (TBPTT): Loss 1044169.6875 | MSE 1044.09 | VarMean 7477.5\n",
      "Iter 2990 (TBPTT): Loss 352299.8125 | MSE 352.29 | VarMean 488.5\n",
      "Iter 3000 (TBPTT): Loss 337084.8750 | MSE 337.08 | VarMean 439.3\n",
      "\n",
      "--- VALIDATION: MSE 240.8084 ---\n",
      "TBPTT Training completed.\n",
      "‚úÖ F√°ze 2 dokonƒçena. Model ulo≈æen do: bkn_curriculum_phase2_len100.pth\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 2: SeqLen 300 | LR 5e-05 | TBPTT: True\n",
      "============================================================\n",
      "   -> Using TBPTT with window size 50\n",
      "üöÄ START TBPTT Training (Window=50): Loss = NLL + 1000.0 * MSE\n",
      "Iter 10 (TBPTT): Loss 397411.3125 | MSE 397.39 | VarMean 1996.7\n",
      "Iter 20 (TBPTT): Loss 1487556.2500 | MSE 1487.50 | VarMean 5681.8\n",
      "Iter 30 (TBPTT): Loss 227761.3906 | MSE 227.76 | VarMean 206.1\n",
      "Iter 40 (TBPTT): Loss 1117864.3750 | MSE 1117.85 | VarMean 1625.3\n",
      "Iter 50 (TBPTT): Loss 190664.8594 | MSE 190.66 | VarMean 182.0\n",
      "\n",
      "--- VALIDATION: MSE 227.0839 ---\n",
      "Iter 60 (TBPTT): Loss 533578.6250 | MSE 533.54 | VarMean 3329.7\n",
      "Iter 70 (TBPTT): Loss 310767.7812 | MSE 310.76 | VarMean 881.5\n",
      "Iter 80 (TBPTT): Loss 1270010.0000 | MSE 1269.92 | VarMean 9090.2\n",
      "Iter 90 (TBPTT): Loss 761025.1875 | MSE 760.96 | VarMean 5955.9\n",
      "Iter 100 (TBPTT): Loss 328901.5000 | MSE 328.89 | VarMean 1159.9\n",
      "\n",
      "--- VALIDATION: MSE 305.5138 ---\n",
      "Iter 110 (TBPTT): Loss 202172.7031 | MSE 202.17 | VarMean 475.6\n",
      "Iter 120 (TBPTT): Loss 376741.5000 | MSE 376.73 | VarMean 834.3\n",
      "Iter 130 (TBPTT): Loss 388645.8438 | MSE 388.63 | VarMean 1171.4\n",
      "Iter 140 (TBPTT): Loss 473598.1875 | MSE 473.58 | VarMean 1735.0\n",
      "Iter 150 (TBPTT): Loss 445281.6250 | MSE 445.25 | VarMean 2646.1\n",
      "\n",
      "--- VALIDATION: MSE 223.0798 ---\n",
      "Iter 160 (TBPTT): Loss 192139.3750 | MSE 192.14 | VarMean 198.3\n",
      "Iter 170 (TBPTT): Loss 209789.8906 | MSE 209.78 | VarMean 316.6\n",
      "Iter 180 (TBPTT): Loss 290691.8750 | MSE 290.68 | VarMean 919.3\n",
      "Iter 190 (TBPTT): Loss 222613.4219 | MSE 222.61 | VarMean 490.7\n",
      "Iter 200 (TBPTT): Loss 604075.7500 | MSE 604.04 | VarMean 3798.1\n",
      "\n",
      "--- VALIDATION: MSE 152.2179 ---\n",
      "Iter 210 (TBPTT): Loss 1130466.8750 | MSE 1130.37 | VarMean 9475.8\n",
      "Iter 220 (TBPTT): Loss 498020.9375 | MSE 497.99 | VarMean 3253.9\n",
      "Iter 230 (TBPTT): Loss 599082.1250 | MSE 599.05 | VarMean 3420.4\n",
      "Iter 240 (TBPTT): Loss 229222.4531 | MSE 229.22 | VarMean 474.3\n",
      "Iter 250 (TBPTT): Loss 238985.7188 | MSE 238.98 | VarMean 376.3\n",
      "\n",
      "--- VALIDATION: MSE 225.8812 ---\n",
      "Iter 260 (TBPTT): Loss 247646.9062 | MSE 247.64 | VarMean 343.2\n",
      "Iter 270 (TBPTT): Loss 275512.6875 | MSE 275.50 | VarMean 837.0\n",
      "Iter 280 (TBPTT): Loss 430878.4375 | MSE 430.85 | VarMean 2165.0\n",
      "Iter 290 (TBPTT): Loss 191953.9375 | MSE 191.95 | VarMean 188.4\n",
      "Iter 300 (TBPTT): Loss 396721.1875 | MSE 396.70 | VarMean 1979.5\n",
      "\n",
      "--- VALIDATION: MSE 200.7133 ---\n",
      "Iter 310 (TBPTT): Loss 198396.7969 | MSE 198.39 | VarMean 201.8\n",
      "Iter 320 (TBPTT): Loss 1183732.0000 | MSE 1183.68 | VarMean 5166.1\n",
      "Iter 330 (TBPTT): Loss 533148.8125 | MSE 533.13 | VarMean 1444.2\n",
      "Iter 340 (TBPTT): Loss 636680.8750 | MSE 636.64 | VarMean 4357.7\n",
      "Iter 350 (TBPTT): Loss 688854.4375 | MSE 688.80 | VarMean 4821.4\n",
      "\n",
      "--- VALIDATION: MSE 150.0769 ---\n",
      "Iter 360 (TBPTT): Loss 405717.4062 | MSE 405.70 | VarMean 1610.9\n",
      "Iter 370 (TBPTT): Loss 195668.2031 | MSE 195.66 | VarMean 207.9\n",
      "Iter 380 (TBPTT): Loss 295880.7188 | MSE 295.87 | VarMean 821.0\n",
      "Iter 390 (TBPTT): Loss 799635.1875 | MSE 799.60 | VarMean 3570.1\n",
      "Iter 400 (TBPTT): Loss 3938400.0000 | MSE 3938.28 | VarMean 12214.1\n",
      "\n",
      "--- VALIDATION: MSE 158.6477 ---\n",
      "Iter 410 (TBPTT): Loss 362119.6875 | MSE 362.10 | VarMean 1544.3\n",
      "Iter 420 (TBPTT): Loss 269797.9375 | MSE 269.79 | VarMean 526.4\n",
      "Iter 430 (TBPTT): Loss 194371.2500 | MSE 194.37 | VarMean 182.8\n",
      "Iter 440 (TBPTT): Loss 421106.7188 | MSE 421.09 | VarMean 1142.6\n",
      "Iter 450 (TBPTT): Loss 202348.4844 | MSE 202.34 | VarMean 178.1\n",
      "\n",
      "--- VALIDATION: MSE 234.7662 ---\n",
      "Iter 460 (TBPTT): Loss 666455.9375 | MSE 666.42 | VarMean 3468.4\n",
      "Iter 470 (TBPTT): Loss 324558.2812 | MSE 324.55 | VarMean 1106.9\n",
      "Iter 480 (TBPTT): Loss 2483124.0000 | MSE 2483.06 | VarMean 6489.4\n",
      "Iter 490 (TBPTT): Loss 668952.1875 | MSE 668.91 | VarMean 3821.9\n",
      "Iter 500 (TBPTT): Loss 492867.3438 | MSE 492.83 | VarMean 3110.7\n",
      "\n",
      "--- VALIDATION: MSE 247.2886 ---\n",
      "Iter 510 (TBPTT): Loss 352306.0312 | MSE 352.30 | VarMean 568.1\n",
      "Iter 520 (TBPTT): Loss 921007.5000 | MSE 920.93 | VarMean 7367.5\n",
      "Iter 530 (TBPTT): Loss 206896.2188 | MSE 206.89 | VarMean 198.0\n",
      "Iter 540 (TBPTT): Loss 160045.7031 | MSE 160.04 | VarMean 181.7\n",
      "Iter 550 (TBPTT): Loss 7247687.0000 | MSE 7247.60 | VarMean 8678.6\n",
      "\n",
      "--- VALIDATION: MSE 195.7463 ---\n",
      "Iter 560 (TBPTT): Loss 1327469.0000 | MSE 1327.42 | VarMean 5190.1\n",
      "Iter 570 (TBPTT): Loss 873878.0000 | MSE 873.81 | VarMean 6112.2\n",
      "Iter 580 (TBPTT): Loss 19925802.0000 | MSE 19925.68 | VarMean 11793.9\n",
      "Iter 590 (TBPTT): Loss 235435.2812 | MSE 235.43 | VarMean 404.8\n",
      "Iter 600 (TBPTT): Loss 2992578.5000 | MSE 2992.52 | VarMean 5313.9\n",
      "\n",
      "--- VALIDATION: MSE 227.3634 ---\n",
      "Iter 610 (TBPTT): Loss 297405.0938 | MSE 297.39 | VarMean 1249.5\n",
      "Iter 620 (TBPTT): Loss 495380.0000 | MSE 495.35 | VarMean 3109.8\n",
      "Iter 630 (TBPTT): Loss 680215.4375 | MSE 680.17 | VarMean 4062.2\n",
      "Iter 640 (TBPTT): Loss 224422.0469 | MSE 224.42 | VarMean 185.0\n",
      "Iter 650 (TBPTT): Loss 10274620.0000 | MSE 10274.54 | VarMean 8350.0\n",
      "\n",
      "--- VALIDATION: MSE 155.5201 ---\n",
      "Iter 660 (TBPTT): Loss 206708.6562 | MSE 206.70 | VarMean 181.6\n",
      "Iter 670 (TBPTT): Loss 316647.2500 | MSE 316.64 | VarMean 752.6\n",
      "Iter 680 (TBPTT): Loss 391577.9375 | MSE 391.56 | VarMean 1566.9\n",
      "Iter 690 (TBPTT): Loss 201378.0469 | MSE 201.37 | VarMean 181.9\n",
      "Iter 700 (TBPTT): Loss 1764155.7500 | MSE 1764.10 | VarMean 5298.2\n",
      "\n",
      "--- VALIDATION: MSE 230.1943 ---\n",
      "Iter 710 (TBPTT): Loss 971753.6250 | MSE 971.68 | VarMean 7449.9\n",
      "Iter 720 (TBPTT): Loss 9055869.0000 | MSE 9055.47 | VarMean 40056.6\n",
      "Iter 730 (TBPTT): Loss 7210083.5000 | MSE 7209.99 | VarMean 9335.0\n",
      "Iter 740 (TBPTT): Loss 444654.3125 | MSE 444.63 | VarMean 2301.3\n",
      "Iter 750 (TBPTT): Loss 262090.4219 | MSE 262.09 | VarMean 203.1\n",
      "\n",
      "--- VALIDATION: MSE 206.9566 ---\n",
      "Iter 760 (TBPTT): Loss 232248.8906 | MSE 232.24 | VarMean 367.7\n",
      "Iter 770 (TBPTT): Loss 216947.5469 | MSE 216.94 | VarMean 196.2\n",
      "Iter 780 (TBPTT): Loss 10447495.0000 | MSE 10447.39 | VarMean 10286.1\n",
      "Iter 790 (TBPTT): Loss 191562.1719 | MSE 191.56 | VarMean 195.0\n",
      "Iter 800 (TBPTT): Loss 668448.5000 | MSE 668.41 | VarMean 3277.3\n",
      "\n",
      "--- VALIDATION: MSE 322.4233 ---\n",
      "Iter 810 (TBPTT): Loss 546721.0625 | MSE 546.69 | VarMean 3159.9\n",
      "Iter 820 (TBPTT): Loss 720543.6250 | MSE 720.52 | VarMean 2513.6\n",
      "Iter 830 (TBPTT): Loss 4740014.0000 | MSE 4739.93 | VarMean 8112.3\n",
      "Iter 840 (TBPTT): Loss 977715.3750 | MSE 977.64 | VarMean 7389.4\n",
      "Iter 850 (TBPTT): Loss 5594352.0000 | MSE 5594.22 | VarMean 12708.8\n",
      "\n",
      "--- VALIDATION: MSE 189.6641 ---\n",
      "Iter 860 (TBPTT): Loss 443660.5000 | MSE 443.63 | VarMean 2469.7\n",
      "Iter 870 (TBPTT): Loss 227153.3750 | MSE 227.15 | VarMean 564.3\n",
      "Iter 880 (TBPTT): Loss 361291.5312 | MSE 361.28 | VarMean 1088.0\n",
      "Iter 890 (TBPTT): Loss 856212.5000 | MSE 856.16 | VarMean 5479.8\n",
      "Iter 900 (TBPTT): Loss 151740.2812 | MSE 151.74 | VarMean 180.2\n",
      "\n",
      "--- VALIDATION: MSE 186.8191 ---\n",
      "Iter 910 (TBPTT): Loss 204117.8594 | MSE 204.11 | VarMean 181.7\n",
      "Iter 920 (TBPTT): Loss 214662.0156 | MSE 214.66 | VarMean 223.8\n",
      "Iter 930 (TBPTT): Loss 650750.7500 | MSE 650.73 | VarMean 2284.4\n",
      "Iter 940 (TBPTT): Loss 565184.1250 | MSE 565.15 | VarMean 3340.6\n",
      "Iter 950 (TBPTT): Loss 447075.9375 | MSE 447.05 | VarMean 1894.8\n",
      "\n",
      "--- VALIDATION: MSE 182.2760 ---\n",
      "Iter 960 (TBPTT): Loss 272919.3125 | MSE 272.91 | VarMean 646.3\n",
      "Iter 970 (TBPTT): Loss 511642.2188 | MSE 511.61 | VarMean 2959.3\n",
      "Iter 980 (TBPTT): Loss 2741723.5000 | MSE 2741.61 | VarMean 11427.2\n",
      "Iter 990 (TBPTT): Loss 192045.5469 | MSE 192.04 | VarMean 194.0\n",
      "Iter 1000 (TBPTT): Loss 196177.1719 | MSE 196.17 | VarMean 199.9\n",
      "\n",
      "--- VALIDATION: MSE 216.0760 ---\n",
      "Iter 1010 (TBPTT): Loss 215409.2812 | MSE 215.41 | VarMean 205.4\n",
      "Iter 1020 (TBPTT): Loss 613428.2500 | MSE 613.40 | VarMean 2253.8\n",
      "Iter 1030 (TBPTT): Loss 250548.4531 | MSE 250.54 | VarMean 1016.1\n",
      "Iter 1040 (TBPTT): Loss 10089453.0000 | MSE 10089.37 | VarMean 7790.1\n",
      "Iter 1050 (TBPTT): Loss 269932.1875 | MSE 269.92 | VarMean 776.0\n",
      "\n",
      "--- VALIDATION: MSE 146.8866 ---\n",
      "Iter 1060 (TBPTT): Loss 855177.1875 | MSE 855.11 | VarMean 6727.5\n",
      "Iter 1070 (TBPTT): Loss 371008.6875 | MSE 371.00 | VarMean 913.5\n",
      "Iter 1080 (TBPTT): Loss 176160.7969 | MSE 176.16 | VarMean 204.3\n",
      "Iter 1090 (TBPTT): Loss 2506017.7500 | MSE 2505.95 | VarMean 6446.4\n",
      "Iter 1100 (TBPTT): Loss 2751552.0000 | MSE 2751.48 | VarMean 6937.2\n",
      "\n",
      "--- VALIDATION: MSE 189.8366 ---\n",
      "Iter 1110 (TBPTT): Loss 15564095.0000 | MSE 15563.97 | VarMean 12367.9\n",
      "Iter 1120 (TBPTT): Loss 222890.9375 | MSE 222.89 | VarMean 182.8\n",
      "Iter 1130 (TBPTT): Loss 206324.7969 | MSE 206.32 | VarMean 219.1\n",
      "Iter 1140 (TBPTT): Loss 281556.6875 | MSE 281.55 | VarMean 386.1\n",
      "Iter 1150 (TBPTT): Loss 772888.6250 | MSE 772.82 | VarMean 6564.9\n",
      "\n",
      "--- VALIDATION: MSE 386.1821 ---\n",
      "Iter 1160 (TBPTT): Loss 264031.2188 | MSE 264.03 | VarMean 414.9\n",
      "Iter 1170 (TBPTT): Loss 224786.0938 | MSE 224.78 | VarMean 423.5\n",
      "Iter 1180 (TBPTT): Loss 689529.8125 | MSE 689.48 | VarMean 4438.7\n",
      "Iter 1190 (TBPTT): Loss 788193.5625 | MSE 788.17 | VarMean 2630.5\n",
      "Iter 1200 (TBPTT): Loss 356894.0938 | MSE 356.87 | VarMean 1737.8\n",
      "\n",
      "--- VALIDATION: MSE 233.0269 ---\n",
      "Iter 1210 (TBPTT): Loss 532580.0000 | MSE 532.55 | VarMean 3240.2\n",
      "Iter 1220 (TBPTT): Loss 1641559.8750 | MSE 1641.48 | VarMean 7790.6\n",
      "Iter 1230 (TBPTT): Loss 3553511.0000 | MSE 3553.35 | VarMean 15528.2\n",
      "Iter 1240 (TBPTT): Loss 198077.7031 | MSE 198.07 | VarMean 188.7\n",
      "Iter 1250 (TBPTT): Loss 388632.1250 | MSE 388.61 | VarMean 2348.9\n",
      "\n",
      "--- VALIDATION: MSE 241.2139 ---\n",
      "Iter 1260 (TBPTT): Loss 184451.1875 | MSE 184.45 | VarMean 152.1\n",
      "Iter 1270 (TBPTT): Loss 377635.6875 | MSE 377.61 | VarMean 2135.8\n",
      "Iter 1280 (TBPTT): Loss 436041.6875 | MSE 436.03 | VarMean 831.6\n",
      "Iter 1290 (TBPTT): Loss 219074.1719 | MSE 219.07 | VarMean 190.5\n",
      "Iter 1300 (TBPTT): Loss 457260.2500 | MSE 457.23 | VarMean 2851.6\n",
      "\n",
      "--- VALIDATION: MSE 165.7591 ---\n",
      "Iter 1310 (TBPTT): Loss 270263.6562 | MSE 270.25 | VarMean 704.6\n",
      "Iter 1320 (TBPTT): Loss 241354.9375 | MSE 241.35 | VarMean 270.5\n",
      "Iter 1330 (TBPTT): Loss 5130045.0000 | MSE 5129.93 | VarMean 11410.0\n",
      "Iter 1340 (TBPTT): Loss 281084.7188 | MSE 281.08 | VarMean 672.6\n",
      "Iter 1350 (TBPTT): Loss 1907829.7500 | MSE 1907.73 | VarMean 10154.1\n",
      "\n",
      "--- VALIDATION: MSE 223.0119 ---\n",
      "Iter 1360 (TBPTT): Loss 210737.3125 | MSE 210.73 | VarMean 259.3\n",
      "Iter 1370 (TBPTT): Loss 364946.7500 | MSE 364.93 | VarMean 1641.4\n",
      "Iter 1380 (TBPTT): Loss 290503.2188 | MSE 290.49 | VarMean 976.4\n",
      "Iter 1390 (TBPTT): Loss 7347137.5000 | MSE 7347.00 | VarMean 14002.9\n",
      "Iter 1400 (TBPTT): Loss 183133.7031 | MSE 183.13 | VarMean 207.1\n",
      "\n",
      "--- VALIDATION: MSE 367.2428 ---\n",
      "Iter 1410 (TBPTT): Loss 557512.1875 | MSE 557.50 | VarMean 1310.5\n",
      "Iter 1420 (TBPTT): Loss 202025.7031 | MSE 202.02 | VarMean 194.0\n",
      "Iter 1430 (TBPTT): Loss 590148.9375 | MSE 590.13 | VarMean 1753.1\n",
      "Iter 1440 (TBPTT): Loss 155493.8281 | MSE 155.49 | VarMean 181.4\n",
      "Iter 1450 (TBPTT): Loss 2043839.1250 | MSE 2043.72 | VarMean 11995.5\n",
      "\n",
      "--- VALIDATION: MSE 147.9222 ---\n",
      "Iter 1460 (TBPTT): Loss 2914226.7500 | MSE 2914.14 | VarMean 8946.8\n",
      "Iter 1470 (TBPTT): Loss 200042.6875 | MSE 200.04 | VarMean 196.2\n",
      "Iter 1480 (TBPTT): Loss 696819.4375 | MSE 696.81 | VarMean 1126.0\n",
      "Iter 1490 (TBPTT): Loss 2150278.2500 | MSE 2150.25 | VarMean 2305.1\n",
      "Iter 1500 (TBPTT): Loss 624316.6875 | MSE 624.28 | VarMean 3716.0\n",
      "\n",
      "--- VALIDATION: MSE 273.2105 ---\n",
      "Iter 1510 (TBPTT): Loss 344426.6562 | MSE 344.40 | VarMean 1969.1\n",
      "Iter 1520 (TBPTT): Loss 337464.3125 | MSE 337.45 | VarMean 1227.3\n",
      "Iter 1530 (TBPTT): Loss 1730517.6250 | MSE 1730.44 | VarMean 7291.0\n",
      "Iter 1540 (TBPTT): Loss 279491.6250 | MSE 279.48 | VarMean 774.3\n",
      "Iter 1550 (TBPTT): Loss 500817.0000 | MSE 500.79 | VarMean 2911.8\n",
      "\n",
      "--- VALIDATION: MSE 193.8254 ---\n",
      "Iter 1560 (TBPTT): Loss 7087640.5000 | MSE 7087.56 | VarMean 8026.6\n",
      "Iter 1570 (TBPTT): Loss 1318531.6250 | MSE 1318.48 | VarMean 5217.4\n",
      "Iter 1580 (TBPTT): Loss 335375.6562 | MSE 335.36 | VarMean 1094.4\n",
      "Iter 1590 (TBPTT): Loss 1171658.5000 | MSE 1171.60 | VarMean 5405.9\n",
      "Iter 1600 (TBPTT): Loss 243725.6719 | MSE 243.72 | VarMean 723.4\n",
      "\n",
      "--- VALIDATION: MSE 443.9159 ---\n",
      "Iter 1610 (TBPTT): Loss 896813.6875 | MSE 896.77 | VarMean 4218.3\n",
      "Iter 1620 (TBPTT): Loss 205860.5000 | MSE 205.86 | VarMean 178.7\n",
      "Iter 1630 (TBPTT): Loss 522141.3438 | MSE 522.11 | VarMean 3020.1\n",
      "Iter 1640 (TBPTT): Loss 470924.4062 | MSE 470.90 | VarMean 2540.8\n",
      "Iter 1650 (TBPTT): Loss 1295038.0000 | MSE 1294.98 | VarMean 5313.8\n",
      "\n",
      "--- VALIDATION: MSE 172.8558 ---\n",
      "Iter 1660 (TBPTT): Loss 354592.0625 | MSE 354.58 | VarMean 1427.7\n",
      "Iter 1670 (TBPTT): Loss 213362.5625 | MSE 213.36 | VarMean 183.3\n",
      "Iter 1680 (TBPTT): Loss 565099.6250 | MSE 565.07 | VarMean 3103.2\n",
      "Iter 1690 (TBPTT): Loss 261946.4688 | MSE 261.94 | VarMean 547.6\n",
      "Iter 1700 (TBPTT): Loss 240725.5000 | MSE 240.72 | VarMean 302.7\n",
      "\n",
      "--- VALIDATION: MSE 232.8164 ---\n",
      "Iter 1710 (TBPTT): Loss 360907.0625 | MSE 360.89 | VarMean 1123.2\n",
      "Iter 1720 (TBPTT): Loss 222242.5000 | MSE 222.24 | VarMean 296.5\n",
      "Iter 1730 (TBPTT): Loss 816429.1875 | MSE 816.37 | VarMean 5955.9\n",
      "Iter 1740 (TBPTT): Loss 424616.9062 | MSE 424.59 | VarMean 2527.7\n",
      "Iter 1750 (TBPTT): Loss 1634218.7500 | MSE 1634.18 | VarMean 3573.0\n",
      "\n",
      "--- VALIDATION: MSE 286.2403 ---\n",
      "Iter 1760 (TBPTT): Loss 1425326.8750 | MSE 1425.25 | VarMean 7685.7\n",
      "Iter 1770 (TBPTT): Loss 1126684.5000 | MSE 1126.64 | VarMean 4030.8\n",
      "Iter 1780 (TBPTT): Loss 229617.5000 | MSE 229.61 | VarMean 335.3\n",
      "Iter 1790 (TBPTT): Loss 609237.6250 | MSE 609.20 | VarMean 3625.4\n",
      "Iter 1800 (TBPTT): Loss 205092.4375 | MSE 205.09 | VarMean 206.1\n",
      "\n",
      "--- VALIDATION: MSE 225.2235 ---\n",
      "Iter 1810 (TBPTT): Loss 463822.5625 | MSE 463.81 | VarMean 1307.7\n",
      "Iter 1820 (TBPTT): Loss 550931.1250 | MSE 550.89 | VarMean 3522.8\n",
      "Iter 1830 (TBPTT): Loss 212104.6875 | MSE 212.10 | VarMean 212.6\n",
      "Iter 1840 (TBPTT): Loss 200228.1875 | MSE 200.22 | VarMean 361.0\n",
      "Iter 1850 (TBPTT): Loss 795720.3125 | MSE 795.65 | VarMean 6472.4\n",
      "\n",
      "--- VALIDATION: MSE 356.2975 ---\n",
      "Iter 1860 (TBPTT): Loss 834914.8125 | MSE 834.85 | VarMean 6400.6\n",
      "Iter 1870 (TBPTT): Loss 1470010.2500 | MSE 1469.95 | VarMean 5380.4\n",
      "Iter 1880 (TBPTT): Loss 498120.9688 | MSE 498.10 | VarMean 1484.5\n",
      "Iter 1890 (TBPTT): Loss 792105.6875 | MSE 792.07 | VarMean 3225.3\n",
      "Iter 1900 (TBPTT): Loss 238212.5625 | MSE 238.20 | VarMean 726.7\n",
      "\n",
      "--- VALIDATION: MSE 192.4712 ---\n",
      "Iter 1910 (TBPTT): Loss 224756.1250 | MSE 224.75 | VarMean 788.4\n",
      "Iter 1920 (TBPTT): Loss 852989.3125 | MSE 852.92 | VarMean 6382.2\n",
      "Iter 1930 (TBPTT): Loss 546800.8750 | MSE 546.76 | VarMean 3900.5\n",
      "Iter 1940 (TBPTT): Loss 4248527.0000 | MSE 4248.39 | VarMean 13611.2\n",
      "Iter 1950 (TBPTT): Loss 221636.8750 | MSE 221.63 | VarMean 588.5\n",
      "\n",
      "--- VALIDATION: MSE 220.2503 ---\n",
      "Iter 1960 (TBPTT): Loss 681129.6250 | MSE 681.09 | VarMean 3977.7\n",
      "Iter 1970 (TBPTT): Loss 1202430.8750 | MSE 1202.40 | VarMean 3107.0\n",
      "Iter 1980 (TBPTT): Loss 284268.1562 | MSE 284.26 | VarMean 742.3\n",
      "Iter 1990 (TBPTT): Loss 14741066.0000 | MSE 14740.96 | VarMean 9999.1\n",
      "Iter 2000 (TBPTT): Loss 687398.1250 | MSE 687.34 | VarMean 5707.0\n",
      "\n",
      "--- VALIDATION: MSE 381.9756 ---\n",
      "Iter 2010 (TBPTT): Loss 544957.0625 | MSE 544.93 | VarMean 2762.0\n",
      "Iter 2020 (TBPTT): Loss 209898.0000 | MSE 209.89 | VarMean 189.7\n",
      "Iter 2030 (TBPTT): Loss 233984.1406 | MSE 233.97 | VarMean 795.9\n",
      "Iter 2040 (TBPTT): Loss 255092.7500 | MSE 255.08 | VarMean 603.5\n",
      "Iter 2050 (TBPTT): Loss 712127.3750 | MSE 712.09 | VarMean 3034.8\n",
      "\n",
      "--- VALIDATION: MSE 227.8362 ---\n",
      "Iter 2060 (TBPTT): Loss 1147197.8750 | MSE 1147.15 | VarMean 4720.0\n",
      "Iter 2070 (TBPTT): Loss 1342077.3750 | MSE 1342.03 | VarMean 4902.8\n",
      "Iter 2080 (TBPTT): Loss 4561672.0000 | MSE 4561.60 | VarMean 7352.6\n",
      "Iter 2090 (TBPTT): Loss 957237.8750 | MSE 957.19 | VarMean 4941.0\n",
      "Iter 2100 (TBPTT): Loss 1646508.2500 | MSE 1646.42 | VarMean 8692.0\n",
      "\n",
      "--- VALIDATION: MSE 436.2517 ---\n",
      "Iter 2110 (TBPTT): Loss 171765.8594 | MSE 171.76 | VarMean 189.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "# P≈ôedpokl√°d√°m, ≈æe funkce train_BayesianKalmanNet_TBPTT je definov√°na (z p≈ôedchoz√≠ho kroku)\n",
    "# from utils import train_BayesianKalmanNet_TBPTT  <-- pokud ji m√°te v souboru\n",
    "\n",
    "# --- 1. DEFINICE CURRICULA ---\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Stabilizace (Kr√°tk√© sekvence - staƒç√≠ standardn√≠ tr√©nink)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,\n",
    "        'iters': 400,\n",
    "        'lr': 1e-3,\n",
    "        'lambda_mse': 1000.0,\n",
    "        'clip_grad': 1.0,\n",
    "        'use_tbptt': False,   # Tady TBPTT nepot≈ôebujeme\n",
    "        'tbptt_steps': None\n",
    "    },\n",
    "    # F√ÅZE 2: Prodlou≈æen√≠ (St≈ôedn√≠ sekvence - ZAP√çN√ÅME TBPTT)\n",
    "    # TBPTT n√°m umo≈æn√≠ tr√©novat na d√©lce 100, ani≈æ by explodovala variance\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100,\n",
    "        'iters': 3000,        # V√≠ce iterac√≠, proto≈æe TBPTT dƒõl√° men≈°√≠ kroky\n",
    "        'lr': 5e-5,           # Opatrn√© uƒçen√≠\n",
    "        'lambda_mse': 1000.0, # St√°le dr≈æ√≠me prioritu na p≈ôesnost\n",
    "        'clip_grad': 0.2,\n",
    "        'use_tbptt': True,    # <--- ZAPNUTO\n",
    "        'tbptt_steps': 20     # Uƒç√≠me se na oknech d√©lky 20\n",
    "    },\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 300,\n",
    "        'iters': 3000,        # V√≠ce iterac√≠, proto≈æe TBPTT dƒõl√° men≈°√≠ kroky\n",
    "        'lr': 1e-5,           # Opatrn√© uƒçen√≠\n",
    "        'lambda_mse': 1000.0, # St√°le dr≈æ√≠me prioritu na p≈ôesnost\n",
    "        'clip_grad': 0.1,\n",
    "        'use_tbptt': True,    # <--- ZAPNUTO\n",
    "        'tbptt_steps': 50     # Uƒç√≠me se na oknech d√©lky 20\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 2. INICIALIZACE MODELU ---\n",
    "print(\"=== INICIALIZACE BKN MODELU ===\")\n",
    "# Pou≈æ√≠v√°me v√°≈° osvƒõdƒçen√Ω setup s vy≈°≈°√≠m hidden size\n",
    "state_knet2 = TAN.StateBayesianKalmanNetTAN(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=12,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        init_max_dropout=0.3, # Zdrav√Ω dropout pro exploration\n",
    "        init_min_dropout=0.1    \n",
    ").to(device)\n",
    "\n",
    "# --- 3. CURRICULUM LOOP ---\n",
    "for phase in curriculum_schedule:\n",
    "    phase_id = phase['phase_id']\n",
    "    seq_len = phase['seq_len']\n",
    "\n",
    "    # Kontrola dostupnosti dat\n",
    "    if phase_id not in datasets_cache:\n",
    "        print(f\"‚ö†Ô∏è Skipping Phase {phase_id}: Data not in cache.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üöÄ START PHASE {phase_id}: SeqLen {seq_len} | LR {phase['lr']} | TBPTT: {phase['use_tbptt']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_loader_phase = datasets_cache[phase_id][0]\n",
    "    val_loader_phase = datasets_cache[phase_id][1]\n",
    "    \n",
    "    # Rozhodov√°n√≠ podle typu tr√©ninku\n",
    "    if phase['use_tbptt']:\n",
    "        # === A) TBPTT Tr√©nink (pro dlouh√© sekvence) ===\n",
    "        print(f\"   -> Using TBPTT with window size {phase['tbptt_steps']}\")\n",
    "        \n",
    "        # Zkontrolujeme, zda model m√° metodu detach_hidden (prevence p√°du)\n",
    "        if not hasattr(state_knet2, 'detach_hidden'):\n",
    "            raise AttributeError(\"Modelu chyb√≠ metoda 'detach_hidden()'! P≈ôidejte ji do t≈ô√≠dy StateBayesianKalmanNetTAN.\")\n",
    "\n",
    "        result = train_BayesianKalmanNet_TBPTT(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=10,               # V TBPTT m≈Ø≈æeme m√≠t men≈°√≠ J, proto≈æe se pr≈Ømƒõruje ƒçastƒõji\n",
    "            tbptt_steps=phase['tbptt_steps'],\n",
    "            validation_period=50,       # Validace ka≈æd√Ωch 50 oken\n",
    "            logging_period=10,\n",
    "            lambda_mse=phase['lambda_mse']\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # === B) Standardn√≠ Hybrid Tr√©nink (pro kr√°tk√© sekvence) ===\n",
    "        print(f\"   -> Using Standard Hybrid Training\")\n",
    "        result = train_BayesianKalmanNet_Hybrid(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=10,\n",
    "            validation_period=20,\n",
    "            logging_period=10,\n",
    "            warmup_iterations=0,\n",
    "            weight_decay_=1e-5,\n",
    "            lambda_mse=phase['lambda_mse']\n",
    "        )\n",
    "    \n",
    "    # Ulo≈æen√≠ a kontrola\n",
    "    save_path = f\"bkn_curriculum_phase{phase_id}_len{seq_len}.pth\"\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"‚úÖ F√°ze {phase_id} dokonƒçena. Model ulo≈æen do: {save_path}\")\n",
    "    \n",
    "    # Jednoduch√° kontrola proti divergenci (pokud best_val neexistuje nebo je inf)\n",
    "    # Pozn: TBPTT funkce vrac√≠ slovn√≠k, hybrid taky, ujist√≠me se, ≈æe kl√≠ƒçe sed√≠\n",
    "    # TBPTT moment√°lnƒõ vrac√≠ jen {'final_model': model}, pro pokroƒçilou kontrolu by to chtƒõlo vr√°tit i loss.\n",
    "    # Ale pro teƒè to nech√°me bƒõ≈æet d√°l.\n",
    "\n",
    "print(\"\\nüéâ Cel√Ω tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'most_consistent_and_accurate_bknet.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ec4ec",
   "metadata": {},
   "source": [
    "# Test na realne trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d79a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === KONFIGURACE MC ===\n",
    "MC_ITERATIONS = 10  \n",
    "J_SAMPLES = 30      \n",
    "PLOT_PER_ITERATION = True  # True = vykresl√≠ grafy (prvn√≠ a posledn√≠ bƒõh), False = ≈æ√°dn√© grafy\n",
    "\n",
    "# P≈ô√≠prava dat\n",
    "real_traj_np = souradniceGNSS[:2, :].T \n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "\n",
    "# --- POMOCN√Å FUNKCE ---\n",
    "def get_reference_test_set(system, real_traj_tensor):\n",
    "    device = system.Ex0.device\n",
    "    hB_np = mat_data['hB']\n",
    "    real_hB_tensor = torch.from_numpy(hB_np).float().to(device).view(-1)\n",
    "\n",
    "    pos_full = real_traj_tensor.clone().to(device)\n",
    "    deltas = pos_full[1:] - pos_full[:-1] \n",
    "    last_vel = deltas[-1:]\n",
    "    velocities = torch.cat([deltas, last_vel], dim=0) \n",
    "    \n",
    "    x_traj_flat = torch.cat([pos_full, velocities], dim=1) \n",
    "    y_traj_flat = system.measure(x_traj_flat) \n",
    "    \n",
    "    seq_len = x_traj_flat.shape[0]\n",
    "    y_traj_flat[:, 0] = real_hB_tensor[:seq_len] \n",
    "    \n",
    "    x_ref = x_traj_flat.unsqueeze(0) \n",
    "    y_ref = y_traj_flat.unsqueeze(0) \n",
    "    \n",
    "    return x_ref, y_ref\n",
    "\n",
    "print(f\"=== SPU≈†TƒöN√ç MONTE CARLO SIMULACE ({MC_ITERATIONS} bƒõh≈Ø) ===\")\n",
    "print(f\"Modely: BKN (J={J_SAMPLES}) vs. UKF vs. PF\")\n",
    "\n",
    "# 1. P≈ô√≠prava Ground Truth\n",
    "x_ref_tensor_static, _ = get_reference_test_set(system_model, real_traj_tensor)\n",
    "x_gt = x_ref_tensor_static.squeeze().cpu().numpy()\n",
    "seq_len = x_gt.shape[0]\n",
    "\n",
    "# 2. Inicializace metrik\n",
    "detailed_results = []\n",
    "agg_metrics = {\n",
    "    \"BKN\": {\"mse\": [], \"pos\": []}, \n",
    "    \"UKF\": {\"mse\": [], \"pos\": []}, \n",
    "    \"PF\":  {\"mse\": [], \"pos\": []}\n",
    "}\n",
    "\n",
    "state_knet2.train() \n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA ---\n",
    "for i in tqdm(range(MC_ITERATIONS), desc=\"Simulace\"):\n",
    "    \n",
    "    # A) Nov√© mƒõ≈ôen√≠ (n√°hodn√Ω ≈°um)\n",
    "    _, y_ref_tensor = get_reference_test_set(system_model, real_traj_tensor)\n",
    "    y_meas = y_ref_tensor \n",
    "    \n",
    "    # B) Inference: BKN Ensemble\n",
    "    with torch.no_grad():\n",
    "        initial_state = x_ref_tensor_static[:, 0, :] \n",
    "        ensemble_preds = []\n",
    "        \n",
    "        for j in range(J_SAMPLES):\n",
    "            state_knet2.reset(batch_size=1, initial_state=initial_state)\n",
    "            trajectory_preds = []\n",
    "            for t in range(1, seq_len):\n",
    "                y_t = y_meas[:, t, :]\n",
    "                x_est, _ = state_knet2.step(y_t)\n",
    "                trajectory_preds.append(x_est)\n",
    "            \n",
    "            trajectory_tensor = torch.stack(trajectory_preds, dim=1) \n",
    "            full_traj = torch.cat([initial_state.unsqueeze(1), trajectory_tensor], dim=1)\n",
    "            ensemble_preds.append(full_traj)\n",
    "            \n",
    "        ensemble_stack = torch.stack(ensemble_preds, dim=0) \n",
    "        x_est_bkn_tensor = ensemble_stack.mean(dim=0).squeeze(0) \n",
    "        cov_diag_bkn = ensemble_stack.var(dim=0).squeeze(0) + 1e-9 \n",
    "        \n",
    "        x_est_bkn = x_est_bkn_tensor.cpu().numpy()\n",
    "        std_bkn = torch.sqrt(cov_diag_bkn).cpu().numpy() \n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    y_for_filters = y_ref_tensor.squeeze(0) \n",
    "    true_init_state = x_ref_tensor_static[0, 0, :] \n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(y_seq=y_for_filters, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=5000) \n",
    "    pf_res = pf.process_sequence(y_seq=y_for_filters, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # D) V√Ωpoƒçet chyb\n",
    "    def calc_errors(x_est, x_gt):\n",
    "        diff = x_est - x_gt\n",
    "        mse = np.mean(diff**2)\n",
    "        pos_err = np.mean(np.sqrt(diff[:, 0]**2 + diff[:, 1]**2))\n",
    "        return mse, pos_err\n",
    "\n",
    "    mse_bkn, pos_bkn = calc_errors(x_est_bkn, x_gt)\n",
    "    mse_ukf, pos_ukf = calc_errors(x_est_ukf, x_gt)\n",
    "    mse_pf, pos_pf = calc_errors(x_est_pf, x_gt)\n",
    "    \n",
    "    agg_metrics[\"BKN\"][\"mse\"].append(mse_bkn); agg_metrics[\"BKN\"][\"pos\"].append(pos_bkn)\n",
    "    agg_metrics[\"UKF\"][\"mse\"].append(mse_ukf); agg_metrics[\"UKF\"][\"pos\"].append(pos_ukf)\n",
    "    agg_metrics[\"PF\"][\"mse\"].append(mse_pf);   agg_metrics[\"PF\"][\"pos\"].append(pos_pf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"BKN_MSE\": mse_bkn, \"UKF_MSE\": mse_ukf, \"PF_MSE\": mse_pf,\n",
    "        \"BKN_Pos\": pos_bkn, \"UKF_Pos\": pos_ukf, \"PF_Pos\": pos_pf\n",
    "    })\n",
    "    \n",
    "    # E) VYKRESLEN√ç (Upraveno pro X i Y s neurƒçitost√≠)\n",
    "    # Vykresl√≠me jen prvn√≠ a posledn√≠ bƒõh, abychom nezahltili v√Ωstup, pokud je PLOT_PER_ITERATION True\n",
    "    if PLOT_PER_ITERATION and (i == 0 or i == MC_ITERATIONS-1):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(24, 6)) # 3 grafy vedle sebe\n",
    "        \n",
    "        # 1. Graf: Trajektorie v mapƒõ (XY)\n",
    "        ax[0].plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='GT')\n",
    "        ax[0].plot(x_est_bkn[:, 0], x_est_bkn[:, 1], 'g-', linewidth=2, label=f'BKN (Err: {pos_bkn:.1f}m)')\n",
    "        ax[0].plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_ukf:.1f}m)')\n",
    "        ax[0].set_title(f\"Run {i+1}: Map Trajectory (XY)\")\n",
    "        ax[0].set_xlabel(\"X [m]\")\n",
    "        ax[0].set_ylabel(\"Y [m]\")\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "        ax[0].axis('equal')\n",
    "        \n",
    "        # Spoleƒçn√° ƒçasov√° osa\n",
    "        time_steps = np.arange(seq_len)\n",
    "\n",
    "        # 2. Graf: V√Ωvoj X v ƒçase s neurƒçitost√≠\n",
    "        ax[1].plot(time_steps, x_gt[:, 0], 'k-', label='GT X')\n",
    "        ax[1].plot(time_steps, x_est_bkn[:, 0], 'g-', label='BKN X')\n",
    "        # P√°smo neurƒçitosti X (+/- 3 sigma)\n",
    "        ax[1].fill_between(time_steps, \n",
    "                           x_est_bkn[:, 0] - 3*std_bkn[:, 0], \n",
    "                           x_est_bkn[:, 0] + 3*std_bkn[:, 0], \n",
    "                           color='green', alpha=0.2, label='Uncertainty (3$\\sigma$)')\n",
    "        ax[1].set_title(\"BKN Uncertainty: X-axis\")\n",
    "        ax[1].set_ylabel(\"Position X [m]\")\n",
    "        ax[1].set_xlabel(\"Time Step\")\n",
    "        ax[1].legend()\n",
    "        ax[1].grid(True)\n",
    "\n",
    "        # 3. Graf: V√Ωvoj Y v ƒçase s neurƒçitost√≠\n",
    "        ax[2].plot(time_steps, x_gt[:, 1], 'k-', label='GT Y')\n",
    "        ax[2].plot(time_steps, x_est_bkn[:, 1], 'g-', label='BKN Y')\n",
    "        # P√°smo neurƒçitosti Y (+/- 3 sigma)\n",
    "        ax[2].fill_between(time_steps, \n",
    "                           x_est_bkn[:, 1] - 3*std_bkn[:, 1], \n",
    "                           x_est_bkn[:, 1] + 3*std_bkn[:, 1], \n",
    "                           color='green', alpha=0.2, label='Uncertainty (3$\\sigma$)')\n",
    "        ax[2].set_title(\"BKN Uncertainty: Y-axis\")\n",
    "        ax[2].set_ylabel(\"Position Y [m]\")\n",
    "        ax[2].set_xlabel(\"Time Step\")\n",
    "        ax[2].legend()\n",
    "        ax[2].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({MC_ITERATIONS} bƒõh≈Ø)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats_str(key):\n",
    "    m_mse, s_mse = np.mean(agg_metrics[key][\"mse\"]), np.std(agg_metrics[key][\"mse\"])\n",
    "    m_pos, s_pos = np.mean(agg_metrics[key][\"pos\"]), np.std(agg_metrics[key][\"pos\"])\n",
    "    return f\"{m_mse:.1f} ¬± {s_mse:.1f} | {m_pos:.2f} ¬± {s_pos:.2f} m\"\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE':<25} | {'Pos Error':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'BKN (Ensemble)':<15} | {get_stats_str('BKN')}\")\n",
    "print(f\"{'UKF':<15} | {get_stats_str('UKF')}\")\n",
    "print(f\"{'PF':<15} | {get_stats_str('PF')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_metrics[\"BKN\"][\"pos\"], agg_metrics[\"UKF\"][\"pos\"], agg_metrics[\"PF\"][\"pos\"]], \n",
    "            labels=['BKN', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({MC_ITERATIONS} runs)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Pro hezkou tabulku\n",
    "import Filters\n",
    "from tqdm import tqdm\n",
    "real_traj_np = souradniceGNSS[:2, :].T \n",
    "\n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "train_source_tensor = real_traj_tensor[:, :]\n",
    "# --- POMOCN√Å FUNKCE PRO GENEROW√ÅN√ç DAT ---\n",
    "def get_reference_test_set(system, real_traj_tensor, reverse=False):\n",
    "    # O≈ô√≠znut√≠ trajektorie (pokud je pot≈ôeba)\n",
    "    # real_traj_tensor = real_traj_tensor[:1050,:] \n",
    "    \n",
    "    device = system.Ex0.device\n",
    "    \n",
    "    # P≈ôedpoklad: mat_data je glob√°ln√≠ promƒõnn√° s naƒçten√Ωm .mat souborem\n",
    "    # hB_np = mat_data['hB']\n",
    "    # real_hB_tensor = torch.from_numpy(hB_np).float().to(device).view(-1)\n",
    "\n",
    "    pos_full = real_traj_tensor.clone().to(device)\n",
    "    \n",
    "    deltas = pos_full[1:] - pos_full[:-1] \n",
    "    last_vel = deltas[-1:]\n",
    "    velocities = torch.cat([deltas, last_vel], dim=0) # [T, 2]\n",
    "    \n",
    "    x_traj_flat = torch.cat([pos_full, velocities], dim=1) # [T, 4]\n",
    "    \n",
    "    # Generov√°n√≠ mƒõ≈ôen√≠ (s n√°hodn√Ωm ≈°umem uvnit≈ô system.measure)\n",
    "    y_traj_flat = system.measure(x_traj_flat) # [T, 3]\n",
    "    \n",
    "    # Nahrazen√≠ barometru re√°ln√Ωmi daty (pokud je to ≈æ√°douc√≠)\n",
    "    seq_len = x_traj_flat.shape[0]\n",
    "    # Pokud chce≈° simulovat ƒçistƒõ syntetick√Ω ≈°um barometru, tento ≈ô√°dek zakomentuj:\n",
    "    # y_traj_flat[:, 0] = real_hB_tensor[:seq_len] \n",
    "    \n",
    "    x_ref = x_traj_flat.unsqueeze(0) # [1, T, 4]\n",
    "    y_ref = y_traj_flat.unsqueeze(0) # [1, T, 3]\n",
    "    \n",
    "    return x_ref, y_ref\n",
    "\n",
    "\n",
    "# --- KONFIGURACE MC ---\n",
    "MC_ITERATIONS = 10  # Nastav rozumn√© ƒç√≠slo (pro 100 graf≈Ø by to zahltilo notebook)\n",
    "PLOT_PER_ITERATION = True # Zda vykreslovat grafy pro ka≈æd√Ω bƒõh\n",
    "\n",
    "print(f\"=== SPU≈†TƒöN√ç MONTE CARLO SIMULACE ({MC_ITERATIONS} bƒõh≈Ø) ===\")\n",
    "print(\"Modely: KalmanNet vs. UKF vs. PF\")\n",
    "\n",
    "# 1. P≈ô√≠prava Ground Truth (GT)\n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "# Z√≠sk√°me GT stavy (X) jen jednou, proto≈æe trajektorie je fixn√≠\n",
    "# Mƒõ≈ôen√≠ (Y) se bude mƒõnit v ka≈æd√© iteraci kv≈Øli ≈°umu\n",
    "x_ref_tensor_static, _ = get_reference_test_set(system_model, real_traj_tensor)\n",
    "x_gt = x_ref_tensor_static.squeeze().cpu().numpy()\n",
    "seq_len = x_gt.shape[0]\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = [] # Seznam slovn√≠k≈Ø pro DataFrame\n",
    "agg_mse = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "agg_pos = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "\n",
    "# Ujist√≠me se, ≈æe KNet je v eval m√≥du\n",
    "state_knet2.eval()\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA ---\n",
    "for i in tqdm(range(MC_ITERATIONS), desc=\"Simulace\"):\n",
    "    \n",
    "    # A) Generov√°n√≠ nov√©ho mƒõ≈ôen√≠ (s nov√Ωm n√°hodn√Ωm ≈°umem)\n",
    "    # Vol√°me funkci znovu, abychom dostali Y s jinou realizac√≠ ≈°umu (pokud system.measure ≈°um√≠)\n",
    "    _, y_ref_tensor = get_reference_test_set(system_model, real_traj_tensor)\n",
    "    \n",
    "    # B) Inference: KalmanNet\n",
    "    with torch.no_grad():\n",
    "        initial_state = x_ref_tensor_static[:, 0, :] # [1, 4]\n",
    "        state_knet2.reset(batch_size=1, initial_state=initial_state)\n",
    "        \n",
    "        knet_preds = []\n",
    "        y_input = y_ref_tensor \n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input[:, t, :]\n",
    "            x_est = state_knet2.step(y_t)\n",
    "            knet_preds.append(x_est)\n",
    "            \n",
    "        knet_preds_tensor = torch.stack(knet_preds, dim=1)\n",
    "        full_knet_est = torch.cat([initial_state.unsqueeze(1), knet_preds_tensor], dim=1)\n",
    "        x_est_knet = full_knet_est.squeeze().cpu().numpy()\n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    y_for_filters = y_ref_tensor.squeeze(0) \n",
    "    \n",
    "    # !!! KL√çƒåOV√Å OPRAVA: Pou≈æijeme SKUTEƒåN√ù startovn√≠ bod trajektorie !!!\n",
    "    true_init_state = x_ref_tensor_static[0, 0, :] \n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(\n",
    "        y_seq=y_for_filters,\n",
    "        Ex0=true_init_state, # Spr√°vn√Ω start\n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=10000) # Poƒçet ƒç√°stic dle v√Ωkonu\n",
    "    pf_res = pf.process_sequence(\n",
    "        y_seq=y_for_filters,\n",
    "        Ex0=true_init_state, # Spr√°vn√Ω start\n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    \n",
    "    # D) V√Ωpoƒçet chyb pro tento bƒõh\n",
    "    # KNet\n",
    "    diff_knet = x_est_knet - x_gt\n",
    "    mse_knet = np.mean(diff_knet**2)\n",
    "    pos_err_knet = np.mean(np.sqrt(diff_knet[:, 0]**2 + diff_knet[:, 1]**2))\n",
    "    \n",
    "    # UKF\n",
    "    diff_ukf = x_est_ukf - x_gt\n",
    "    mse_ukf = np.mean(diff_ukf**2)\n",
    "    pos_err_ukf = np.mean(np.sqrt(diff_ukf[:, 0]**2 + diff_ukf[:, 1]**2))\n",
    "    \n",
    "    # PF\n",
    "    diff_pf = x_est_pf - x_gt\n",
    "    mse_pf = np.mean(diff_pf**2)\n",
    "    pos_err_pf = np.mean(np.sqrt(diff_pf[:, 0]**2 + diff_pf[:, 1]**2))\n",
    "    \n",
    "    # Ulo≈æen√≠ do agreg√°toru\n",
    "    agg_mse[\"KNet\"].append(mse_knet)\n",
    "    agg_pos[\"KNet\"].append(pos_err_knet)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf)\n",
    "    agg_pos[\"UKF\"].append(pos_err_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf)\n",
    "    agg_pos[\"PF\"].append(pos_err_pf)\n",
    "\n",
    "    # Ulo≈æen√≠ do detailn√≠ho seznamu\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"KNet_MSE\": mse_knet,\n",
    "        \"UKF_MSE\": mse_ukf,\n",
    "        \"PF_MSE\": mse_pf,\n",
    "        \"KNet_PosErr\": pos_err_knet,\n",
    "        \"UKF_PosErr\": pos_err_ukf,\n",
    "        \"PF_PosErr\": pos_err_pf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠ grafu pro TENTO bƒõh\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        \n",
    "        plt.plot(x_est_knet[:, 0], x_est_knet[:, 1], 'g-', linewidth=1.5, label=f'KalmanNet (MSE: {mse_knet:.1f})')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (MSE: {mse_ukf:.1f})')\n",
    "        plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.8, label=f'PF (MSE: {mse_pf:.1f})')\n",
    "        \n",
    "        plt.title(f\"Run {i+1}/{MC_ITERATIONS}: Trajectory Comparison\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "\n",
    "# 1. Detailn√≠ tabulka v≈°ech bƒõh≈Ø\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY PO JEDNOTLIV√ùCH BƒöZ√çCH\")\n",
    "print(\"=\"*80)\n",
    "# Form√°tov√°n√≠ tabulky pro hezƒç√≠ v√Ωpis\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"KNet_MSE\", \"UKF_MSE\", \"PF_MSE\", \"KNet_PosErr\", \"UKF_PosErr\", \"PF_PosErr\"]])\n",
    "\n",
    "# 2. Souhrnn√° statistika\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({MC_ITERATIONS} bƒõh≈Ø)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats(key):\n",
    "    return np.mean(agg_mse[key]), np.std(agg_mse[key]), np.mean(agg_pos[key]), np.std(agg_pos[key])\n",
    "\n",
    "knet_stats = get_stats(\"KNet\")\n",
    "ukf_stats = get_stats(\"UKF\")\n",
    "pf_stats = get_stats(\"PF\")\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE (Mean ¬± Std)':<25} | {'Pos Error (Mean ¬± Std)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'KalmanNet':<15} | {knet_stats[0]:.1f} ¬± {knet_stats[1]:.1f} | {knet_stats[2]:.2f} ¬± {knet_stats[3]:.2f} m\")\n",
    "print(f\"{'UKF':<15} | {ukf_stats[0]:.1f} ¬± {ukf_stats[1]:.1f} | {ukf_stats[2]:.2f} ¬± {ukf_stats[3]:.2f} m\")\n",
    "print(f\"{'PF':<15} | {pf_stats[0]:.1f} ¬± {pf_stats[1]:.1f} | {pf_stats[2]:.2f} ¬± {pf_stats[3]:.2f} m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3. Fin√°ln√≠ Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_pos[\"KNet\"], agg_pos[\"UKF\"], agg_pos[\"PF\"]], labels=['KalmanNet', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({MC_ITERATIONS} runs)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
