{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/luky/skola/KalmanNet-main/data/data.mat\n",
      "Project root added: /home/luky/skola/KalmanNet-main\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 9.057863\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[5.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import TAN\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [323.7707824707031, -13.519903182983398, -29.721908569335938]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=128 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 4: Seq=300 | Batch=64 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_synthetic_controlled'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 5e-4,             \n",
    "        'batch_size': 128\n",
    "    },\n",
    "       # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    # {\n",
    "    #     'phase_id': 3,\n",
    "    #     'seq_len': 200,         \n",
    "    #     'epochs': 200, \n",
    "    #     'lr': 5e-5,             \n",
    "    #     'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    # },\n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 4,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr':1e-4,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE NOV√âHO MODELU ===\n",
      "DEBUG: Layer 'output_final_linear.0' initialized near zero (Start K=0).\n",
      "Model inicializov√°n. Poƒçet parametr≈Ø: 141876\n",
      "=== SPU≈†TƒöN√ç TR√âNINKU ===\n",
      "\n",
      "--- PHASE 1: SeqLen 10 ---\n",
      "-> Using Standard Training (with Init Noise)\n",
      "Start training on cuda...\n",
      "Epoch [1/500] | Train Loss: 725.0785 | Val Loss: 338.6989 -> New Best!\n",
      "Epoch [2/500] | Train Loss: 262.4735 | Val Loss: 266.3407 -> New Best!\n",
      "Epoch [3/500] | Train Loss: 187.2541 | Val Loss: 172.1994 -> New Best!\n",
      "Epoch [4/500] | Train Loss: 100.9269 | Val Loss: 66.8836 -> New Best!\n",
      "Epoch [5/500] | Train Loss: 36.9086 | Val Loss: 21.5606 | PosMSE: 65.6, VelMSE: 8.2 -> New Best!\n",
      "Epoch [6/500] | Train Loss: 16.6444 | Val Loss: 15.4033 -> New Best!\n",
      "Epoch [7/500] | Train Loss: 13.3210 | Val Loss: 13.5547 -> New Best!\n",
      "Epoch [8/500] | Train Loss: 11.1031 | Val Loss: 11.5959 -> New Best!\n",
      "Epoch [9/500] | Train Loss: 9.9123 | Val Loss: 11.1708 -> New Best!\n",
      "Epoch [10/500] | Train Loss: 9.0041 | Val Loss: 9.8246 | PosMSE: 14.5, VelMSE: 3.6 -> New Best!\n",
      "Epoch [11/500] | Train Loss: 8.3186 | Val Loss: 9.4624 -> New Best!\n",
      "Epoch [12/500] | Train Loss: 7.7275 | Val Loss: 9.0461 -> New Best!\n",
      "Epoch [13/500] | Train Loss: 7.2153 | Val Loss: 8.3194 -> New Best!\n",
      "Epoch [15/500] | Train Loss: 6.0542 | Val Loss: 8.0701 | PosMSE: 9.2, VelMSE: 2.9 -> New Best!\n",
      "Epoch [16/500] | Train Loss: 5.6154 | Val Loss: 7.3902 -> New Best!\n",
      "Epoch [19/500] | Train Loss: 5.1198 | Val Loss: 7.2771 -> New Best!\n",
      "Epoch [20/500] | Train Loss: 4.8205 | Val Loss: 6.9720 | PosMSE: 6.9, VelMSE: 2.7 -> New Best!\n",
      "Epoch [22/500] | Train Loss: 4.5110 | Val Loss: 6.7909 -> New Best!\n",
      "Epoch [23/500] | Train Loss: 4.3244 | Val Loss: 6.4272 -> New Best!\n",
      "Epoch [24/500] | Train Loss: 4.2037 | Val Loss: 6.4001 -> New Best!\n",
      "Epoch [25/500] | Train Loss: 4.1393 | Val Loss: 6.4934 | PosMSE: 5.6, VelMSE: 2.7\n",
      "Epoch [26/500] | Train Loss: 4.1478 | Val Loss: 6.3258 -> New Best!\n",
      "Epoch [30/500] | Train Loss: 3.7080 | Val Loss: 6.2780 | PosMSE: 4.8, VelMSE: 2.6 -> New Best!\n",
      "Epoch [34/500] | Train Loss: 3.3670 | Val Loss: 6.2665 -> New Best!\n",
      "Epoch [35/500] | Train Loss: 3.3258 | Val Loss: 7.2629 | PosMSE: 4.1, VelMSE: 2.6\n",
      "Epoch [36/500] | Train Loss: 3.3045 | Val Loss: 6.0799 -> New Best!\n",
      "Epoch [40/500] | Train Loss: 3.0723 | Val Loss: 6.6802 | PosMSE: 3.6, VelMSE: 2.5\n",
      "Epoch [45/500] | Train Loss: 2.8462 | Val Loss: 6.5303 | PosMSE: 3.2, VelMSE: 2.5\n",
      "Epoch [50/500] | Train Loss: 2.6913 | Val Loss: 25.1638 | PosMSE: 2.9, VelMSE: 2.5\n",
      "Epoch [55/500] | Train Loss: 2.5693 | Val Loss: 24.8861 | PosMSE: 2.7, VelMSE: 2.5\n",
      "Epoch [60/500] | Train Loss: 2.4649 | Val Loss: 6.2142 | PosMSE: 2.5, VelMSE: 2.4\n",
      "Epoch [65/500] | Train Loss: 2.3240 | Val Loss: 22.7437 | PosMSE: 2.2, VelMSE: 2.4\n",
      "\n",
      "Early stopping triggered after 66 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 6.079866\n",
      "Phase 1 completed. Model saved.\n",
      "\n",
      "--- PHASE 2: SeqLen 100 ---\n",
      "-> Using TBPTT Sliding Window (k=40, w=20)\n",
      "INFO: Detected from model attribute that it returns covariance: False\n",
      "INFO: Starting training with TBPTT(k=2, w=6)\n",
      "Epoch [1/200] | Train Loss: 2806.1093 | Val Loss: 171.6380 -> New Best!\n",
      "Epoch [2/200] | Train Loss: 823.9090 | Val Loss: 71.1769 -> New Best!\n",
      "Epoch [4/200] | Train Loss: 714.8496 | Val Loss: 56.4021 -> New Best!\n",
      "Epoch [5/200] | Train Loss: 1030.7228 | Val Loss: 87.9510 | Val MSE: 87.95\n",
      "Epoch [6/200] | Train Loss: 396.9413 | Val Loss: 54.9008 -> New Best!\n",
      "Epoch [7/200] | Train Loss: 55.1002 | Val Loss: 50.4709 -> New Best!\n",
      "Epoch [10/200] | Train Loss: 391.4828 | Val Loss: 57.8853 | Val MSE: 57.89\n",
      "Epoch [12/200] | Train Loss: 615.8106 | Val Loss: 47.9273 -> New Best!\n",
      "Epoch [15/200] | Train Loss: 43.9094 | Val Loss: 47.2462 | Val MSE: 47.25 -> New Best!\n",
      "Epoch [16/200] | Train Loss: 44.6854 | Val Loss: 44.4757 -> New Best!\n",
      "Epoch [20/200] | Train Loss: 40.6021 | Val Loss: 54.8962 | Val MSE: 54.90\n",
      "Epoch [23/200] | Train Loss: 38.5731 | Val Loss: 44.3941 -> New Best!\n",
      "Epoch [25/200] | Train Loss: 40.1292 | Val Loss: 45.0114 | Val MSE: 45.01\n",
      "Epoch [26/200] | Train Loss: 38.6714 | Val Loss: 43.5674 -> New Best!\n",
      "Epoch [28/200] | Train Loss: 38.7098 | Val Loss: 42.8968 -> New Best!\n",
      "Epoch [29/200] | Train Loss: 38.2823 | Val Loss: 42.7624 -> New Best!\n",
      "Epoch [30/200] | Train Loss: 37.9563 | Val Loss: 43.3075 | Val MSE: 43.31\n",
      "Epoch [35/200] | Train Loss: 36.1851 | Val Loss: 44.5197 | Val MSE: 44.52\n",
      "Epoch [37/200] | Train Loss: 35.6911 | Val Loss: 41.7429 -> New Best!\n",
      "Epoch [40/200] | Train Loss: 36.0533 | Val Loss: 44.0530 | Val MSE: 44.05\n",
      "Epoch [45/200] | Train Loss: 32.8666 | Val Loss: 46.1508 | Val MSE: 46.15\n",
      "Epoch [50/200] | Train Loss: 32.7198 | Val Loss: 44.9637 | Val MSE: 44.96\n",
      "Epoch [55/200] | Train Loss: 32.1007 | Val Loss: 44.4122 | Val MSE: 44.41\n",
      "Epoch [60/200] | Train Loss: 32.8120 | Val Loss: 46.7885 | Val MSE: 46.79\n",
      "Epoch [65/200] | Train Loss: 30.4346 | Val Loss: 52.6450 | Val MSE: 52.64\n",
      "\n",
      "Early stopping triggered after 67 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 41.742949\n",
      "Phase 2 completed. Model saved.\n",
      "\n",
      "--- PHASE 4: SeqLen 300 ---\n",
      "-> Using TBPTT Sliding Window (k=50, w=20)\n",
      "INFO: Detected from model attribute that it returns covariance: False\n",
      "INFO: Starting training with TBPTT(k=2, w=6)\n",
      "Epoch [1/200] | Train Loss: 115.1317 | Val Loss: 118.7437 -> New Best!\n",
      "Epoch [3/200] | Train Loss: 107.2779 | Val Loss: 118.4221 -> New Best!\n",
      "Epoch [5/200] | Train Loss: 106.3025 | Val Loss: 120.8304 | Val MSE: 120.83\n",
      "Epoch [8/200] | Train Loss: 100.3260 | Val Loss: 114.4593 -> New Best!\n",
      "Epoch [10/200] | Train Loss: 100.4901 | Val Loss: 118.7878 | Val MSE: 118.79\n",
      "Epoch [15/200] | Train Loss: 94.8919 | Val Loss: 119.4171 | Val MSE: 119.42\n",
      "Epoch [20/200] | Train Loss: 94.4046 | Val Loss: 124.1943 | Val MSE: 124.19\n",
      "Epoch [25/200] | Train Loss: 89.2199 | Val Loss: 150.6802 | Val MSE: 150.68\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 114.459305\n",
      "Phase 4 completed. Model saved.\n",
      "üéâ Tr√©nink dokonƒçen.\n"
     ]
    }
   ],
   "source": [
    "# --- A) KONFIGURACE S√çTƒö ---\n",
    "print(\"=== INICIALIZACE NOV√âHO MODELU ===\")\n",
    "from state_NN_models import TAN\n",
    "\n",
    "state_knet2 = TAN.StateKalmanNetTAN(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=6,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        gru_hidden_dim_multiplier=4      \n",
    ").to(device)\n",
    "\n",
    "print(f\"Model inicializov√°n. Poƒçet parametr≈Ø: {sum(p.numel() for p in state_knet2.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- B) SPU≈†TƒöN√ç TR√âNINKU ---\n",
    "print(\"=== SPU≈†TƒöN√ç TR√âNINKU ===\")\n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    phase_id = phase['phase_id']\n",
    "    seq_len = phase['seq_len']\n",
    "    \n",
    "    print(f\"\\n--- PHASE {phase_id}: SeqLen {seq_len} ---\")\n",
    "    \n",
    "    # 1. Naƒçten√≠ dat\n",
    "    if phase_id not in datasets_cache:\n",
    "        raise ValueError(\"Data nejsou vygenerov√°na! Spus≈• prvn√≠ bu≈àku.\")\n",
    "        \n",
    "    train_loader, val_loader = datasets_cache[phase_id]\n",
    "    \n",
    "    # 2. Z√°kladn√≠ argumenty spoleƒçn√© pro obƒõ metody\n",
    "    common_args = {\n",
    "        'model': state_knet2,\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'device': device,\n",
    "        'epochs': phase['epochs'],\n",
    "        'lr': phase['lr'],\n",
    "    }\n",
    "\n",
    "    # 3. Rozvƒõtven√≠ logiky podle d√©lky sekvence\n",
    "    if seq_len <= 20:\n",
    "        # === F√ÅZE 1 & 2: Kr√°tk√© sekvence (Standardn√≠ tr√©nink) ===\n",
    "        # Zde chceme init_noise pro robustnost startu\n",
    "        print(\"-> Using Standard Training (with Init Noise)\")\n",
    "        trainer.train_state_KalmanNetTAN(\n",
    "            **common_args,\n",
    "            optimizer_type=torch.optim.AdamW,\n",
    "            weight_decay=1e-3,\n",
    "            clip_grad=1.0,\n",
    "            early_stopping_patience=30\n",
    "        )\n",
    "    elif seq_len == 100 or seq_len == 200:\n",
    "        # F√ÅZE 2: TBPTT (Opraveno z '110' na '100')\n",
    "        # S procesn√≠m ≈°umem u≈æ nem≈Ø≈æeme tr√©novat 100 krok≈Ø v kuse (explodovaly by gradienty)\n",
    "        # Pou≈æijeme TBPTT s oknem 30-40 krok≈Ø.\n",
    "        print(f\"-> Using TBPTT Sliding Window (k=40, w=20)\")\n",
    "        trainer.train_state_KalmanNet_sliding_windowTAN(\n",
    "            **common_args,\n",
    "            weight_decay_=1e-3,\n",
    "            early_stopping_patience=30,\n",
    "            tbptt_k=2,   # !!! ZV√ù≈†ENO: Mus√≠ vidƒõt dozadu, aby poznal drift\n",
    "            tbptt_w=6,   # Posun o 20\n",
    "            optimizer_=torch.optim.AdamW,\n",
    "            clip_grad=1.0\n",
    "        )\n",
    "        \n",
    "    else: # SeqLen 300\n",
    "        # F√ÅZE 3: TBPTT Long\n",
    "        print(f\"-> Using TBPTT Sliding Window (k=50, w=20)\")\n",
    "        trainer.train_state_KalmanNet_sliding_windowTAN(\n",
    "            **common_args,\n",
    "            weight_decay_=1e-4,\n",
    "            early_stopping_patience=20,\n",
    "            tbptt_k=2,   # !!! ZV√ù≈†ENO Z 2 NA 50 !!!\n",
    "            tbptt_w=6,   \n",
    "            optimizer_=torch.optim.AdamW,\n",
    "            clip_grad=1.0 # Velmi opatrn√© √∫pravy vah\n",
    "        )\n",
    "    # save_path = f'knet_robust_len{seq_len}.pth'\n",
    "    # torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Phase {phase_id} completed. Model saved.\")\n",
    "\n",
    "print(\"üéâ Tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'knet_curriculum_model28_vyrovnane_s_UKF24.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VYHODNOCEN√ç NA TESTOVAC√ç SADƒö ===\n",
      "Naƒç√≠t√°m data z: ./generated_data_synthetic_controlled/test_set/test.pt\n",
      "Poƒçet testovac√≠ch trajektori√≠: 5\n",
      "D√©lka sekvence: 1000\n",
      "Modely: KalmanNet vs. UKF vs. PF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluace:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [12:45<19:08, 382.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Auxiliary PF\u001b[39;00m\n\u001b[1;32m    103\u001b[0m apf \u001b[38;5;241m=\u001b[39m TAN\u001b[38;5;241m.\u001b[39mAuxiliaryParticleFilterTAN(system_model, num_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m) \n\u001b[0;32m--> 104\u001b[0m apf_res \u001b[38;5;241m=\u001b[39m \u001b[43mapf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_obs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_init_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP0\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m x_est_apf \u001b[38;5;241m=\u001b[39m apf_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# D) V√Ωpoƒçet chyb\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# O≈ôe≈æeme na d√©lku min(odhad, gt) pro jistotu\u001b[39;00m\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/Filters/TAN/AuxiliaryParticleFilter.py:157\u001b[0m, in \u001b[0;36mAuxiliaryParticleFilterTAN.process_sequence\u001b[0;34m(self, y_seq, u_sequence, Ex0, P0)\u001b[0m\n\u001b[1;32m    153\u001b[0m weights_aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_log_weights(log_weights_aux)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# --- APF F√ÅZE 2: Resampling ---\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Vybereme indexy \"nadƒõjn√Ωch\" ƒç√°stic\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parent_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_systematic_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Vybereme rodiƒçovsk√© ƒç√°stice\u001b[39;00m\n\u001b[1;32m    160\u001b[0m parents \u001b[38;5;241m=\u001b[39m current_particles[parent_indices]\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/Filters/TAN/AuxiliaryParticleFilter.py:67\u001b[0m, in \u001b[0;36mAuxiliaryParticleFilterTAN._systematic_resample\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m cumulative_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(weights)\n\u001b[1;32m     66\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m N:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m positions[i] \u001b[38;5;241m<\u001b[39m cumulative_sum[j]:\n\u001b[1;32m     69\u001b[0m         indexes[i] \u001b[38;5;241m=\u001b[39m j\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import Filters\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from Filters import TAN\n",
    "\n",
    "# === KONFIGURACE ===\n",
    "TEST_DATA_PATH = './generated_data_synthetic_controlled/test_set/test.pt'\n",
    "PLOT_PER_ITERATION = False  # Vykreslovat graf pro ka≈ædou trajektorii?\n",
    "MAX_TEST_SAMPLES = 5      # Kolik trajektori√≠ z test setu vyhodnotit (max 10, co jsme vygenerovali)\n",
    "\n",
    "print(f\"=== VYHODNOCEN√ç NA TESTOVAC√ç SADƒö ===\")\n",
    "print(f\"Naƒç√≠t√°m data z: {TEST_DATA_PATH}\")\n",
    "\n",
    "# 1. Naƒçten√≠ Testovac√≠ sady\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Soubor {TEST_DATA_PATH} neexistuje! Spus≈•te generov√°n√≠ testovac√≠ sady.\")\n",
    "\n",
    "test_data = torch.load(TEST_DATA_PATH, map_location=device)\n",
    "X_test_all = test_data['x']  # Ground Truth [N, Seq, 4]\n",
    "Y_test_all = test_data['y']  # Measurements [N, Seq, 3]\n",
    "\n",
    "n_samples = min(X_test_all.shape[0], MAX_TEST_SAMPLES)\n",
    "print(f\"Poƒçet testovac√≠ch trajektori√≠: {n_samples}\")\n",
    "print(f\"D√©lka sekvence: {X_test_all.shape[1]}\")\n",
    "print(\"Modely: KalmanNet vs. UKF vs. PF\")\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = []\n",
    "agg_mse = {\"KNet\": [], \"UKF\": [], \"PF\": [], \"APF\": []}\n",
    "agg_pos = {\"KNet\": [], \"UKF\": [], \"PF\": [], \"APF\": []}\n",
    "\n",
    "# Ujist√≠me se, ≈æe KNet je v eval m√≥du\n",
    "state_knet2.eval()\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA (Iterace p≈ôes testovac√≠ trajektorie) ---\n",
    "for i in tqdm(range(n_samples), desc=\"Evaluace\"):\n",
    "    \n",
    "    # A) P≈ô√≠prava dat pro tento bƒõh\n",
    "    x_gt_tensor = X_test_all[i]      # [Seq, 4]\n",
    "    y_obs_tensor = Y_test_all[i]     # [Seq, 3]\n",
    "    \n",
    "    x_gt = x_gt_tensor.cpu().numpy()\n",
    "    seq_len = x_gt.shape[0]\n",
    "    \n",
    "    # Skuteƒçn√Ω startovn√≠ stav (pro inicializaci filtr≈Ø)\n",
    "    true_init_state = x_gt_tensor[0] # [4]\n",
    "    \n",
    "    # B) Inference: KalmanNet\n",
    "    # KNet oƒçek√°v√° [Batch, Seq, Dim], tak≈æe mus√≠me p≈ôidat dimenzi\n",
    "    with torch.no_grad():\n",
    "        initial_state_batch = true_init_state.unsqueeze(0) # [1, 4]\n",
    "        \n",
    "        # Reset stavu s√≠tƒõ\n",
    "        state_knet2.reset(batch_size=1, initial_state=initial_state_batch)\n",
    "        \n",
    "        knet_preds = []\n",
    "        # KNet zpracov√°v√° sekvenci krok po kroku (nebo bychom mohli upravit forward na celou sekvenci)\n",
    "        # Zde zachov√°me logiku step-by-step pro konzistenci\n",
    "        \n",
    "        # Vstup y_obs_tensor m√° tvar [Seq, 3]. Pot≈ôebujeme [1, 3] pro ka≈æd√Ω krok\n",
    "        y_input_batch = y_obs_tensor.unsqueeze(0) # [1, Seq, 3]\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input_batch[:, t, :] # [1, 3]\n",
    "            x_est = state_knet2.step(y_t)\n",
    "            knet_preds.append(x_est)\n",
    "            \n",
    "        # Slo≈æen√≠ predikce (p≈ôid√°me poƒç√°teƒçn√≠ stav)\n",
    "        if len(knet_preds) > 0:\n",
    "            knet_preds_tensor = torch.stack(knet_preds, dim=1) # [1, Seq-1, 4]\n",
    "            full_knet_est = torch.cat([initial_state_batch.unsqueeze(1), knet_preds_tensor], dim=1)\n",
    "        else:\n",
    "            full_knet_est = initial_state_batch.unsqueeze(1)\n",
    "            \n",
    "        x_est_knet = full_knet_est.squeeze().cpu().numpy()\n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    # Filtry oƒçek√°vaj√≠ [Seq, Dim] (bez batch dimenze, pokud tak byly naps√°ny)\n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = TAN.UnscentedKalmanFilterTAN(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF (Sn√≠≈æil jsem poƒçet ƒç√°stic na 2000 pro rychlost, pro fin√°ln√≠ diplomku dejte v√≠ce)\n",
    "    pf = TAN.ParticleFilterTAN(system_model, num_particles=1000) \n",
    "    pf_res = pf.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # Auxiliary PF\n",
    "    apf = TAN.AuxiliaryParticleFilterTAN(system_model, num_particles=200000) \n",
    "    apf_res = apf.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_apf = apf_res['x_filtered'].cpu().numpy()\n",
    "    \n",
    "    # D) V√Ωpoƒçet chyb\n",
    "    # O≈ôe≈æeme na d√©lku min(odhad, gt) pro jistotu\n",
    "    min_len = min(len(x_gt), len(x_est_knet), len(x_est_ukf))\n",
    "    \n",
    "    # KNet\n",
    "    diff_knet = x_est_knet[:min_len] - x_gt[:min_len]\n",
    "    mse_knet = np.mean(np.sum(diff_knet[:, :2]**2, axis=1)) # Pouze XY chyba\n",
    "    pos_err_knet = np.mean(np.sqrt(diff_knet[:, 0]**2 + diff_knet[:, 1]**2))\n",
    "    \n",
    "    # UKF\n",
    "    diff_ukf = x_est_ukf[:min_len] - x_gt[:min_len]\n",
    "    mse_ukf = np.mean(np.sum(diff_ukf[:, :2]**2, axis=1))\n",
    "    pos_err_ukf = np.mean(np.sqrt(diff_ukf[:, 0]**2 + diff_ukf[:, 1]**2))\n",
    "    \n",
    "    # PF\n",
    "    diff_pf = x_est_pf[:min_len] - x_gt[:min_len]\n",
    "    mse_pf = np.mean(np.sum(diff_pf[:, :2]**2, axis=1))\n",
    "    pos_err_pf = np.mean(np.sqrt(diff_pf[:, 0]**2 + diff_pf[:, 1]**2))\n",
    "\n",
    "    # APF\n",
    "    diff_apf = x_est_apf[:min_len] - x_gt[:min_len]\n",
    "    mse_apf = np.mean(np.sum(diff_apf[:, :2]**2, axis=1))\n",
    "    pos_err_apf = np.mean(np.sqrt(diff_apf[:, 0]**2 + diff_apf[:, 1]**2))\n",
    "    \n",
    "    # Ulo≈æen√≠\n",
    "    agg_mse[\"KNet\"].append(mse_knet)\n",
    "    agg_pos[\"KNet\"].append(pos_err_knet)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf)\n",
    "    agg_pos[\"UKF\"].append(pos_err_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf)\n",
    "    agg_pos[\"PF\"].append(pos_err_pf)\n",
    "    agg_mse[\"APF\"].append(mse_apf)\n",
    "    agg_pos[\"APF\"].append(pos_err_apf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"KNet_MSE\": mse_knet,\n",
    "        \"UKF_MSE\": mse_ukf,\n",
    "        \"PF_MSE\": mse_pf,\n",
    "        \"KNet_PosErr\": pos_err_knet,\n",
    "        \"UKF_PosErr\": pos_err_ukf,\n",
    "        \"PF_PosErr\": pos_err_pf,\n",
    "        \"APF_MSE\": mse_apf,\n",
    "        \"APF_PosErr\": pos_err_apf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        # Vykresl√≠me jen XY trajektorii\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        plt.plot(x_est_knet[:, 0], x_est_knet[:, 1], 'g-', linewidth=2, label=f'KalmanNet (Err: {pos_err_knet:.1f}m)')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_err_ukf:.1f}m)')\n",
    "        plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.6, label=f'PF (Err: {pos_err_pf:.1f}m)')\n",
    "        plt.plot(x_est_apf[:, 0], x_est_apf[:, 1], 'm-.', linewidth=1, alpha=0.6, label=f'APF (Err: {pos_err_apf:.1f}m)')\n",
    "        plt.title(f\"Test Trajectory {i+1} (Length {seq_len})\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY PO JEDNOTLIV√ùCH TRAJEKTORI√çCH\")\n",
    "print(\"=\"*80)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"KNet_MSE\", \"UKF_MSE\", \"PF_MSE\", \"APF_MSE\", \"KNet_PosErr\", \"UKF_PosErr\", \"PF_PosErr\", \"APF_PosErr\"]])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({n_samples} trajektori√≠)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats(key):\n",
    "    return np.mean(agg_mse[key]), np.std(agg_mse[key]), np.mean(agg_pos[key]), np.std(agg_pos[key])\n",
    "\n",
    "knet_stats = get_stats(\"KNet\")\n",
    "ukf_stats = get_stats(\"UKF\")\n",
    "pf_stats = get_stats(\"PF\")\n",
    "apf_stats = get_stats(\"APF\")\n",
    "print(f\"{'Model':<15} | {'MSE (Mean ¬± Std)':<25} | {'Pos Error (Mean ¬± Std)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'KalmanNet':<15} | {knet_stats[0]:.1f} ¬± {knet_stats[1]:.1f} | {knet_stats[2]:.2f} ¬± {knet_stats[3]:.2f} m\")\n",
    "print(f\"{'UKF':<15} | {ukf_stats[0]:.1f} ¬± {ukf_stats[1]:.1f} | {ukf_stats[2]:.2f} ¬± {ukf_stats[3]:.2f} m\")\n",
    "print(f\"{'PF':<15} | {pf_stats[0]:.1f} ¬± {pf_stats[1]:.1f} | {pf_stats[2]:.2f} ¬± {pf_stats[3]:.2f} m\")\n",
    "print(f\"{'APF':<15} | {apf_stats[0]:.1f} ¬± {apf_stats[1]:.1f} | {apf_stats[2]:.2f} ¬± {apf_stats[3]:.2f} m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_pos[\"KNet\"], agg_pos[\"UKF\"], agg_pos[\"PF\"], agg_pos[\"APF\"]], labels=['KalmanNet', 'UKF', 'PF', 'APF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({n_samples} test trajectories)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
