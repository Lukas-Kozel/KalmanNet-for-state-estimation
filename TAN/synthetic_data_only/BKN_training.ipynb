{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/luky/skola/KalmanNet-main/data/data.mat\n",
      "Project root added: /home/luky/skola/KalmanNet-main\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 1\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[3.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import TAN\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [323.7707824707031, -13.519903182983398, -29.721908569335938]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=128 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 3: Seq=200 | Batch=64 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 4: Seq=300 | Batch=64 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_synthetic_controlled'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-4,             \n",
    "        'batch_size': 128\n",
    "    },\n",
    "       # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 200,         \n",
    "        'epochs': 200, \n",
    "        'lr': 5e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    },\n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 4,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cde2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_BayesianKalmanNet_Hybrid(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    warmup_iterations=0, weight_decay_=1e-5,\n",
    "    lambda_mse=100.0  # <--- NOV√ù PARAMETR: Kotva pro MSE\n",
    "):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Scheduler: Pokud se loss zasekne, sn√≠≈æ√≠me LR (pom√°h√° stabilizovat konvergenci)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    score_at_best = {\"val_nll\": 0.0, \"val_mse\": 0.0}\n",
    "    best_iter_count = 0\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Hybrid Training: Loss = NLL + {lambda_mse} * MSE\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true (Ground Truth) !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            # --- Training ---\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                            raise ValueError(f\"NaN in x_filtered_t at sample {j}, step {t}\")\n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance (ƒçist√Ω rozptyl s√≠tƒõ)\n",
    "            # P≈ôiƒç√≠t√°me 1e-9 jen proti dƒõlen√≠ nulou, nen√≠ to \"noise floor\"\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-9 \n",
    "            \n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean()\n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET HYBRIDN√ç LOSS ---\n",
    "            \n",
    "            # A) MSE ƒå√°st (P≈ôesnost)\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            \n",
    "            # B) NLL ƒå√°st (Konzistence)\n",
    "            # 0.5 * (log(var) + (target - pred)^2 / var)\n",
    "            error_sq = (x_hat_sequence - target_sequence) ** 2\n",
    "            nll_loss = 0.5 * (torch.log(cov_diag_sequence) + error_sq / cov_diag_sequence).mean()\n",
    "            \n",
    "            # C) Celkov√° Loss (Hybrid)\n",
    "            # Zde je ta magie: I kdy≈æ NLL chce ut√©ct s varianc√≠, lambda_mse * mse ho dr≈æ√≠ zp√°tky\n",
    "            loss = nll_loss + (lambda_mse * mse_loss) + regularization_loss * 1.0\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "            if clip_grad > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- Logging ---\n",
    "            diff = x_hat_sequence - target_sequence\n",
    "            mean_error = diff.abs().mean().item()\n",
    "            min_variance = cov_diag_sequence.min().item()\n",
    "\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Zjist√≠me dropout pravdƒõpodobnosti (jen pro info)\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                \n",
    "                print(f\"--- Iter [{train_iter_count}/{total_train_iter}] ---\")\n",
    "                print(f\"    Total Loss: {loss.item():.4f}\")\n",
    "                print(f\"    MSE Component: {mse_loss.item():.4f} (Weighted: {(mse_loss.item() * lambda_mse):.4f})\")\n",
    "                print(f\"    NLL Component: {nll_loss.item():.4f}\")\n",
    "                print(f\"    Min Variance: {min_variance:.2e}\")\n",
    "                print(f\"    p1={p1:.3f}, p2={p2:.3f}\")\n",
    "\n",
    "            # --- Validation step ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                # Step scheduleru podle tr√©novac√≠ loss (nebo validace, pokud bys to p≈ôedƒõlal)\n",
    "                scheduler.step(loss)\n",
    "                \n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                all_val_x_true_cpu, all_val_x_hat_cpu, all_val_P_hat_cpu = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val_batch, y_meas_val_batch in val_loader:\n",
    "                        val_batch_size, val_seq_len, _ = x_true_val_batch.shape\n",
    "                        x_true_val_batch = x_true_val_batch.to(device)\n",
    "                        y_meas_val_batch = y_meas_val_batch.to(device)\n",
    "                        val_ensemble_trajectories = []\n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=val_batch_size, initial_state=x_true_val_batch[:, 0, :])\n",
    "                            val_current_x_hats = []\n",
    "                            for t in range(1, val_seq_len):\n",
    "                                y_t_val = y_meas_val_batch[:, t, :]\n",
    "                                x_filtered_t, _ = model.step(y_t_val)\n",
    "                                val_current_x_hats.append(x_filtered_t)\n",
    "                            val_ensemble_trajectories.append(torch.stack(val_current_x_hats, dim=1))\n",
    "                        \n",
    "                        # Agregace validace\n",
    "                        val_ensemble = torch.stack(val_ensemble_trajectories, dim=0)\n",
    "                        val_preds_seq = val_ensemble.mean(dim=0)\n",
    "                        \n",
    "                        val_target_seq = x_true_val_batch[:, 1:, :]\n",
    "                        val_mse_list.append(F.mse_loss(val_preds_seq, val_target_seq).item())\n",
    "                        \n",
    "                        # P≈ô√≠prava pro ANEES\n",
    "                        initial_state_val = x_true_val_batch[:, 0, :].unsqueeze(1)\n",
    "                        full_x_hat = torch.cat([initial_state_val, val_preds_seq], dim=1)\n",
    "                        \n",
    "                        # Epistemick√° variance\n",
    "                        val_covs_diag = val_ensemble.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        # Vytvo≈ôen√≠ diagon√°ln√≠ch matic P\n",
    "                        # (Zjednodu≈°en√° konstrukce pro ANEES calc)\n",
    "                        # Pro p≈ôesn√© ANEES bychom mƒõli dƒõlat outer product, \n",
    "                        # ale diagon√°la z var() je dobr√° aproximace pro BKN\n",
    "                        val_covs_full = torch.zeros(val_batch_size, val_seq_len-1, 4, 4, device=device)\n",
    "                        for b in range(val_batch_size):\n",
    "                            for t in range(val_seq_len-1):\n",
    "                                val_covs_full[b, t] = torch.diag(val_covs_diag[b, t])\n",
    "\n",
    "                        P0 = model.system_model.P0.unsqueeze(0).repeat(val_batch_size, 1, 1).unsqueeze(1)\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true_cpu.append(x_true_val_batch.cpu())\n",
    "                        all_val_x_hat_cpu.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat_cpu.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                final_x_true_list = torch.cat(all_val_x_true_cpu, dim=0)\n",
    "                final_x_hat_list = torch.cat(all_val_x_hat_cpu, dim=0)\n",
    "                final_P_hat_list = torch.cat(all_val_P_hat_cpu, dim=0)\n",
    "                \n",
    "                # V√Ωpoƒçet ANEES\n",
    "                avg_val_anees = trainer.calculate_anees_vectorized(final_x_true_list, final_x_hat_list, final_P_hat_list)\n",
    "                \n",
    "                print(f\"  Average MSE: {avg_val_mse:.4f}, Average ANEES: {avg_val_anees:.4f}\")\n",
    "                \n",
    "                # Ukl√°d√°n√≠ modelu:\n",
    "                # ZDE POZOR: Ukl√°d√°me, pokud je ANEES rozumn√© A Z√ÅROVE≈á MSE nen√≠ katastrofick√©.\n",
    "                # Nebo prostƒõ ukl√°d√°me podle nejlep≈°√≠ho ANEES jako d≈ô√≠v, ale s vƒõdom√≠m hybridn√≠ loss.\n",
    "                if not np.isnan(avg_val_anees) and avg_val_anees < best_val_anees and avg_val_anees > 0:\n",
    "                    print(\"  >>> New best VALIDATION ANEES! Saving model. <<<\")\n",
    "                    best_val_anees = avg_val_anees\n",
    "                    best_iter_count = train_iter_count\n",
    "                    score_at_best['val_mse'] = avg_val_mse\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                print(\"-\" * 50)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count} with ANEES {best_val_anees:.4f}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(\"No best model was saved; returning last state.\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_anees\": best_val_anees,\n",
    "        \"best_val_nll\": score_at_best['val_nll'],\n",
    "        \"best_val_mse\": score_at_best['val_mse'],\n",
    "        \"best_iter\": best_iter_count,\n",
    "        \"final_model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE NOV√âHO MODELU ===\n",
      "INFO: Aplikuji 'Start Zero' inicializaci pro Kalman Gain.\n",
      "DEBUG: V√Ωstupn√≠ vrstva vynulov√°na (Soft Start).\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luky/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ START Hybrid Training: Loss = NLL + 100.0 * MSE\n",
      "--- Iter [1/2000] ---\n",
      "    Total Loss: 20019800064.0000\n",
      "    MSE Component: 244559.2656 (Weighted: 24455926.5625)\n",
      "    NLL Component: 19995344896.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [2/2000] ---\n",
      "    Total Loss: 13119876096.0000\n",
      "    MSE Component: 251342.7812 (Weighted: 25134278.1250)\n",
      "    NLL Component: 13094742016.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [3/2000] ---\n",
      "    Total Loss: 14000433152.0000\n",
      "    MSE Component: 245195.4688 (Weighted: 24519546.8750)\n",
      "    NLL Component: 13975913472.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [4/2000] ---\n",
      "    Total Loss: 16730799104.0000\n",
      "    MSE Component: 233098.5000 (Weighted: 23309850.0000)\n",
      "    NLL Component: 16707488768.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [5/2000] ---\n",
      "    Total Loss: 12090051584.0000\n",
      "    MSE Component: 242148.6406 (Weighted: 24214864.0625)\n",
      "    NLL Component: 12065837056.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [6/2000] ---\n",
      "    Total Loss: 19968833536.0000\n",
      "    MSE Component: 204674.7656 (Weighted: 20467476.5625)\n",
      "    NLL Component: 19948365824.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [7/2000] ---\n",
      "    Total Loss: 26828435456.0000\n",
      "    MSE Component: 306836.0938 (Weighted: 30683609.3750)\n",
      "    NLL Component: 26797752320.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [8/2000] ---\n",
      "    Total Loss: 15665119232.0000\n",
      "    MSE Component: 288428.1250 (Weighted: 28842812.5000)\n",
      "    NLL Component: 15636276224.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [9/2000] ---\n",
      "    Total Loss: 24483764224.0000\n",
      "    MSE Component: 272581.0625 (Weighted: 27258106.2500)\n",
      "    NLL Component: 24456505344.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [10/2000] ---\n",
      "    Total Loss: 17498593280.0000\n",
      "    MSE Component: 345722.2500 (Weighted: 34572225.0000)\n",
      "    NLL Component: 17464020992.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "\n",
      "--- Validation at iteration 10 ---\n",
      "  Average MSE: 257621.0039, Average ANEES: 2845285120.0000\n",
      "  >>> New best VALIDATION ANEES! Saving model. <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [11/2000] ---\n",
      "    Total Loss: 15810032640.0000\n",
      "    MSE Component: 255325.9062 (Weighted: 25532590.6250)\n",
      "    NLL Component: 15784500224.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n",
      "--- Iter [12/2000] ---\n",
      "    Total Loss: 15224590336.0000\n",
      "    MSE Component: 287756.4688 (Weighted: 28775646.8750)\n",
      "    NLL Component: 15195814912.0000\n",
      "    Min Variance: 1.00e-09\n",
      "    p1=0.371, p2=0.212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m datasets_cache[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain_BayesianKalmanNet_Hybrid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_knet2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_train_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# M√≠rnƒõ del≈°√≠\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# <--- M√≠rnƒõ zv√Ω≈°eno pro dynamiku\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# <--- PRODLOU≈ΩENO (d≈Øle≈æit√© pro MSE)\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Uvoln√≠me trochu gradienty\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mJ_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Pro rychlost ladƒõn√≠ staƒç√≠ 5, na fin√°le dej 10+\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_mse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# <--- NOV√ù PARAMETR: Kotva pro MSE\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéâ Tr√©nink dokonƒçen.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m, in \u001b[0;36mtrain_BayesianKalmanNet_Hybrid\u001b[0;34m(model, train_loader, val_loader, device, total_train_iter, learning_rate, clip_grad, J_samples, validation_period, logging_period, warmup_iterations, weight_decay_, lambda_mse)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, seq_len):\n\u001b[1;32m     54\u001b[0m     y_t \u001b[38;5;241m=\u001b[39m y_meas_batch[:, t, :]\n\u001b[0;32m---> 55\u001b[0m     x_filtered_t, reg_t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(x_filtered_t)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN in x_filtered_t at sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/state_NN_models/TAN/StateBayesianKalmanNet.py:62\u001b[0m, in \u001b[0;36mStateBayesianKalmanNetTAN.step\u001b[0;34m(self, y_t)\u001b[0m\n\u001b[1;32m     60\u001b[0m x_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_model\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_filtered_t_minus_1)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# y_{t|t-1} = h(x_{t|t-1})\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_predicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# O≈°et≈ôen√≠ dimenz√≠\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_predicted\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: y_predicted \u001b[38;5;241m=\u001b[39m y_predicted\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/Systems/DynamicSystem_for_TAN.py:90\u001b[0m, in \u001b[0;36mDynamicSystemTAN.h\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbmm(H_batch, x_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Aplikujeme neline√°rn√≠ funkci p≈ô√≠mo na cel√Ω d√°vkov√Ω tenzor.\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 79\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     76\u001b[0m x_min, x_max \u001b[38;5;241m=\u001b[39m x_axis_unique\u001b[38;5;241m.\u001b[39mmin(), x_axis_unique\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     77\u001b[0m y_min, y_max \u001b[38;5;241m=\u001b[39m y_axis_unique\u001b[38;5;241m.\u001b[39mmin(), y_axis_unique\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 79\u001b[0m h_wrapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mh_nl_differentiable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterMap_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_max\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m system_model \u001b[38;5;241m=\u001b[39m DynamicSystemTAN(\n\u001b[1;32m     89\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39mstate_dim,\n\u001b[1;32m     90\u001b[0m     obs_dim\u001b[38;5;241m=\u001b[39mobs_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    100\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mh_nl_differentiable\u001b[0;34m(x, map_tensor, x_min, x_max, y_min, y_max)\u001b[0m\n\u001b[1;32m     45\u001b[0m px_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (px \u001b[38;5;241m-\u001b[39m x_min) \u001b[38;5;241m/\u001b[39m (x_max \u001b[38;5;241m-\u001b[39m x_min) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     46\u001b[0m py_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (py \u001b[38;5;241m-\u001b[39m y_min) \u001b[38;5;241m/\u001b[39m (y_max \u001b[38;5;241m-\u001b[39m y_min) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 48\u001b[0m sampling_grid \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m vyska_terenu_batch \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39mgrid_sample(\n\u001b[1;32m     51\u001b[0m     map_tensor\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     52\u001b[0m     sampling_grid, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m vyska_terenu \u001b[38;5;241m=\u001b[39m vyska_terenu_batch\u001b[38;5;241m.\u001b[39mview(batch_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from state_NN_models import TAN\n",
    "# --- A) KONFIGURACE S√çTƒö ---\n",
    "print(\"=== INICIALIZACE NOV√âHO MODELU ===\")\n",
    "state_knet2 = TAN.StateBayesianKalmanNetTAN(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=8,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        init_max_dropout=0.4,\n",
    "        init_min_dropout=0.2      \n",
    ").to(device)\n",
    "\n",
    "train_loader, val_loader = datasets_cache[2]\n",
    "print(len(train_loader))\n",
    "\n",
    "train_BayesianKalmanNet_Hybrid(\n",
    "    model=state_knet2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    total_train_iter=2000,          # M√≠rnƒõ del≈°√≠\n",
    "    learning_rate=1e-5,             # <--- M√≠rnƒõ zv√Ω≈°eno pro dynamiku\n",
    "    warmup_iterations=0,          # <--- PRODLOU≈ΩENO (d≈Øle≈æit√© pro MSE)\n",
    "    clip_grad=0.25,                  # Uvoln√≠me trochu gradienty\n",
    "    J_samples=5,                    # Pro rychlost ladƒõn√≠ staƒç√≠ 5, na fin√°le dej 10+\n",
    "    validation_period=10,\n",
    "    logging_period=1,\n",
    "    weight_decay_=1e-3,\n",
    "    lambda_mse=100.0                # <--- NOV√ù PARAMETR: Kotva pro MSE\n",
    ")\n",
    "\n",
    "print(\"üéâ Tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'knet_curriculum_model_1GRU_relativne_ok_vysledky.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import Filters\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === KONFIGURACE ===\n",
    "TEST_DATA_PATH = './generated_data_synthetic_controlled/test_set/test.pt'\n",
    "PLOT_PER_ITERATION = True  # Vykreslovat graf pro ka≈ædou trajektorii?\n",
    "MAX_TEST_SAMPLES = 1      # Kolik trajektori√≠ z test setu vyhodnotit (max 10, co jsme vygenerovali)\n",
    "\n",
    "print(f\"=== VYHODNOCEN√ç NA TESTOVAC√ç SADƒö ===\")\n",
    "print(f\"Naƒç√≠t√°m data z: {TEST_DATA_PATH}\")\n",
    "\n",
    "# 1. Naƒçten√≠ Testovac√≠ sady\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Soubor {TEST_DATA_PATH} neexistuje! Spus≈•te generov√°n√≠ testovac√≠ sady.\")\n",
    "\n",
    "test_data = torch.load(TEST_DATA_PATH, map_location=device)\n",
    "X_test_all = test_data['x']  # Ground Truth [N, Seq, 4]\n",
    "Y_test_all = test_data['y']  # Measurements [N, Seq, 3]\n",
    "\n",
    "n_samples = min(X_test_all.shape[0], MAX_TEST_SAMPLES)\n",
    "print(f\"Poƒçet testovac√≠ch trajektori√≠: {n_samples}\")\n",
    "print(f\"D√©lka sekvence: {X_test_all.shape[1]}\")\n",
    "print(\"Modely: KalmanNet vs. UKF vs. PF\")\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = []\n",
    "agg_mse = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "agg_pos = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "\n",
    "# Ujist√≠me se, ≈æe KNet je v eval m√≥du\n",
    "state_knet2.eval()\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA (Iterace p≈ôes testovac√≠ trajektorie) ---\n",
    "for i in tqdm(range(n_samples), desc=\"Evaluace\"):\n",
    "    \n",
    "    # A) P≈ô√≠prava dat pro tento bƒõh\n",
    "    x_gt_tensor = X_test_all[i]      # [Seq, 4]\n",
    "    y_obs_tensor = Y_test_all[i]     # [Seq, 3]\n",
    "    \n",
    "    x_gt = x_gt_tensor.cpu().numpy()\n",
    "    seq_len = x_gt.shape[0]\n",
    "    \n",
    "    # Skuteƒçn√Ω startovn√≠ stav (pro inicializaci filtr≈Ø)\n",
    "    true_init_state = x_gt_tensor[0] # [4]\n",
    "    \n",
    "    # B) Inference: KalmanNet\n",
    "    # KNet oƒçek√°v√° [Batch, Seq, Dim], tak≈æe mus√≠me p≈ôidat dimenzi\n",
    "    with torch.no_grad():\n",
    "        initial_state_batch = true_init_state.unsqueeze(0) # [1, 4]\n",
    "        \n",
    "        # Reset stavu s√≠tƒõ\n",
    "        state_knet2.reset(batch_size=1, initial_state=initial_state_batch)\n",
    "        \n",
    "        knet_preds = []\n",
    "        # KNet zpracov√°v√° sekvenci krok po kroku (nebo bychom mohli upravit forward na celou sekvenci)\n",
    "        # Zde zachov√°me logiku step-by-step pro konzistenci\n",
    "        \n",
    "        # Vstup y_obs_tensor m√° tvar [Seq, 3]. Pot≈ôebujeme [1, 3] pro ka≈æd√Ω krok\n",
    "        y_input_batch = y_obs_tensor.unsqueeze(0) # [1, Seq, 3]\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input_batch[:, t, :] # [1, 3]\n",
    "            x_est = state_knet2.step(y_t)\n",
    "            knet_preds.append(x_est)\n",
    "            \n",
    "        # Slo≈æen√≠ predikce (p≈ôid√°me poƒç√°teƒçn√≠ stav)\n",
    "        if len(knet_preds) > 0:\n",
    "            knet_preds_tensor = torch.stack(knet_preds, dim=1) # [1, Seq-1, 4]\n",
    "            full_knet_est = torch.cat([initial_state_batch.unsqueeze(1), knet_preds_tensor], dim=1)\n",
    "        else:\n",
    "            full_knet_est = initial_state_batch.unsqueeze(1)\n",
    "            \n",
    "        x_est_knet = full_knet_est.squeeze().cpu().numpy()\n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    # Filtry oƒçek√°vaj√≠ [Seq, Dim] (bez batch dimenze, pokud tak byly naps√°ny)\n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF (Sn√≠≈æil jsem poƒçet ƒç√°stic na 2000 pro rychlost, pro fin√°ln√≠ diplomku dejte v√≠ce)\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=10000) \n",
    "    pf_res = pf.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    \n",
    "    # D) V√Ωpoƒçet chyb\n",
    "    # O≈ôe≈æeme na d√©lku min(odhad, gt) pro jistotu\n",
    "    min_len = min(len(x_gt), len(x_est_knet), len(x_est_ukf))\n",
    "    \n",
    "    # KNet\n",
    "    diff_knet = x_est_knet[:min_len] - x_gt[:min_len]\n",
    "    mse_knet = np.mean(np.sum(diff_knet[:, :2]**2, axis=1)) # Pouze XY chyba\n",
    "    pos_err_knet = np.mean(np.sqrt(diff_knet[:, 0]**2 + diff_knet[:, 1]**2))\n",
    "    \n",
    "    # UKF\n",
    "    diff_ukf = x_est_ukf[:min_len] - x_gt[:min_len]\n",
    "    mse_ukf = np.mean(np.sum(diff_ukf[:, :2]**2, axis=1))\n",
    "    pos_err_ukf = np.mean(np.sqrt(diff_ukf[:, 0]**2 + diff_ukf[:, 1]**2))\n",
    "    \n",
    "    # PF\n",
    "    diff_pf = x_est_pf[:min_len] - x_gt[:min_len]\n",
    "    mse_pf = np.mean(np.sum(diff_pf[:, :2]**2, axis=1))\n",
    "    pos_err_pf = np.mean(np.sqrt(diff_pf[:, 0]**2 + diff_pf[:, 1]**2))\n",
    "    \n",
    "    # Ulo≈æen√≠\n",
    "    agg_mse[\"KNet\"].append(mse_knet)\n",
    "    agg_pos[\"KNet\"].append(pos_err_knet)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf)\n",
    "    agg_pos[\"UKF\"].append(pos_err_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf)\n",
    "    agg_pos[\"PF\"].append(pos_err_pf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"KNet_MSE\": mse_knet,\n",
    "        \"UKF_MSE\": mse_ukf,\n",
    "        \"PF_MSE\": mse_pf,\n",
    "        \"KNet_PosErr\": pos_err_knet,\n",
    "        \"UKF_PosErr\": pos_err_ukf,\n",
    "        \"PF_PosErr\": pos_err_pf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        # Vykresl√≠me jen XY trajektorii\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        plt.plot(x_est_knet[:, 0], x_est_knet[:, 1], 'g-', linewidth=2, label=f'KalmanNet (Err: {pos_err_knet:.1f}m)')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_err_ukf:.1f}m)')\n",
    "        plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.6, label=f'PF (Err: {pos_err_pf:.1f}m)')\n",
    "        \n",
    "        plt.title(f\"Test Trajectory {i+1} (Length {seq_len})\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY PO JEDNOTLIV√ùCH TRAJEKTORI√çCH\")\n",
    "print(\"=\"*80)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"KNet_MSE\", \"UKF_MSE\", \"PF_MSE\", \"KNet_PosErr\", \"UKF_PosErr\", \"PF_PosErr\"]])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({n_samples} trajektori√≠)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats(key):\n",
    "    return np.mean(agg_mse[key]), np.std(agg_mse[key]), np.mean(agg_pos[key]), np.std(agg_pos[key])\n",
    "\n",
    "knet_stats = get_stats(\"KNet\")\n",
    "ukf_stats = get_stats(\"UKF\")\n",
    "pf_stats = get_stats(\"PF\")\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE (Mean ¬± Std)':<25} | {'Pos Error (Mean ¬± Std)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'KalmanNet':<15} | {knet_stats[0]:.1f} ¬± {knet_stats[1]:.1f} | {knet_stats[2]:.2f} ¬± {knet_stats[3]:.2f} m\")\n",
    "print(f\"{'UKF':<15} | {ukf_stats[0]:.1f} ¬± {ukf_stats[1]:.1f} | {ukf_stats[2]:.2f} ¬± {ukf_stats[3]:.2f} m\")\n",
    "print(f\"{'PF':<15} | {pf_stats[0]:.1f} ¬± {pf_stats[1]:.1f} | {pf_stats[2]:.2f} ¬± {pf_stats[3]:.2f} m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_pos[\"KNet\"], agg_pos[\"UKF\"], agg_pos[\"PF\"]], labels=['KalmanNet', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({n_samples} test trajectories)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
