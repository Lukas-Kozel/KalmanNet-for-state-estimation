{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/luky/skola/KalmanNet-main/data/data.mat\n",
      "Project root added: /home/luky/skola/KalmanNet-main\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luky/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 1\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[3.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import TAN\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [323.7707824707031, -13.519903182983398, -29.721908569335938]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=256 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 3: Seq=300 | Batch=128 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_synthetic_controlled'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-4,             \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-5,             \n",
    "        'batch_size': 128       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a867d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    \"\"\"\n",
    "    Bezpeƒçn√° NLL loss funkce.\n",
    "    \"\"\"\n",
    "    safe_var = var + min_var\n",
    "    error_sq = (preds - target) ** 2\n",
    "    # Clampujeme velikost chyby v ƒçitateli, aby loss neexplodovala,\n",
    "    # ale gradient do variance (ve jmenovateli) z≈Østal zachov√°n.\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    return nll.mean()\n",
    "\n",
    "def train_BayesianKalmanNet_TwoPhase(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    mse_warmup_iters=0,  # <--- NOV√ù PARAMETR: Kolik iterac√≠ tr√©novat jen na MSE\n",
    "    weight_decay_=1e-5\n",
    "):\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Scheduler (voliteln√Ω, zde vypnut√Ω pro jednoduchost)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)\n",
    "\n",
    "    best_val_metric = float('inf') # Buƒè MSE nebo ANEES podle f√°ze\n",
    "    best_model_state = None\n",
    "    best_iter_count = 0\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Two-Phase Training\")\n",
    "    print(f\"    Phase 1: MSE Warmup (0 - {mse_warmup_iters} iters)\")\n",
    "    print(f\"    Phase 2: NLL Optimization ({mse_warmup_iters} - {total_train_iter} iters)\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            # Detekce NaN v datech\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            # --- Training Step ---\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                \n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    \n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                        raise ValueError(f\"NaN in x_filtered_t at sample {j}, step {t}\")\n",
    "                        \n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                \n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-9 \n",
    "            \n",
    "            # Normalizovan√° regularizace (na d√©lku sekvence)\n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean() / seq_len\n",
    "            \n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET LOSS (Dvƒõ f√°ze) ---\n",
    "            \n",
    "            # V≈ædy spoƒç√≠t√°me oboj√≠ pro logov√°n√≠\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            nll_loss = gaussian_nll_safe(\n",
    "                target=target_sequence, \n",
    "                preds=x_hat_sequence, \n",
    "                var=cov_diag_sequence, \n",
    "                min_var=1e-5, \n",
    "                max_error_sq=100.0\n",
    "            )\n",
    "            \n",
    "            # Rozhodov√°n√≠ o optimalizaƒçn√≠ Loss\n",
    "            loss_mode = \"\"\n",
    "            if train_iter_count < mse_warmup_iters:\n",
    "                # F√ÅZE 1: Warmup na MSE\n",
    "                # Ignorujeme NLL, soust≈ôed√≠me se na trefen√≠ trajektorie\n",
    "                loss = mse_loss + regularization_loss\n",
    "                loss_mode = \"MSE_WARMUP\"\n",
    "            else:\n",
    "                # F√ÅZE 2: NLL Optimalizace\n",
    "                # Minimalizujeme NLL (kalibrace nejistoty + p≈ôesnost)\n",
    "                # MSE zde nen√≠ p≈ô√≠mo v gradientu (je schovan√© v NLL), ale logujeme ho\n",
    "                loss = nll_loss + regularization_loss\n",
    "                loss_mode = \"NLL_OPTIM\"\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # --- DIAGNOSTIKA GRADIENT≈Æ ---\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                total_norm = 0.0\n",
    "                max_grad = 0.0\n",
    "                nan_grad_detected = False\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2).item()\n",
    "                        total_norm += param_norm ** 2\n",
    "                        p_max = p.grad.data.abs().max().item()\n",
    "                        if p_max > max_grad: max_grad = p_max\n",
    "                        if torch.isnan(p.grad).any():\n",
    "                            nan_grad_detected = True\n",
    "                total_norm = total_norm ** 0.5\n",
    "                \n",
    "                if nan_grad_detected:\n",
    "                     print(f\"!!! WARNING: NaN gradient detected at iter {train_iter_count} !!!\")\n",
    "\n",
    "            if clip_grad > 0: \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- LOGGING ---\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Statistiky variance\n",
    "                    min_variance = cov_diag_sequence.min().item()\n",
    "                    max_variance = cov_diag_sequence.max().item()\n",
    "                    mean_variance = cov_diag_sequence.mean().item()\n",
    "                    \n",
    "                    # Dropout pravdƒõpodobnosti\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                    \n",
    "                    # Chyba v metrech (L1)\n",
    "                    diff = x_hat_sequence - target_sequence\n",
    "                    mean_error = diff.abs().mean().item()\n",
    "                \n",
    "                print(f\"--- Iter [{train_iter_count}/{total_train_iter}] ({loss_mode}) ---\")\n",
    "                print(f\"    Total Loss:     {loss.item():.4f}\")\n",
    "                print(f\"    MSE (Metric):   {mse_loss.item():.4f}\")\n",
    "                print(f\"    NLL (Metric):   {nll_loss.item():.4f}\")\n",
    "                print(f\"    Reg Loss:       {regularization_loss.item():.6f}\")\n",
    "                print(f\"    Var Stats:      Min={min_variance:.2e}, Max={max_variance:.2e}, Mean={mean_variance:.2e}\")\n",
    "                print(f\"    Mean Error L1:  {mean_error:.4f} m\")\n",
    "                print(f\"    Grad Norm:      {total_norm:.4f} (Max: {max_grad:.4f})\")\n",
    "                print(f\"    Dropout:        p1={p1:.4f}, p2={p2:.4f}\")\n",
    "\n",
    " # --- VALIDATION (Upraven√° ƒç√°st) ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                \n",
    "                # Pro ANEES\n",
    "                all_val_x_true, all_val_x_hat, all_val_P_hat = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val, y_meas_val in val_loader:\n",
    "                        v_bs, v_seq, _ = x_true_val.shape\n",
    "                        x_true_val = x_true_val.to(device)\n",
    "                        y_meas_val = y_meas_val.to(device)\n",
    "                        \n",
    "                        val_ensemble_trajs = []\n",
    "                        \n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=v_bs, initial_state=x_true_val[:, 0, :])\n",
    "                            v_x_hats = []\n",
    "                            for t in range(1, v_seq):\n",
    "                                est, _ = model.step(y_meas_val[:, t, :])\n",
    "                                v_x_hats.append(est)\n",
    "                            val_ensemble_trajs.append(torch.stack(v_x_hats, dim=1))\n",
    "                        \n",
    "                        val_ens_stack = torch.stack(val_ensemble_trajs, dim=0) # [J, B, T, D]\n",
    "                        val_mean = val_ens_stack.mean(dim=0)\n",
    "                        val_var_diag = val_ens_stack.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        val_mse_list.append(F.mse_loss(val_mean, x_true_val[:, 1:, :]).item())\n",
    "                        \n",
    "                        # --- P≈ô√≠prava dat pro ANEES ---\n",
    "                        # 1. Stavy (Ground Truth a Odhad)\n",
    "                        # P≈ôid√°me startovn√≠ bod (t=0), abychom mƒõli celou sekvenci\n",
    "                        full_x_hat = torch.cat([x_true_val[:, 0, :].unsqueeze(1), val_mean], dim=1)\n",
    "                        \n",
    "                        # 2. Kovariance (P)\n",
    "                        # Vytvo≈ô√≠me pln√© matice 4x4 z diagon√°ln√≠ho rozptylu\n",
    "                        val_covs_full = torch.zeros(v_bs, v_seq-1, 4, 4, device=device)\n",
    "                        # Rychl√° vektorizovan√° konstrukce diagon√°ly\n",
    "                        # (M√≠sto cyklu p≈ôes batch a ƒças pou≈æijeme diag_embed, pokud to PyTorch verze um√≠)\n",
    "                        try:\n",
    "                            val_covs_full = torch.diag_embed(val_var_diag)\n",
    "                        except:\n",
    "                            # Fallback pro star≈°√≠ verze nebo pokud to sel≈æe\n",
    "                            for b in range(v_bs):\n",
    "                                for t in range(v_seq-1):\n",
    "                                    val_covs_full[b, t] = torch.diag(val_var_diag[b, t])\n",
    "                        \n",
    "                        # P0 (poƒç√°teƒçn√≠ nejistota - mal√°)\n",
    "                        P0 = torch.eye(4, device=device).unsqueeze(0).unsqueeze(0).repeat(v_bs, 1, 1, 1) * 1e-6\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true.append(x_true_val.cpu())\n",
    "                        all_val_x_hat.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                \n",
    "                # --- V√ùPOƒåET ANEES ---\n",
    "                try:\n",
    "                    cat_true = torch.cat(all_val_x_true, dim=0)\n",
    "                    cat_hat = torch.cat(all_val_x_hat, dim=0)\n",
    "                    cat_P = torch.cat(all_val_P_hat, dim=0)\n",
    "                    \n",
    "                    # Vol√°me va≈°i funkci (p≈ôedpokl√°d√°m, ≈æe je v modulu 'trainer' nebo 'utils')\n",
    "                    # Pokud ji nem√°te naimportovanou, mus√≠te ji definovat.\n",
    "                    # Zde pou≈æ√≠v√°m 'trainer.calculate_anees_vectorized' z va≈°eho p≈Øvodn√≠ho k√≥du\n",
    "                    if hasattr(trainer, 'calculate_anees_vectorized'):\n",
    "                         avg_val_anees = trainer.calculate_anees_vectorized(cat_true, cat_hat, cat_P)\n",
    "                    else:\n",
    "                         avg_val_anees = float('nan') # Placeholder\n",
    "                         \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating ANEES: {e}\")\n",
    "                    avg_val_anees = float('nan')\n",
    "\n",
    "                print(f\"  Avg MSE: {avg_val_mse:.4f} | Avg ANEES: {avg_val_anees:.4f}\")\n",
    "                \n",
    "                # --- LOGIKA UKL√ÅD√ÅN√ç (Smart Saving) ---\n",
    "                # Ukl√°d√°me, pokud se zlep≈°√≠ MSE (priorita 1).\n",
    "                # Volitelnƒõ: Ve f√°zi NLL by se dalo ukl√°dat, pokud se zlep≈°√≠ ANEES (k ide√°ln√≠ 4.0),\n",
    "                # ale to je riskantn√≠, pokud by MSE vyletƒõlo.\n",
    "                # Z≈Østa≈àme u MSE jako \"kotvy kvality\".\n",
    "                \n",
    "                current_metric = avg_val_mse\n",
    "                if current_metric < best_val_metric:\n",
    "                    print(f\"  >>> New Best Model! (MSE: {best_val_metric:.4f} -> {current_metric:.4f}) <<<\")\n",
    "                    best_val_metric = current_metric\n",
    "                    best_iter_count = train_iter_count\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                \n",
    "                print(\"-\" * 50)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3ad555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def calculate_anees_vectorized(x_true, x_hat, P_hat, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Robustn√≠ v√Ωpoƒçet ANEES pomoc√≠ Pseudo-Inverze.\n",
    "    Zvl√°d√° i situaci, kdy J_samples < State_Dim (singul√°rn√≠ matice).\n",
    "    \"\"\"\n",
    "    # 1. Zplo≈°tƒõn√≠ (Flattening)\n",
    "    if x_true.dim() == 3: x_true = x_true.reshape(-1, x_true.shape[-1])\n",
    "    if x_hat.dim() == 3: x_hat = x_hat.reshape(-1, x_hat.shape[-1])\n",
    "    if P_hat.dim() == 4: P_hat = P_hat.reshape(-1, P_hat.shape[-2], P_hat.shape[-1])\n",
    "\n",
    "    # 2. V√Ωpoƒçet chyby\n",
    "    error = (x_true - x_hat).unsqueeze(-1)  # [N, Dim, 1]\n",
    "    \n",
    "    # 3. Pseudo-Inverze matice P\n",
    "    # pinv je mnohem stabilnƒõj≈°√≠ pro Low-Rank matice (kdy≈æ m√°me m√°lo vzork≈Ø)\n",
    "    # hermitian=True ≈ô√≠k√°, ≈æe matice je symetrick√° (co≈æ kovariance je)\n",
    "    P_inv = torch.linalg.pinv(P_hat, hermitian=True)\n",
    "    \n",
    "    # 4. ANEES = error^T * P_inv * error\n",
    "    # bmm: [N, 1, Dim] @ [N, Dim, Dim] -> [N, 1, Dim]\n",
    "    #      [N, 1, Dim] @ [N, Dim, 1] -> [N, 1, 1]\n",
    "    \n",
    "    temp = torch.bmm(error.transpose(1, 2), P_inv)\n",
    "    anees_per_sample = torch.bmm(temp, error).squeeze() # [N]\n",
    "    \n",
    "    # Ochrana proti numerick√Ωm artefakt≈Øm (malink√° z√°porn√° ƒç√≠sla jako -1e-10)\n",
    "    anees_per_sample = torch.clamp(anees_per_sample, min=0.0)\n",
    "\n",
    "    return anees_per_sample.mean().item()\n",
    "\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    # 1. Bezpeƒçn√° variance (epsilon) - spr√°vnƒõ\n",
    "    safe_var = var + min_var\n",
    "    \n",
    "    # 2. Kvadratick√° chyba\n",
    "    error_sq = (preds - target) ** 2\n",
    "    \n",
    "    # 3. === OPRAVA ===\n",
    "    # Clampujeme \"velikost chyby\", nikoliv \"velikost trestu\".\n",
    "    # T√≠m ≈ô√≠k√°me: \"Pokud je chyba vƒõt≈°√≠ ne≈æ 10m (100m^2), chovej se, jako by byla 10m.\"\n",
    "    # Vzorec z≈Øst√°v√°: Const / var.\n",
    "    # Derivace je: -Const / var^2. (To je z√°porn√© ƒç√≠slo -> zvy≈°ov√°n√≠ var sni≈æuje Loss -> SPR√ÅVNƒö!)\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    \n",
    "    # 4. V√Ωpoƒçet s o≈ô√≠znutou chybou\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    \n",
    "    return nll.mean()\n",
    "\n",
    "def train_BayesianKalmanNet_Hybrid(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    warmup_iterations=0, weight_decay_=1e-5,\n",
    "    lambda_mse=100.0  # <--- NOV√ù PARAMETR: Kotva pro MSE\n",
    "):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Scheduler: Pokud se loss zasekne, sn√≠≈æ√≠me LR (pom√°h√° stabilizovat konvergenci)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer, mode='min', factor=0.5, patience=50\n",
    "    # )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    score_at_best = {\"val_nll\": 0.0, \"val_mse\": 0.0}\n",
    "    best_iter_count = 0\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Hybrid Training: Loss = NLL + {lambda_mse} * MSE\")\n",
    "    print(f\"    Logging period: {logging_period} iterations\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true (Ground Truth) !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            # --- Training ---\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                            raise ValueError(f\"NaN in x_filtered_t at sample {j}, step {t}\")\n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance (ƒçist√Ω rozptyl s√≠tƒõ)\n",
    "            # P≈ôiƒç√≠t√°me 1e-9 jen proti dƒõlen√≠ nulou, nen√≠ to \"noise floor\"\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-9 \n",
    "            \n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean()/seq_len\n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET HYBRIDN√ç LOSS ---\n",
    "            \n",
    "            # A) MSE ƒå√°st (P≈ôesnost)\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            \n",
    "            # B) NLL ƒå√°st (Konzistence)\n",
    "            # 0.5 * (log(var) + (target - pred)^2 / var)\n",
    "            cov_diag_clamped = torch.clamp(cov_diag_sequence, min=1e-4, max=1e6)\n",
    "            error_sq = (x_hat_sequence - target_sequence) ** 2\n",
    "            nll_term = 0.5 * (torch.log(cov_diag_clamped) + error_sq / cov_diag_clamped)\n",
    "            nll_loss = gaussian_nll_safe(\n",
    "                target=target_sequence, \n",
    "                preds=x_hat_sequence, \n",
    "                var=cov_diag_sequence, \n",
    "                min_var=1e-5,       # Epsilon pro stabilitu\n",
    "                max_error_sq=100.0 # O≈ôez√°n√≠ extr√©mn√≠ch chyb (pokud je implementov√°no)\n",
    "            )\n",
    "            mean_var = cov_diag_sequence.mean()\n",
    "            var_penalty = torch.relu(mean_var - 100.0) * 0.01\n",
    "            \n",
    "            # C) Celkov√° Loss (Hybrid)\n",
    "            # Zde je ta magie: I kdy≈æ NLL chce ut√©ct s varianc√≠, lambda_mse * mse ho dr≈æ√≠ zp√°tky\n",
    "            weighted_mse = lambda_mse * mse_loss\n",
    "            loss = nll_loss + weighted_mse + regularization_loss * 10.0 + var_penalty\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # --- DIAGNOSTIC LOGGING (Gradients) ---\n",
    "            # Zaznamen√°me statistiky gradient≈Ø p≈ôed o≈ô√≠znut√≠m (clippingem)\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                total_norm = 0.0\n",
    "                max_grad = 0.0\n",
    "                min_grad = float('inf')\n",
    "                nan_grad_detected = False\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2).item()\n",
    "                        total_norm += param_norm ** 2\n",
    "                        p_max = p.grad.data.abs().max().item()\n",
    "                        p_min = p.grad.data.abs().min().item()\n",
    "                        if p_max > max_grad: max_grad = p_max\n",
    "                        if p_min < min_grad: min_grad = p_min\n",
    "                        if torch.isnan(p.grad).any():\n",
    "                            nan_grad_detected = True\n",
    "                total_norm = total_norm ** 0.5\n",
    "                \n",
    "                if nan_grad_detected:\n",
    "                     print(f\"!!! WARNING: NaN gradient detected at iter {train_iter_count} !!!\")\n",
    "\n",
    "            if clip_grad > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- Logging ---\n",
    "            diff = x_hat_sequence - target_sequence\n",
    "            mean_error = diff.abs().mean().item()\n",
    "            min_variance = cov_diag_sequence.min().item()\n",
    "            max_variance = cov_diag_sequence.max().item()\n",
    "            mean_variance = cov_diag_sequence.mean().item()\n",
    "\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Zjist√≠me dropout pravdƒõpodobnosti (jen pro info)\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                \n",
    "                print(f\"--- Iter [{train_iter_count}/{total_train_iter}] ---\")\n",
    "                print(f\"    Total Loss:     {loss.item():.4f}\")\n",
    "                print(f\"    MSE (Raw):      {mse_loss.item():.6f}\")\n",
    "                print(f\"    MSE (Weighted): {weighted_mse.item():.4f} (lambda={lambda_mse})\")\n",
    "                print(f\"    NLL Component:  {nll_loss.item():.4f}\")\n",
    "                print(f\"    Reg Loss:       {regularization_loss.item():.6f}\")\n",
    "                print(f\"    Variance stats: Min={min_variance:.2e}, Max={max_variance:.2e}, Mean={mean_variance:.2e}\")\n",
    "                print(f\"    Mean Error L1:  {mean_error:.4f}\")\n",
    "                print(f\"    Grad Norm:      {total_norm:.4f} (Max abs grad: {max_grad:.4f})\")\n",
    "                print(f\"    Dropout probs:  p1={p1:.4f}, p2={p2:.4f}\")\n",
    "                \n",
    "                # Check pro \"Variance collapse\"\n",
    "                if mean_variance < 1e-8:\n",
    "                    print(\"    !!! WARNING: Variance is extremely low (Collapse risk) !!!\")\n",
    "\n",
    "            # --- Validation step ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                # Step scheduleru podle tr√©novac√≠ loss (nebo validace, pokud bys to p≈ôedƒõlal)\n",
    "                # scheduler.step(loss)\n",
    "                \n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                all_val_x_true_cpu, all_val_x_hat_cpu, all_val_P_hat_cpu = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val_batch, y_meas_val_batch in val_loader:\n",
    "                        val_batch_size, val_seq_len, _ = x_true_val_batch.shape\n",
    "                        x_true_val_batch = x_true_val_batch.to(device)\n",
    "                        y_meas_val_batch = y_meas_val_batch.to(device)\n",
    "                        val_ensemble_trajectories = []\n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=val_batch_size, initial_state=x_true_val_batch[:, 0, :])\n",
    "                            val_current_x_hats = []\n",
    "                            for t in range(1, val_seq_len):\n",
    "                                y_t_val = y_meas_val_batch[:, t, :]\n",
    "                                x_filtered_t, _ = model.step(y_t_val)\n",
    "                                val_current_x_hats.append(x_filtered_t)\n",
    "                            val_ensemble_trajectories.append(torch.stack(val_current_x_hats, dim=1))\n",
    "                        \n",
    "                        # Agregace validace\n",
    "                        val_ensemble = torch.stack(val_ensemble_trajectories, dim=0)\n",
    "                        val_preds_seq = val_ensemble.mean(dim=0)\n",
    "                        \n",
    "                        val_target_seq = x_true_val_batch[:, 1:, :]\n",
    "                        val_mse_list.append(F.mse_loss(val_preds_seq, val_target_seq).item())\n",
    "                        \n",
    "                        # P≈ô√≠prava pro ANEES\n",
    "                        initial_state_val = x_true_val_batch[:, 0, :].unsqueeze(1)\n",
    "                        full_x_hat = torch.cat([initial_state_val, val_preds_seq], dim=1)\n",
    "                        \n",
    "                        # Epistemick√° variance\n",
    "                        val_covs_diag = val_ensemble.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        # Vytvo≈ôen√≠ diagon√°ln√≠ch matic P\n",
    "                        # (Zjednodu≈°en√° konstrukce pro ANEES calc)\n",
    "                        # Pro p≈ôesn√© ANEES bychom mƒõli dƒõlat outer product, \n",
    "                        # ale diagon√°la z var() je dobr√° aproximace pro BKN\n",
    "                        val_covs_full = torch.zeros(val_batch_size, val_seq_len-1, 4, 4, device=device)\n",
    "                        for b in range(val_batch_size):\n",
    "                            for t in range(val_seq_len-1):\n",
    "                                val_covs_full[b, t] = torch.diag(val_covs_diag[b, t])\n",
    "\n",
    "                        P0 = model.system_model.P0.unsqueeze(0).repeat(val_batch_size, 1, 1).unsqueeze(1)\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true_cpu.append(x_true_val_batch.cpu())\n",
    "                        all_val_x_hat_cpu.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat_cpu.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                final_x_true_list = torch.cat(all_val_x_true_cpu, dim=0)\n",
    "                final_x_hat_list = torch.cat(all_val_x_hat_cpu, dim=0)\n",
    "                final_P_hat_list = torch.cat(all_val_P_hat_cpu, dim=0)\n",
    "                \n",
    "                # V√Ωpoƒçet ANEES\n",
    "                try:\n",
    "                    avg_val_anees = calculate_anees_vectorized(final_x_true_list, final_x_hat_list, final_P_hat_list)\n",
    "                except Exception as e:\n",
    "                    print(f\"  !!! Error calculating ANEES: {e}\")\n",
    "                    avg_val_anees = float('nan')\n",
    "                \n",
    "                print(f\"  Average MSE: {avg_val_mse:.4f}, Average ANEES: {avg_val_anees:.4f}\")\n",
    "                \n",
    "                # Ukl√°d√°n√≠ modelu:\n",
    "                if not np.isnan(avg_val_anees) and avg_val_anees < best_val_anees and avg_val_anees > 0:\n",
    "                    print(f\"  >>> New best VALIDATION ANEES! Saving model. (Old: {best_val_anees:.4f} -> New: {avg_val_anees:.4f}) <<<\")\n",
    "                    best_val_anees = avg_val_anees\n",
    "                    best_iter_count = train_iter_count\n",
    "                    score_at_best['val_mse'] = avg_val_mse\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                print(\"-\" * 50)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count} with ANEES {best_val_anees:.4f}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(\"No best model was saved; returning last state.\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_anees\": best_val_anees,\n",
    "        \"best_val_nll\": score_at_best['val_nll'],\n",
    "        \"best_val_mse\": score_at_best['val_mse'],\n",
    "        \"best_iter\": best_iter_count,\n",
    "        \"final_model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0fe051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# P≈ôedpokl√°d√°me existenci helperu (pokud ne, definujte ho jako v minul√© odpovƒõdi)\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    safe_var = var + min_var\n",
    "    error_sq = (preds - target) ** 2\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    return nll.mean()\n",
    "\n",
    "def train_BayesianKalmanNet_TBPTT_TwoPhase(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad=1.0,\n",
    "    J_samples=5, tbptt_steps=20,\n",
    "    validation_period=50, logging_period=10,\n",
    "    mse_warmup_iters=0, # Poƒçet updat≈Ø pro MSE f√°zi\n",
    "    weight_decay_=1e-5\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    best_val_mse = float('inf')\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0 # Poƒç√≠tadlo updat≈Ø (gradient steps)\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START TBPTT Two-Phase Training (Window={tbptt_steps})\")\n",
    "    print(f\"    Phase 1: MSE Warmup (0 - {mse_warmup_iters} steps)\")\n",
    "    print(f\"    Phase 2: NLL Optimization ({mse_warmup_iters} - {total_train_iter} steps)\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            batch_size, seq_len, dim_x = x_true_batch.shape\n",
    "            \n",
    "            # --- SUPER BATCH (Vektorizace) ---\n",
    "            x_true_super = x_true_batch.repeat_interleave(J_samples, dim=0)\n",
    "            y_meas_super = y_meas_batch.repeat_interleave(J_samples, dim=0)\n",
    "            super_batch_size = x_true_super.shape[0]\n",
    "\n",
    "            # 1. Reset na zaƒç√°tku sekvence\n",
    "            model.reset(batch_size=super_batch_size, initial_state=x_true_super[:, 0, :])\n",
    "            \n",
    "            # Gradienty nulujeme p≈ôed sekvenc√≠\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 2. TBPTT Smyƒçka p≈ôes okna\n",
    "            for t_start in range(1, seq_len, tbptt_steps):\n",
    "                if train_iter_count >= total_train_iter: done = True; break\n",
    "\n",
    "                t_end = min(t_start + tbptt_steps, seq_len)\n",
    "                current_window_len = t_end - t_start\n",
    "                if current_window_len <= 0: continue\n",
    "\n",
    "                # A) Forward pass oknem\n",
    "                window_x_preds = []\n",
    "                window_regs = []\n",
    "                \n",
    "                for t in range(t_start, t_end):\n",
    "                    y_t = y_meas_super[:, t, :]\n",
    "                    x_est, reg = model.step(y_t)\n",
    "                    window_x_preds.append(x_est)\n",
    "                    window_regs.append(reg)\n",
    "                \n",
    "                # B) Zpracov√°n√≠ v√Ωsledk≈Ø okna\n",
    "                preds_super = torch.stack(window_x_preds, dim=1) # [Batch*J, Window, 4]\n",
    "                regs_super = torch.stack(window_regs)\n",
    "                \n",
    "                # Reshape pro statistiku [Batch, J, Window, 4]\n",
    "                preds_reshaped = preds_super.view(batch_size, J_samples, current_window_len, dim_x)\n",
    "                \n",
    "                x_hat_seq = preds_reshaped.mean(dim=1)\n",
    "                cov_diag_seq = preds_reshaped.var(dim=1) + 1e-9\n",
    "                \n",
    "                target_seq = x_true_batch[:, t_start:t_end, :]\n",
    "\n",
    "                # C) V√Ωpoƒçet Loss (Two-Phase)\n",
    "                mse_loss = F.mse_loss(x_hat_seq, target_seq)\n",
    "                \n",
    "                # NLL (pou≈æijeme bezpeƒçnou funkci)\n",
    "                nll_loss = gaussian_nll_safe(target_seq, x_hat_seq, cov_diag_seq, max_error_sq=100.0)\n",
    "                \n",
    "                # Regularizace (pr≈Ømƒõr na krok)\n",
    "                reg_loss = regs_super.mean()\n",
    "                \n",
    "                # Rozhodov√°n√≠ o Loss\n",
    "                if train_iter_count < mse_warmup_iters:\n",
    "                    loss = mse_loss + reg_loss\n",
    "                    mode = \"MSE\"\n",
    "                else:\n",
    "                    # NLL F√°ze (m≈Ø≈æeme p≈ôidat malou kotvu MSE, nap≈ô. 0.1, pro stabilitu)\n",
    "                    loss = nll_loss + reg_loss + (0.1 * mse_loss) \n",
    "                    mode = \"NLL\"\n",
    "\n",
    "                # D) Backward & Update\n",
    "                loss.backward()\n",
    "                \n",
    "                if clip_grad > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() # D≈Øle≈æit√©: nulujeme po ka≈æd√©m kroku TBPTT\n",
    "                \n",
    "                # E) Detach Hidden State (Kl√≠ƒçov√© pro TBPTT)\n",
    "                model.detach_hidden()\n",
    "                \n",
    "                train_iter_count += 1\n",
    "\n",
    "                # --- Logging ---\n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    with torch.no_grad():\n",
    "                        p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                        p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                        diff = x_hat_seq - target_seq\n",
    "                        mae = diff.abs().mean().item()\n",
    "                    \n",
    "                    print(f\"Iter {train_iter_count} ({mode}): Loss {loss.item():.4f} | MSE {mse_loss.item():.2f} | NLL {nll_loss.item():.2f} | MAE {mae:.2f}m\")\n",
    "                    print(f\"    Dropout: p1={p1:.3f}, p2={p2:.3f} | VarMean: {cov_diag_seq.mean().item():.1f}\")\n",
    "\n",
    "                # --- Validation (Uvnit≈ô TBPTT smyƒçky) ---\n",
    "                if train_iter_count % validation_period == 0:\n",
    "                    model.eval()\n",
    "                    val_mse_list = []\n",
    "                    \n",
    "                    # Pro ANEES sbƒõr\n",
    "                    all_val_x_true, all_val_x_hat, all_val_P = [], [], []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for x_v, y_v in val_loader:\n",
    "                            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "                            b_v, s_v, _ = x_v.shape\n",
    "                            \n",
    "                            # Validace bƒõ≈æ√≠ Open-Loop na cel√© sekvenci (bez TBPTT)\n",
    "                            x_v_sup = x_v.repeat_interleave(J_samples, dim=0)\n",
    "                            y_v_sup = y_v.repeat_interleave(J_samples, dim=0)\n",
    "                            \n",
    "                            model.reset(batch_size=b_v*J_samples, initial_state=x_v_sup[:,0,:])\n",
    "                            preds_list = []\n",
    "                            \n",
    "                            for ti in range(1, s_v):\n",
    "                                est, _ = model.step(y_v_sup[:, ti, :])\n",
    "                                preds_list.append(est)\n",
    "                            \n",
    "                            preds_stack = torch.stack(preds_list, dim=1).view(b_v, J_samples, s_v-1, 4)\n",
    "                            val_mean = preds_stack.mean(dim=1)\n",
    "                            val_var = preds_stack.var(dim=1) + 1e-9\n",
    "                            \n",
    "                            val_mse_list.append(F.mse_loss(val_mean, x_v[:, 1:, :]).item())\n",
    "                            \n",
    "                            # Data pro ANEES\n",
    "                            full_hat = torch.cat([x_v[:,0,:].unsqueeze(1), val_mean], dim=1)\n",
    "                            # Diagonal P construction\n",
    "                            val_P_full = torch.zeros(b_v, s_v-1, 4, 4, device=device)\n",
    "                            for b in range(b_v):\n",
    "                                for t in range(s_v-1):\n",
    "                                    val_P_full[b,t] = torch.diag(val_var[b,t])\n",
    "                            P0 = torch.eye(4, device=device).unsqueeze(0).unsqueeze(0).repeat(b_v, 1, 1, 1)*1e-6\n",
    "                            full_P = torch.cat([P0, val_P_full], dim=1)\n",
    "                            \n",
    "                            all_val_x_true.append(x_v.cpu())\n",
    "                            all_val_x_hat.append(full_hat.cpu())\n",
    "                            all_val_P.append(full_P.cpu())\n",
    "\n",
    "                    avg_val_mse = np.mean(val_mse_list)\n",
    "                    \n",
    "                    # Calc ANEES\n",
    "                    try:\n",
    "                        cat_true = torch.cat(all_val_x_true, dim=0)\n",
    "                        cat_hat = torch.cat(all_val_x_hat, dim=0)\n",
    "                        cat_P = torch.cat(all_val_P, dim=0)\n",
    "                        # Pokud m√°te funkci importovanou\n",
    "                        avg_anees = calculate_anees_vectorized(cat_true, cat_hat, cat_P)\n",
    "                    except:\n",
    "                        avg_anees = float('nan')\n",
    "\n",
    "                    print(f\"\\n--- VALIDATION: MSE {avg_val_mse:.2f} | ANEES {avg_anees:.2f} ---\")\n",
    "                    \n",
    "                    # Ukl√°d√°n√≠ (podle MSE)\n",
    "                    if avg_val_mse < best_val_mse:\n",
    "                        print(f\"  >>> New Best Model! (Old: {best_val_mse:.2f} -> New: {avg_val_mse:.2f}) <<<\")\n",
    "                        best_val_mse = avg_val_mse\n",
    "                        best_model_state = deepcopy(model.state_dict())\n",
    "                    print(\"-\" * 40)\n",
    "                    \n",
    "                    model.train()\n",
    "\n",
    "    print(\"TBPTT Training completed.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e45239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    safe_var = var + min_var\n",
    "    error_sq = (preds - target) ** 2\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    return nll.mean()\n",
    "def calculate_anees_vectorized(x_true, x_hat, P_hat, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Robustn√≠ v√Ωpoƒçet ANEES pomoc√≠ Pseudo-Inverze.\n",
    "    Zvl√°d√° i situaci, kdy J_samples < State_Dim (singul√°rn√≠ matice).\n",
    "    \"\"\"\n",
    "    # 1. Zplo≈°tƒõn√≠ (Flattening)\n",
    "    if x_true.dim() == 3: x_true = x_true.reshape(-1, x_true.shape[-1])\n",
    "    if x_hat.dim() == 3: x_hat = x_hat.reshape(-1, x_hat.shape[-1])\n",
    "    if P_hat.dim() == 4: P_hat = P_hat.reshape(-1, P_hat.shape[-2], P_hat.shape[-1])\n",
    "\n",
    "    # 2. V√Ωpoƒçet chyby\n",
    "    error = (x_true - x_hat).unsqueeze(-1)  # [N, Dim, 1]\n",
    "    \n",
    "    # 3. Pseudo-Inverze matice P\n",
    "    # pinv je mnohem stabilnƒõj≈°√≠ pro Low-Rank matice (kdy≈æ m√°me m√°lo vzork≈Ø)\n",
    "    # hermitian=True ≈ô√≠k√°, ≈æe matice je symetrick√° (co≈æ kovariance je)\n",
    "    P_inv = torch.linalg.pinv(P_hat, hermitian=True)\n",
    "    \n",
    "    # 4. ANEES = error^T * P_inv * error\n",
    "    # bmm: [N, 1, Dim] @ [N, Dim, Dim] -> [N, 1, Dim]\n",
    "    #      [N, 1, Dim] @ [N, Dim, 1] -> [N, 1, 1]\n",
    "    \n",
    "    temp = torch.bmm(error.transpose(1, 2), P_inv)\n",
    "    anees_per_sample = torch.bmm(temp, error).squeeze() # [N]\n",
    "    \n",
    "    # Ochrana proti numerick√Ωm artefakt≈Øm (malink√° z√°porn√° ƒç√≠sla jako -1e-10)\n",
    "    anees_per_sample = torch.clamp(anees_per_sample, min=0.0)\n",
    "\n",
    "    return anees_per_sample.mean().item()\n",
    "\n",
    "def train_BayesianKalmanNet_TBPTT_Windowed(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad=1.0,\n",
    "    J_samples=5, \n",
    "    tbptt_k=5,   \n",
    "    tbptt_w=20,  \n",
    "    validation_period=50, logging_period=10,\n",
    "    mse_warmup_iters=0, \n",
    "    weight_decay_=1e-5,\n",
    "    lambda_mse=100.0,\n",
    "    calibration_parameter=10.0\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    best_val_score = float('inf')\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0 \n",
    "    done = False\n",
    "\n",
    "    if not hasattr(model, 'detach_hidden'):\n",
    "         raise AttributeError(\"Modelu chyb√≠ metoda 'detach_hidden()'.\")\n",
    "\n",
    "    print(f\"üöÄ START TBPTT Windowed Training (k={tbptt_k}, w={tbptt_w})\")\n",
    "    print(f\"    Phase 1: MSE Warmup (0 - {mse_warmup_iters} updates)\")\n",
    "    print(f\"    Phase 2: NLL Optimization (> {mse_warmup_iters} updates)\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            batch_size, seq_len, dim_x = x_true_batch.shape\n",
    "            \n",
    "            x_true_super = x_true_batch.repeat_interleave(J_samples, dim=0)\n",
    "            y_meas_super = y_meas_batch.repeat_interleave(J_samples, dim=0)\n",
    "            super_batch_size = x_true_super.shape[0]\n",
    "\n",
    "            # 1. Hard Reset na zaƒç√°tku sekvence\n",
    "            # Vyƒçist√≠me v≈°e, aby reset vytvo≈ôil spr√°vn√© velikosti\n",
    "            if hasattr(model, 'h_prev'): model.h_prev = None\n",
    "            if hasattr(model, 'x_filtered_t_minus_1'): model.x_filtered_t_minus_1 = None\n",
    "            if hasattr(model, 'x_filtered_t_minus_2'): model.x_filtered_t_minus_2 = None\n",
    "            \n",
    "            model.reset(batch_size=super_batch_size, initial_state=x_true_super[:, 0, :])\n",
    "            \n",
    "            # 2. TBPTT Loop\n",
    "            for t_start in range(1, seq_len, tbptt_w):\n",
    "                if train_iter_count >= total_train_iter: done = True; break\n",
    "\n",
    "                t_end = min(t_start + tbptt_w, seq_len)\n",
    "                current_window_len = t_end - t_start\n",
    "                if current_window_len <= 0: continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                window_x_preds = []\n",
    "                window_regs = []\n",
    "\n",
    "                # A) Forward pass\n",
    "                for t in range(t_start, t_end):\n",
    "                    y_t = y_meas_super[:, t, :]\n",
    "                    x_est, reg = model.step(y_t)\n",
    "                    window_x_preds.append(x_est)\n",
    "                    window_regs.append(reg)\n",
    "                    \n",
    "                    if (t - t_start + 1) % tbptt_k == 0:\n",
    "                        model.detach_hidden()\n",
    "\n",
    "                model.detach_hidden()\n",
    "\n",
    "                # B) Loss Calculation\n",
    "                preds_super = torch.stack(window_x_preds, dim=1)\n",
    "                regs_super = torch.stack(window_regs)\n",
    "                preds_reshaped = preds_super.view(batch_size, J_samples, current_window_len, dim_x)\n",
    "                \n",
    "                x_hat_seq = preds_reshaped.mean(dim=1)\n",
    "                cov_diag_seq = preds_reshaped.var(dim=1) + 1e-9\n",
    "                target_seq = x_true_batch[:, t_start:t_end, :]\n",
    "\n",
    "                mse_loss = F.mse_loss(x_hat_seq, target_seq)\n",
    "                nll_loss = gaussian_nll_safe(target_seq, x_hat_seq, cov_diag_seq, max_error_sq=100.0)\n",
    "                reg_loss = regs_super.mean()\n",
    "\n",
    "                if train_iter_count < mse_warmup_iters:\n",
    "                    loss = mse_loss + reg_loss\n",
    "                    mode = \"MSE_WARMUP\"\n",
    "                else:\n",
    "                    loss = nll_loss + reg_loss + (lambda_mse * mse_loss)\n",
    "                    mode = \"NLL_OPTIM\"\n",
    "\n",
    "                # C) Update\n",
    "                loss.backward()\n",
    "                if clip_grad > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() \n",
    "                model.detach_hidden()\n",
    "                \n",
    "                train_iter_count += 1\n",
    "                \n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    with torch.no_grad():\n",
    "                        diff = x_hat_seq - target_seq\n",
    "                        mae = diff.abs().mean().item()\n",
    "                        \n",
    "                        # Z√≠sk√°n√≠ hodnot dropoutu (sigmoida z logit≈Ø)\n",
    "                        p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                        p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                        \n",
    "                    print(f\"Iter {train_iter_count} ({mode}): \"\n",
    "                          f\"Loss {loss.item():.4f} | \"\n",
    "                          f\"MSE {mse_loss.item():.2f} | \"\n",
    "                          f\"NLL {nll_loss.item():.2f} | \"\n",
    "                          f\"Reg {reg_loss.item():.5f} | \" # Regularizaƒçn√≠ loss\n",
    "                          f\"MAE {mae:.2f}m | \"\n",
    "                          f\"p1={p1:.3f}, p2={p2:.3f}\")    # Dropout pravdƒõpodobnosti\n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    with torch.no_grad():\n",
    "                        diff = x_hat_seq - target_seq\n",
    "                        mae = diff.abs().mean().item()\n",
    "                    print(f\"Iter {train_iter_count} ({mode}): Loss {loss.item():.4f} | MSE {mse_loss.item():.2f} | NLL {nll_loss.item():.2f} | MAE {mae:.2f}m\")\n",
    "\n",
    "                # --- VALIDATION WITH FULL STATE RESTORE (FIXED) ---\n",
    "                if train_iter_count % validation_period == 0:\n",
    "                    # 1. ULO≈ΩIT KOMPLETN√ç STAV TR√âNINKU\n",
    "                    # Mus√≠me ulo≈æit √∫plnƒõ v≈°echno, co se mƒõn√≠ v ƒçase t\n",
    "                    train_state = {}\n",
    "                    \n",
    "                    if model.h_prev is not None:\n",
    "                        train_state['h_prev'] = model.h_prev.detach().clone()\n",
    "                    \n",
    "                    train_state['x_filt_1'] = model.x_filtered_t_minus_1.detach().clone()\n",
    "                    train_state['x_pred_1'] = model.x_pred_t_minus_1.detach().clone()\n",
    "                    \n",
    "                    # !!! ZDE BYLA CHYBA: Mus√≠me ulo≈æit i t-2 a y_t-1 !!!\n",
    "                    if hasattr(model, 'x_filtered_t_minus_2') and model.x_filtered_t_minus_2 is not None:\n",
    "                        train_state['x_filt_2'] = model.x_filtered_t_minus_2.detach().clone()\n",
    "                    \n",
    "                    if hasattr(model, 'y_t_minus_1') and model.y_t_minus_1 is not None:\n",
    "                        train_state['y_1'] = model.y_t_minus_1.detach().clone()\n",
    "                    \n",
    "                    if hasattr(model, 'P_t_minus_1') and model.P_t_minus_1 is not None:\n",
    "                        train_state['P'] = model.P_t_minus_1.detach().clone()\n",
    "\n",
    "                    # 2. Spustit Validaci\n",
    "                    model.eval()\n",
    "                    val_mse_list = []\n",
    "                    all_val_x_true, all_val_x_hat, all_val_P = [], [], []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for x_v, y_v in val_loader:\n",
    "                            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "                            b_v, s_v, _ = x_v.shape\n",
    "                            x_v_sup = x_v.repeat_interleave(J_samples, dim=0)\n",
    "                            y_v_sup = y_v.repeat_interleave(J_samples, dim=0)\n",
    "                            \n",
    "                            # Hard reset pro validaci (sma≈æe intern√≠ promƒõnn√©)\n",
    "                            if hasattr(model, 'h_prev'): model.h_prev = None\n",
    "                            model.reset(batch_size=b_v*J_samples, initial_state=x_v_sup[:,0,:])\n",
    "                            \n",
    "                            preds_list = []\n",
    "                            for ti in range(1, s_v):\n",
    "                                est, _ = model.step(y_v_sup[:, ti, :])\n",
    "                                preds_list.append(est)\n",
    "                            \n",
    "                            preds_stack = torch.stack(preds_list, dim=1).view(b_v, J_samples, s_v-1, 4)\n",
    "                            val_mean = preds_stack.mean(dim=1)\n",
    "                            val_var = preds_stack.var(dim=1) + 1e-9\n",
    "                            target_v = x_v[:, 1:, :] \n",
    "                            \n",
    "                            val_mse_list.append(F.mse_loss(val_mean, target_v).item())\n",
    "                            \n",
    "                            # Ukl√°d√°me jen srovnateln√© tensory (bez t=0)\n",
    "                            all_val_x_true.append(target_v.cpu()) # Zde byla chyba (bylo x_v)\n",
    "                            all_val_x_hat.append(val_mean.cpu())\n",
    "                            \n",
    "                            # P matice mus√≠ odpov√≠dat val_mean (tedy bez t=0)\n",
    "                            val_P_full = torch.zeros(b_v, s_v-1, 4, 4, device=device)\n",
    "                            for i in range(4): val_P_full[:, :, i, i] = val_var[:, :, i]\n",
    "                            # P0 nep≈ôid√°v√°me, proto≈æe nem√°me odhad pro t=0\n",
    "                            all_val_P.append(val_P_full.cpu())\n",
    "\n",
    "                    avg_val_mse = np.mean(val_mse_list)\n",
    "                    \n",
    "                    try:\n",
    "                        cat_true = torch.cat(all_val_x_true, dim=0)\n",
    "                        cat_hat = torch.cat(all_val_x_hat, dim=0)\n",
    "                        cat_P = torch.cat(all_val_P, dim=0)\n",
    "                        # Nyn√≠ maj√≠ tensory shodnou d√©lku, tak≈æe reshape projde\n",
    "                        avg_anees = calculate_anees_vectorized(cat_true, cat_hat, cat_P)\n",
    "                    except Exception as e:\n",
    "                        # Vyp√≠≈°eme chybu, a≈• v√≠me, co se dƒõje, m√≠sto tich√©ho nan\n",
    "                        print(f\"ANEES Error: {e}\")\n",
    "                        avg_anees = float('nan')\n",
    "\n",
    "                    anees_penalty = abs(avg_anees - 4.0) if not np.isnan(avg_anees) else 100.0\n",
    "                    \n",
    "                    # Score = MSE + 10 * ANEES_deviation\n",
    "                    # Pokud je ANEES 4.0, score = MSE. \n",
    "                    # Pokud je ANEES 20.0, score = MSE + 160 (v√Ωrazn√© zhor≈°en√≠).\n",
    "                    hybrid_score = avg_val_mse + (calibration_parameter * anees_penalty)\n",
    "\n",
    "                    print(f\"\\n--- VALIDATION: MSE {avg_val_mse:.2f} | ANEES {avg_anees:.2f} | Score {hybrid_score:.2f} ---\")\n",
    "                    \n",
    "                    # Ukl√°d√°me, pokud je hybridn√≠ sk√≥re lep≈°√≠ (men≈°√≠)\n",
    "                    if hybrid_score < best_val_score:\n",
    "                        print(f\"  >>> New Best Model! (Old Score: {best_val_score:.2f} -> New: {hybrid_score:.2f}) <<<\")\n",
    "                        best_val_score = hybrid_score\n",
    "                        best_model_state = deepcopy(model.state_dict())\n",
    "                    print(\"-\" * 40)\n",
    "                    \n",
    "                    # 3. OBNOVIT KOMPLETN√ç STAV TR√âNINKU\n",
    "                    model.train()\n",
    "                    if 'h_prev' in train_state: model.h_prev = train_state['h_prev']\n",
    "                    model.x_filtered_t_minus_1 = train_state['x_filt_1']\n",
    "                    model.x_pred_t_minus_1 = train_state['x_pred_1']\n",
    "                    \n",
    "                    if 'x_filt_2' in train_state: model.x_filtered_t_minus_2 = train_state['x_filt_2']\n",
    "                    if 'y_1' in train_state: model.y_t_minus_1 = train_state['y_1']\n",
    "                    if 'P' in train_state: model.P_t_minus_1 = train_state['P']\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b715c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    safe_var = var + min_var\n",
    "    error_sq = (preds - target) ** 2\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    return nll.mean()\n",
    "\n",
    "def calculate_anees_internal(x_true, x_hat, P_hat):\n",
    "    if P_hat.dim() == x_hat.dim(): # Diagon√°ln√≠ variance\n",
    "        error_sq = (x_true - x_hat) ** 2\n",
    "        nees_per_sample = (error_sq / (P_hat + 1e-9)).sum(dim=-1)\n",
    "        return nees_per_sample.mean().item()\n",
    "    return float('nan')\n",
    "\n",
    "def train_BayesianKalmanNet_TBPTT_Windowed(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad=1.0,\n",
    "    J_samples=5, \n",
    "    tbptt_k=5,   \n",
    "    tbptt_w=20,  \n",
    "    validation_period=50, logging_period=10,\n",
    "    mse_warmup_iters=0, \n",
    "    weight_decay_=1e-5,\n",
    "    lambda_mse=1.0,\n",
    "    calibration_parameter=10.0\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    best_val_score = float('inf')\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0 \n",
    "    done = False\n",
    "\n",
    "    if not hasattr(model, 'detach_hidden'):\n",
    "         raise AttributeError(\"Modelu chyb√≠ metoda 'detach_hidden()'.\")\n",
    "\n",
    "    print(f\"üöÄ START TBPTT Windowed Training (k={tbptt_k}, w={tbptt_w})\")\n",
    "    print(f\"    Phase 1: MSE Focused (Soft NLL) (0 - {mse_warmup_iters} updates)\")\n",
    "    print(f\"    Phase 2: NLL Optimization (> {mse_warmup_iters} updates)\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            batch_size, seq_len, dim_x = x_true_batch.shape\n",
    "            \n",
    "            x_true_super = x_true_batch.repeat_interleave(J_samples, dim=0)\n",
    "            y_meas_super = y_meas_batch.repeat_interleave(J_samples, dim=0)\n",
    "            super_batch_size = x_true_super.shape[0]\n",
    "\n",
    "            # 1. Hard Reset\n",
    "            if hasattr(model, 'h_prev'): model.h_prev = None\n",
    "            if hasattr(model, 'x_filtered_t_minus_1'): model.x_filtered_t_minus_1 = None\n",
    "            if hasattr(model, 'x_filtered_t_minus_2'): model.x_filtered_t_minus_2 = None\n",
    "            model.reset(batch_size=super_batch_size, initial_state=x_true_super[:, 0, :])\n",
    "            \n",
    "            # 2. TBPTT Loop\n",
    "            for t_start in range(1, seq_len, tbptt_w):\n",
    "                if train_iter_count >= total_train_iter: done = True; break\n",
    "\n",
    "                t_end = min(t_start + tbptt_w, seq_len)\n",
    "                if t_end - t_start <= 0: continue\n",
    "\n",
    "                # Teacher Forcing (20%)\n",
    "                use_teacher_forcing = (np.random.rand() < 0.2) \n",
    "                if use_teacher_forcing and t_start > 1:\n",
    "                    gt_state_prev = x_true_super[:, t_start-1, :]\n",
    "                    model.x_filtered_t_minus_1 = gt_state_prev.detach().clone()\n",
    "                    model.x_pred_t_minus_1 = gt_state_prev.detach().clone()\n",
    "                    model.detach_hidden() \n",
    "                else:\n",
    "                    model.detach_hidden()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                window_x_preds = []\n",
    "                window_regs = []\n",
    "\n",
    "                # A) Forward pass\n",
    "                for t in range(t_start, t_end):\n",
    "                    y_t = y_meas_super[:, t, :]\n",
    "                    x_est, reg = model.step(y_t)\n",
    "                    window_x_preds.append(x_est)\n",
    "                    window_regs.append(reg)\n",
    "                    \n",
    "                    if (t - t_start + 1) % tbptt_k == 0:\n",
    "                        model.detach_hidden()\n",
    "\n",
    "                model.detach_hidden()\n",
    "\n",
    "                # B) Loss Calculation\n",
    "                preds_super = torch.stack(window_x_preds, dim=1)\n",
    "                regs_super = torch.stack(window_regs)\n",
    "                preds_reshaped = preds_super.view(batch_size, J_samples, -1, dim_x)\n",
    "                \n",
    "                x_hat_seq = preds_reshaped.mean(dim=1)\n",
    "                cov_diag_seq = preds_reshaped.var(dim=1) + 1e-9\n",
    "                target_seq = x_true_batch[:, t_start:t_end, :]\n",
    "\n",
    "                mse_loss = F.mse_loss(x_hat_seq, target_seq)\n",
    "                nll_loss = gaussian_nll_safe(target_seq, x_hat_seq, cov_diag_seq, max_error_sq=100.0)\n",
    "                reg_loss = regs_super.mean()\n",
    "\n",
    "                # Soft Warmup Strategy\n",
    "                if train_iter_count < mse_warmup_iters:\n",
    "                    loss = (1.0 * mse_loss) + nll_loss + reg_loss\n",
    "                    mode = \"MSE_WARMUP\"\n",
    "                else:\n",
    "                    loss = nll_loss + reg_loss + (lambda_mse * mse_loss)\n",
    "                    mode = \"NLL_OPTIM\"\n",
    "\n",
    "                # C) Update\n",
    "                loss.backward()\n",
    "                if clip_grad > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() \n",
    "                model.detach_hidden()\n",
    "                \n",
    "                train_iter_count += 1\n",
    "\n",
    "                # --- LOGGING (S P≈òIDANOU METRIKOU SIGMA) ---\n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    with torch.no_grad():\n",
    "                        mae = (x_hat_seq - target_seq).abs().mean().item()\n",
    "                        # Pr≈Ømƒõrn√° smƒõrodatn√° odchylka v metrech (sqrt(variance))\n",
    "                        avg_sigma = cov_diag_seq.mean().sqrt().item()\n",
    "                        \n",
    "                        p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                        p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                        \n",
    "                    print(f\"Iter {train_iter_count} ({mode}): \"\n",
    "                          f\"Loss {loss.item():.2f} | \"\n",
    "                          f\"MSE {mse_loss.item():.2f} | \"\n",
    "                          f\"NLL {nll_loss.item():.2f} | \"\n",
    "                          f\"Sigma {avg_sigma:.2f}m | \" # <--- ZDE VID√çTE NEURƒåITOST\n",
    "                          f\"MAE {mae:.2f}m | \"\n",
    "                          f\"p1={p1:.2f}, p2={p2:.2f}\")\n",
    "\n",
    "                # --- VALIDATION ---\n",
    "                if train_iter_count % validation_period == 0:\n",
    "                    train_state = {}\n",
    "                    if model.h_prev is not None: train_state['h_prev'] = model.h_prev.detach().clone()\n",
    "                    train_state['x_filt_1'] = model.x_filtered_t_minus_1.detach().clone()\n",
    "                    train_state['x_pred_1'] = model.x_pred_t_minus_1.detach().clone()\n",
    "                    if hasattr(model, 'x_filtered_t_minus_2') and model.x_filtered_t_minus_2 is not None:\n",
    "                        train_state['x_filt_2'] = model.x_filtered_t_minus_2.detach().clone()\n",
    "                    if hasattr(model, 'y_t_minus_1') and model.y_t_minus_1 is not None:\n",
    "                        train_state['y_1'] = model.y_t_minus_1.detach().clone()\n",
    "                    if hasattr(model, 'P_t_minus_1') and model.P_t_minus_1 is not None:\n",
    "                        train_state['P'] = model.P_t_minus_1.detach().clone()\n",
    "\n",
    "                    model.eval()\n",
    "                    val_mse_list = []\n",
    "                    all_val_x_true, all_val_x_hat, all_val_P = [], [], []\n",
    "                    \n",
    "                    # Pro v√Ωpoƒçet pr≈Ømƒõrn√© sigmy ve validaci\n",
    "                    val_sigma_sum = 0.0\n",
    "                    val_batch_count = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for x_v, y_v in val_loader:\n",
    "                            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "                            b_v, s_v, _ = x_v.shape\n",
    "                            x_v_sup = x_v.repeat_interleave(J_samples, dim=0)\n",
    "                            y_v_sup = y_v.repeat_interleave(J_samples, dim=0)\n",
    "                            \n",
    "                            if hasattr(model, 'h_prev'): model.h_prev = None\n",
    "                            model.reset(batch_size=b_v*J_samples, initial_state=x_v_sup[:,0,:])\n",
    "                            \n",
    "                            preds_list = []\n",
    "                            for ti in range(1, s_v):\n",
    "                                est, _ = model.step(y_v_sup[:, ti, :])\n",
    "                                preds_list.append(est)\n",
    "                            \n",
    "                            preds_stack = torch.stack(preds_list, dim=1).view(b_v, J_samples, s_v-1, 4)\n",
    "                            val_mean = preds_stack.mean(dim=1)\n",
    "                            val_var = preds_stack.var(dim=1) + 1e-9\n",
    "                            target_v = x_v[:, 1:, :] \n",
    "                            \n",
    "                            val_mse_list.append(F.mse_loss(val_mean, target_v).item())\n",
    "                            \n",
    "                            # Logging sigma stats\n",
    "                            val_sigma_sum += val_var.mean().sqrt().item()\n",
    "                            val_batch_count += 1\n",
    "                            \n",
    "                            all_val_x_true.append(target_v)\n",
    "                            all_val_x_hat.append(val_mean)\n",
    "                            all_val_P.append(val_var)\n",
    "\n",
    "                    avg_val_mse = np.mean(val_mse_list)\n",
    "                    avg_val_sigma = val_sigma_sum / max(1, val_batch_count)\n",
    "                    \n",
    "                    try:\n",
    "                        cat_true = torch.cat(all_val_x_true, dim=0)\n",
    "                        cat_hat = torch.cat(all_val_x_hat, dim=0)\n",
    "                        cat_P = torch.cat(all_val_P, dim=0)\n",
    "                        avg_anees = calculate_anees_internal(cat_true, cat_hat, cat_P)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ANEES Error: {e}\")\n",
    "                        avg_anees = 100.0\n",
    "\n",
    "                    anees_penalty = abs(avg_anees - 4.0)\n",
    "                    hybrid_score = avg_val_mse + (calibration_parameter * anees_penalty)\n",
    "\n",
    "                    print(f\"--- VALIDATION: MSE {avg_val_mse:.2f} | Sigma {avg_val_sigma:.2f}m | ANEES {avg_anees:.2f} | Score {hybrid_score:.2f} ---\")\n",
    "                    \n",
    "                    if hybrid_score < best_val_score:\n",
    "                        print(f\"  >>> New Best Model! (Score: {best_val_score:.2f} -> {hybrid_score:.2f}) <<<\")\n",
    "                        best_val_score = hybrid_score\n",
    "                        best_model_state = deepcopy(model.state_dict())\n",
    "                    print(\"-\" * 40)\n",
    "                    \n",
    "                    # 3. Restore State\n",
    "                    model.train()\n",
    "                    if 'h_prev' in train_state: model.h_prev = train_state['h_prev']\n",
    "                    model.x_filtered_t_minus_1 = train_state['x_filt_1']\n",
    "                    model.x_pred_t_minus_1 = train_state['x_pred_1']\n",
    "                    if 'x_filt_2' in train_state: model.x_filtered_t_minus_2 = train_state['x_filt_2']\n",
    "                    if 'y_1' in train_state: model.y_t_minus_1 = train_state['y_1']\n",
    "                    if 'P' in train_state: model.P_t_minus_1 = train_state['P']\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864566d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "# Ensure utils is importable if needed\n",
    "# import utils.utils as utils \n",
    "\n",
    "def gaussian_nll_safe(target, preds, var, min_var=1e-6, max_error_sq=100.0):\n",
    "    \"\"\"\n",
    "    Bezpeƒçn√° NLL loss funkce.\n",
    "    \"\"\"\n",
    "    safe_var = var + min_var\n",
    "    error_sq = (preds - target) ** 2\n",
    "    # Clampujeme velikost chyby v ƒçitateli, aby loss neexplodovala\n",
    "    error_sq_clamped = torch.clamp(error_sq, max=max_error_sq)\n",
    "    nll = 0.5 * (torch.log(safe_var) + error_sq_clamped / safe_var)\n",
    "    return nll.mean()\n",
    "\n",
    "def strain_BayesianKalmanNet_TwoPhase(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    mse_warmup_iters=0,\n",
    "    calibration_parameter=0.0, # <--- NOV√ù PARAMETR: V√°ha pro ANEES v hybridn√≠m sk√≥re\n",
    "    weight_decay_=1e-5\n",
    "):\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Tracking\n",
    "    best_hybrid_score = float('inf') # Hlavn√≠ metrika pro ukl√°d√°n√≠\n",
    "    best_model_state = None\n",
    "    best_iter_count = 0\n",
    "    \n",
    "    # V√Ωsledky pro return\n",
    "    best_val_mse = float('inf')\n",
    "    best_val_anees = float('inf')\n",
    "    best_val_nll = float('inf')\n",
    "\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Two-Phase Training\")\n",
    "    print(f\"    Phase 1: MSE Warmup (0 - {mse_warmup_iters} iters)\")\n",
    "    print(f\"    Phase 2: NLL Optimization ({mse_warmup_iters} - {total_train_iter} iters)\")\n",
    "    print(f\"    Saving Strategy: Hybrid Score = MSE + ({calibration_parameter} * |ANEES - 4.0|)\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            # Detekce NaN v datech\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            # --- Training Step ---\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                \n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    \n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                        # Fail-safe pro numerickou nestabilitu\n",
    "                        print(f\"NaN detected in forward pass at iter {train_iter_count}\")\n",
    "                        loss = torch.tensor(float('nan'), requires_grad=True) # Dummy NaN loss\n",
    "                        break \n",
    "                        \n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                \n",
    "                if len(current_trajectory_x_hats) != (seq_len - 1): break # Pokud nastal break v inner loop\n",
    "\n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            if len(all_trajectories_for_ensemble) < J_samples:\n",
    "                 # Pokud nƒõjak√Ω sample selhal, p≈ôeskoƒç√≠me update\n",
    "                 optimizer.zero_grad()\n",
    "                 continue\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-9 \n",
    "            \n",
    "            # Regularizace\n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean() / seq_len\n",
    "            \n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET LOSS ---\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            nll_loss = gaussian_nll_safe(\n",
    "                target=target_sequence, \n",
    "                preds=x_hat_sequence, \n",
    "                var=cov_diag_sequence\n",
    "            )\n",
    "            \n",
    "            # P≈ôep√≠n√°n√≠ f√°z√≠\n",
    "            mode = \"\"\n",
    "            if train_iter_count < mse_warmup_iters:\n",
    "                loss = mse_loss + regularization_loss\n",
    "                mode = \"Warmup\"\n",
    "            else:\n",
    "                loss = nll_loss + regularization_loss\n",
    "                mode = \"Optim\"\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad > 0: \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- LOGGING ---\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # V√Ωpoƒçet pr≈Ømƒõrn√© smƒõrodatn√© odchylky v metrech (Sigma)\n",
    "                    # cov_diag_sequence je variance [m^2], odmocn√≠me pro metry\n",
    "                    avg_sigma = torch.sqrt(cov_diag_sequence).mean().item()\n",
    "                    \n",
    "                    # MAE (Mean Absolute Error) v metrech\n",
    "                    mae = (x_hat_sequence - target_sequence).abs().mean().item()\n",
    "                    \n",
    "                    # Dropout pravdƒõpodobnosti\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                \n",
    "                # Form√°tovan√Ω v√Ωpis dle po≈æadavku\n",
    "                print(f\"Iter {train_iter_count} ({mode}): \"\n",
    "                      f\"Loss {loss.item():.2f} | \"\n",
    "                      f\"MSE {mse_loss.item():.2f} | \"\n",
    "                      f\"NLL {nll_loss.item():.2f} | \"\n",
    "                      f\"Sigma {avg_sigma:.2f}m | \" \n",
    "                      f\"MAE {mae:.2f}m | \"\n",
    "                      f\"p1={p1:.2f}, p2={p2:.2f}\")\n",
    "\n",
    "            # --- VALIDATION ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                val_nll_list = []\n",
    "                \n",
    "                all_val_x_true, all_val_x_hat, all_val_P_hat = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val, y_meas_val in val_loader:\n",
    "                        v_bs, v_seq, _ = x_true_val.shape\n",
    "                        x_true_val = x_true_val.to(device)\n",
    "                        y_meas_val = y_meas_val.to(device)\n",
    "                        \n",
    "                        val_ensemble_trajs = []\n",
    "                        \n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=v_bs, initial_state=x_true_val[:, 0, :])\n",
    "                            v_x_hats = []\n",
    "                            for t in range(1, v_seq):\n",
    "                                est, _ = model.step(y_meas_val[:, t, :])\n",
    "                                v_x_hats.append(est)\n",
    "                            val_ensemble_trajs.append(torch.stack(v_x_hats, dim=1))\n",
    "                        \n",
    "                        val_ens_stack = torch.stack(val_ensemble_trajs, dim=0)\n",
    "                        val_mean = val_ens_stack.mean(dim=0)\n",
    "                        val_var_diag = val_ens_stack.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        # Metriky\n",
    "                        val_mse_list.append(F.mse_loss(val_mean, x_true_val[:, 1:, :]).item())\n",
    "                        val_nll_list.append(gaussian_nll_safe(x_true_val[:, 1:, :], val_mean, val_var_diag).item())\n",
    "                        \n",
    "                        # Data pro ANEES\n",
    "                        full_x_hat = torch.cat([x_true_val[:, 0, :].unsqueeze(1), val_mean], dim=1)\n",
    "                        \n",
    "                        # Konstrukce pln√© kovarianƒçn√≠ matice P\n",
    "                        # Pou≈æijeme diag_embed pro efektivitu, fallback na cyklus\n",
    "                        try:\n",
    "                            val_covs_full = torch.diag_embed(val_var_diag)\n",
    "                        except:\n",
    "                            val_covs_full = torch.zeros(v_bs, v_seq-1, 4, 4, device=device)\n",
    "                            for b in range(v_bs):\n",
    "                                for t in range(v_seq-1):\n",
    "                                    val_covs_full[b, t] = torch.diag(val_var_diag[b, t])\n",
    "                        \n",
    "                        P0 = torch.eye(4, device=device).unsqueeze(0).unsqueeze(0).repeat(v_bs, 1, 1, 1) * 1e-6\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true.append(x_true_val.cpu())\n",
    "                        all_val_x_hat.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                avg_val_nll = np.mean(val_nll_list)\n",
    "                \n",
    "                # V√Ωpoƒçet ANEES\n",
    "                try:\n",
    "                    # Zde p≈ôedpokl√°d√°m, ≈æe funkce calculate_anees_vectorized je dostupn√°\n",
    "                    # buƒè v 'trainer' nebo importovan√° z 'utils'\n",
    "                    from utils import trainer as tr_utils # Lok√°ln√≠ import pro jistotu\n",
    "                    avg_val_anees = tr_utils.calculate_anees_vectorized(\n",
    "                        torch.cat(all_val_x_true, dim=0), \n",
    "                        torch.cat(all_val_x_hat, dim=0), \n",
    "                        torch.cat(all_val_P_hat, dim=0)\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback pokud import sel≈æe, nastav√≠me na 4.0 (ide√°l) aby nezkazilo sk√≥re\n",
    "                    # nebo NaN pro info\n",
    "                    avg_val_anees = 100.0 # Velk√© ƒç√≠slo aby to bylo vidƒõt\n",
    "                    # print(\"Warning: ANEES calculation failed.\")\n",
    "\n",
    "                # --- HYBRID SCORE CALCULATION ---\n",
    "                anees_diff = abs(avg_val_anees - 4.0)\n",
    "                hybrid_score = avg_val_mse + (calibration_parameter * anees_diff)\n",
    "\n",
    "                print(f\"  > Val MSE: {avg_val_mse:.4f} | Val ANEES: {avg_val_anees:.4f} | Hybrid Score: {hybrid_score:.4f}\")\n",
    "                \n",
    "                # Ukl√°d√°n√≠ podle Hybrid Score\n",
    "                if hybrid_score < best_hybrid_score:\n",
    "                    print(f\"  >>> ‚≠ê New Best Model! (Score: {best_hybrid_score:.4f} -> {hybrid_score:.4f})\")\n",
    "                    best_hybrid_score = hybrid_score\n",
    "                    best_iter_count = train_iter_count\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                    \n",
    "                    # Ulo≈æ√≠me si metriky tohoto nejlep≈°√≠ho modelu\n",
    "                    best_val_mse = avg_val_mse\n",
    "                    best_val_anees = avg_val_anees\n",
    "                    best_val_nll = avg_val_nll\n",
    "                \n",
    "                print(\"-\" * 60)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count} (Score: {best_hybrid_score:.4f})\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Vrac√≠me slovn√≠k v√Ωsledk≈Ø pro ulo≈æen√≠\n",
    "    return {\n",
    "        \"final_model\": model,\n",
    "        \"best_iter\": best_iter_count,\n",
    "        \"best_val_mse\": best_val_mse,\n",
    "        \"best_val_anees\": best_val_anees,\n",
    "        \"best_val_nll\": best_val_nll\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE BKN MODELU ===\n",
      "INFO: Aplikuji upravenou inicializaci pro BKN.\n",
      "DEBUG: V√Ωstupn√≠ vrstva inicializov√°na konzervativnƒõ (interval -0.1 a≈æ 0.1).\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 1: SeqLen 10 | LR 0.001 | Lambda MSE: 1.0\n",
      "============================================================\n",
      "   -> Using Standard Hybrid Training\n",
      "üöÄ START Two-Phase Training\n",
      "    Phase 1: MSE Warmup (0 - 1000 iters)\n",
      "    Phase 2: NLL Optimization (1000 - 2000 iters)\n",
      "    Saving Strategy: Hybrid Score = MSE + (10.0 * |ANEES - 4.0|)\n",
      "Iter 10 (Warmup): Loss 326.40 | MSE 326.40 | NLL 16.82 | Sigma 7.13m | MAE 10.30m | p1=0.43, p2=0.44\n",
      "\n",
      "--- Validation at iteration 10 ---\n",
      "  > Val MSE: 284.0581 | Val ANEES: 15.8909 | Hybrid Score: 402.9667\n",
      "  >>> ‚≠ê New Best Model! (Score: inf -> 402.9667)\n",
      "------------------------------------------------------------\n",
      "Iter 20 (Warmup): Loss 80.82 | MSE 80.81 | NLL 2.60 | Sigma 5.90m | MAE 5.54m | p1=0.43, p2=0.44\n",
      "\n",
      "--- Validation at iteration 20 ---\n",
      "  > Val MSE: 74.0805 | Val ANEES: 9.6974 | Hybrid Score: 131.0545\n",
      "  >>> ‚≠ê New Best Model! (Score: 402.9667 -> 131.0545)\n",
      "------------------------------------------------------------\n",
      "Iter 30 (Warmup): Loss 39.61 | MSE 39.60 | NLL 2.22 | Sigma 5.13m | MAE 3.84m | p1=0.42, p2=0.44\n",
      "\n",
      "--- Validation at iteration 30 ---\n",
      "  > Val MSE: 46.8369 | Val ANEES: 6.1683 | Hybrid Score: 68.5195\n",
      "  >>> ‚≠ê New Best Model! (Score: 131.0545 -> 68.5195)\n",
      "------------------------------------------------------------\n",
      "Iter 40 (Warmup): Loss 40.42 | MSE 40.42 | NLL 1.85 | Sigma 5.40m | MAE 3.39m | p1=0.42, p2=0.44\n",
      "\n",
      "--- Validation at iteration 40 ---\n",
      "  > Val MSE: 32.8545 | Val ANEES: 4.1143 | Hybrid Score: 33.9980\n",
      "  >>> ‚≠ê New Best Model! (Score: 68.5195 -> 33.9980)\n",
      "------------------------------------------------------------\n",
      "Iter 50 (Warmup): Loss 25.59 | MSE 25.58 | NLL 1.83 | Sigma 5.17m | MAE 3.11m | p1=0.42, p2=0.43\n",
      "\n",
      "--- Validation at iteration 50 ---\n",
      "  > Val MSE: 32.9075 | Val ANEES: 4.2068 | Hybrid Score: 34.9759\n",
      "------------------------------------------------------------\n",
      "Iter 60 (Warmup): Loss 40.68 | MSE 40.68 | NLL 1.86 | Sigma 5.29m | MAE 3.20m | p1=0.41, p2=0.43\n",
      "\n",
      "--- Validation at iteration 60 ---\n",
      "  > Val MSE: 33.1187 | Val ANEES: 4.2636 | Hybrid Score: 35.7551\n",
      "------------------------------------------------------------\n",
      "Iter 70 (Warmup): Loss 20.12 | MSE 20.12 | NLL 1.81 | Sigma 4.84m | MAE 2.82m | p1=0.41, p2=0.43\n",
      "\n",
      "--- Validation at iteration 70 ---\n",
      "  > Val MSE: 24.2038 | Val ANEES: 4.3532 | Hybrid Score: 27.7361\n",
      "  >>> ‚≠ê New Best Model! (Score: 33.9980 -> 27.7361)\n",
      "------------------------------------------------------------\n",
      "Iter 80 (Warmup): Loss 17.81 | MSE 17.81 | NLL 1.78 | Sigma 4.88m | MAE 2.71m | p1=0.41, p2=0.42\n",
      "\n",
      "--- Validation at iteration 80 ---\n",
      "  > Val MSE: 25.8371 | Val ANEES: 4.3530 | Hybrid Score: 29.3673\n",
      "------------------------------------------------------------\n",
      "Iter 90 (Warmup): Loss 21.89 | MSE 21.89 | NLL 15.35 | Sigma 4.79m | MAE 2.84m | p1=0.40, p2=0.42\n",
      "\n",
      "--- Validation at iteration 90 ---\n",
      "  > Val MSE: 33.6512 | Val ANEES: 5.0215 | Hybrid Score: 43.8663\n",
      "------------------------------------------------------------\n",
      "Iter 100 (Warmup): Loss 18.02 | MSE 18.02 | NLL 1.76 | Sigma 4.51m | MAE 2.68m | p1=0.40, p2=0.42\n",
      "\n",
      "--- Validation at iteration 100 ---\n",
      "  > Val MSE: 31.2969 | Val ANEES: 4.6418 | Hybrid Score: 37.7148\n",
      "------------------------------------------------------------\n",
      "Iter 110 (Warmup): Loss 15.48 | MSE 15.48 | NLL 1.82 | Sigma 4.61m | MAE 2.56m | p1=0.40, p2=0.42\n",
      "\n",
      "--- Validation at iteration 110 ---\n",
      "  > Val MSE: 27.4778 | Val ANEES: 4.3967 | Hybrid Score: 31.4445\n",
      "------------------------------------------------------------\n",
      "Iter 120 (Warmup): Loss 14.24 | MSE 14.24 | NLL 1.78 | Sigma 4.32m | MAE 2.53m | p1=0.40, p2=0.41\n",
      "\n",
      "--- Validation at iteration 120 ---\n",
      "  > Val MSE: 37.8281 | Val ANEES: 4.5026 | Hybrid Score: 42.8536\n",
      "------------------------------------------------------------\n",
      "Iter 130 (Warmup): Loss 31.42 | MSE 31.41 | NLL 1.76 | Sigma 4.67m | MAE 2.73m | p1=0.39, p2=0.41\n",
      "\n",
      "--- Validation at iteration 130 ---\n",
      "  > Val MSE: 23.5246 | Val ANEES: 4.4224 | Hybrid Score: 27.7481\n",
      "------------------------------------------------------------\n",
      "Iter 140 (Warmup): Loss 20.09 | MSE 20.09 | NLL 1.73 | Sigma 4.60m | MAE 2.67m | p1=0.39, p2=0.41\n",
      "\n",
      "--- Validation at iteration 140 ---\n",
      "  > Val MSE: 25.6657 | Val ANEES: 4.4561 | Hybrid Score: 30.2270\n",
      "------------------------------------------------------------\n",
      "Iter 150 (Warmup): Loss 14.53 | MSE 14.53 | NLL 1.71 | Sigma 4.26m | MAE 2.49m | p1=0.39, p2=0.41\n",
      "\n",
      "--- Validation at iteration 150 ---\n",
      "  > Val MSE: 22.6989 | Val ANEES: 4.1261 | Hybrid Score: 23.9601\n",
      "  >>> ‚≠ê New Best Model! (Score: 27.7361 -> 23.9601)\n",
      "------------------------------------------------------------\n",
      "Iter 160 (Warmup): Loss 12.16 | MSE 12.16 | NLL 1.67 | Sigma 4.26m | MAE 2.33m | p1=0.38, p2=0.40\n",
      "\n",
      "--- Validation at iteration 160 ---\n",
      "  > Val MSE: 22.2327 | Val ANEES: 4.5425 | Hybrid Score: 27.6582\n",
      "------------------------------------------------------------\n",
      "Iter 170 (Warmup): Loss 15.55 | MSE 15.55 | NLL 1.71 | Sigma 4.52m | MAE 2.47m | p1=0.38, p2=0.40\n",
      "\n",
      "--- Validation at iteration 170 ---\n",
      "  > Val MSE: 25.4928 | Val ANEES: 4.2071 | Hybrid Score: 27.5642\n",
      "------------------------------------------------------------\n",
      "Iter 180 (Warmup): Loss 20.40 | MSE 20.40 | NLL 15.29 | Sigma 4.50m | MAE 2.67m | p1=0.38, p2=0.40\n",
      "\n",
      "--- Validation at iteration 180 ---\n",
      "  > Val MSE: 27.5889 | Val ANEES: 4.7278 | Hybrid Score: 34.8673\n",
      "------------------------------------------------------------\n",
      "Iter 190 (Warmup): Loss 13.48 | MSE 13.48 | NLL 1.68 | Sigma 4.36m | MAE 2.44m | p1=0.37, p2=0.40\n",
      "\n",
      "--- Validation at iteration 190 ---\n",
      "  > Val MSE: 29.5943 | Val ANEES: 4.4424 | Hybrid Score: 34.0180\n",
      "------------------------------------------------------------\n",
      "Iter 200 (Warmup): Loss 13.25 | MSE 13.25 | NLL 1.79 | Sigma 4.23m | MAE 2.42m | p1=0.37, p2=0.39\n",
      "\n",
      "--- Validation at iteration 200 ---\n",
      "  > Val MSE: 22.3633 | Val ANEES: 4.6250 | Hybrid Score: 28.6137\n",
      "------------------------------------------------------------\n",
      "Iter 210 (Warmup): Loss 10.72 | MSE 10.72 | NLL 1.70 | Sigma 4.34m | MAE 2.24m | p1=0.37, p2=0.39\n",
      "\n",
      "--- Validation at iteration 210 ---\n",
      "  > Val MSE: 21.5824 | Val ANEES: 4.4309 | Hybrid Score: 25.8913\n",
      "------------------------------------------------------------\n",
      "Iter 220 (Warmup): Loss 10.65 | MSE 10.65 | NLL 1.72 | Sigma 4.10m | MAE 2.20m | p1=0.37, p2=0.39\n",
      "\n",
      "--- Validation at iteration 220 ---\n",
      "  > Val MSE: 26.6442 | Val ANEES: 4.6518 | Hybrid Score: 33.1625\n",
      "------------------------------------------------------------\n",
      "Iter 230 (Warmup): Loss 17.77 | MSE 17.77 | NLL 1.83 | Sigma 4.31m | MAE 2.39m | p1=0.36, p2=0.39\n",
      "\n",
      "--- Validation at iteration 230 ---\n",
      "  > Val MSE: 19.6164 | Val ANEES: 4.5101 | Hybrid Score: 24.7178\n",
      "------------------------------------------------------------\n",
      "Iter 240 (Warmup): Loss 17.83 | MSE 17.82 | NLL 1.73 | Sigma 4.48m | MAE 2.45m | p1=0.36, p2=0.39\n",
      "\n",
      "--- Validation at iteration 240 ---\n",
      "  > Val MSE: 19.8180 | Val ANEES: 4.4307 | Hybrid Score: 24.1252\n",
      "------------------------------------------------------------\n",
      "Iter 250 (Warmup): Loss 20.11 | MSE 20.10 | NLL 1.71 | Sigma 4.40m | MAE 2.36m | p1=0.36, p2=0.38\n",
      "\n",
      "--- Validation at iteration 250 ---\n",
      "  > Val MSE: 18.1315 | Val ANEES: 4.5339 | Hybrid Score: 23.4702\n",
      "  >>> ‚≠ê New Best Model! (Score: 23.9601 -> 23.4702)\n",
      "------------------------------------------------------------\n",
      "Iter 260 (Warmup): Loss 10.36 | MSE 10.36 | NLL 15.20 | Sigma 3.95m | MAE 2.13m | p1=0.36, p2=0.38\n",
      "\n",
      "--- Validation at iteration 260 ---\n",
      "  > Val MSE: 16.3274 | Val ANEES: 4.6839 | Hybrid Score: 23.1661\n",
      "  >>> ‚≠ê New Best Model! (Score: 23.4702 -> 23.1661)\n",
      "------------------------------------------------------------\n",
      "Iter 270 (Warmup): Loss 12.05 | MSE 12.05 | NLL 1.79 | Sigma 4.23m | MAE 2.29m | p1=0.35, p2=0.38\n",
      "\n",
      "--- Validation at iteration 270 ---\n",
      "  > Val MSE: 16.7490 | Val ANEES: 4.7390 | Hybrid Score: 24.1392\n",
      "------------------------------------------------------------\n",
      "Iter 280 (Warmup): Loss 17.06 | MSE 17.06 | NLL 1.71 | Sigma 4.51m | MAE 2.41m | p1=0.35, p2=0.38\n",
      "\n",
      "--- Validation at iteration 280 ---\n",
      "  > Val MSE: 22.3072 | Val ANEES: 4.7084 | Hybrid Score: 29.3913\n",
      "------------------------------------------------------------\n",
      "Iter 290 (Warmup): Loss 12.27 | MSE 12.26 | NLL 1.70 | Sigma 3.85m | MAE 2.20m | p1=0.35, p2=0.37\n",
      "\n",
      "--- Validation at iteration 290 ---\n",
      "  > Val MSE: 22.8671 | Val ANEES: 4.9320 | Hybrid Score: 32.1871\n",
      "------------------------------------------------------------\n",
      "Iter 300 (Warmup): Loss 16.21 | MSE 16.21 | NLL 15.28 | Sigma 4.32m | MAE 2.28m | p1=0.35, p2=0.37\n",
      "\n",
      "--- Validation at iteration 300 ---\n",
      "  > Val MSE: 21.5368 | Val ANEES: 4.9480 | Hybrid Score: 31.0167\n",
      "------------------------------------------------------------\n",
      "Iter 310 (Warmup): Loss 15.27 | MSE 15.26 | NLL 1.71 | Sigma 4.09m | MAE 2.16m | p1=0.34, p2=0.37\n",
      "\n",
      "--- Validation at iteration 310 ---\n",
      "  > Val MSE: 30.9666 | Val ANEES: 5.3202 | Hybrid Score: 44.1685\n",
      "------------------------------------------------------------\n",
      "Iter 320 (Warmup): Loss 9.94 | MSE 9.93 | NLL 18.39 | Sigma 3.93m | MAE 2.06m | p1=0.34, p2=0.37\n",
      "\n",
      "--- Validation at iteration 320 ---\n",
      "  > Val MSE: 14.2650 | Val ANEES: 5.1663 | Hybrid Score: 25.9285\n",
      "------------------------------------------------------------\n",
      "Iter 330 (Warmup): Loss 16.30 | MSE 16.30 | NLL 1.66 | Sigma 4.08m | MAE 2.11m | p1=0.34, p2=0.36\n",
      "\n",
      "--- Validation at iteration 330 ---\n",
      "  > Val MSE: 16.6019 | Val ANEES: 5.1472 | Hybrid Score: 28.0734\n",
      "------------------------------------------------------------\n",
      "Iter 340 (Warmup): Loss 9.42 | MSE 9.42 | NLL 1.73 | Sigma 3.86m | MAE 2.12m | p1=0.34, p2=0.36\n",
      "\n",
      "--- Validation at iteration 340 ---\n",
      "  > Val MSE: 15.4530 | Val ANEES: 5.1993 | Hybrid Score: 27.4463\n",
      "------------------------------------------------------------\n",
      "Iter 350 (Warmup): Loss 11.72 | MSE 11.72 | NLL 1.69 | Sigma 3.89m | MAE 2.11m | p1=0.33, p2=0.36\n",
      "\n",
      "--- Validation at iteration 350 ---\n",
      "  > Val MSE: 24.2359 | Val ANEES: 5.4064 | Hybrid Score: 38.3002\n",
      "------------------------------------------------------------\n",
      "Iter 360 (Warmup): Loss 9.43 | MSE 9.43 | NLL 1.68 | Sigma 3.79m | MAE 2.06m | p1=0.33, p2=0.36\n",
      "\n",
      "--- Validation at iteration 360 ---\n",
      "  > Val MSE: 24.0591 | Val ANEES: 5.4488 | Hybrid Score: 38.5472\n",
      "------------------------------------------------------------\n",
      "Iter 370 (Warmup): Loss 8.94 | MSE 8.94 | NLL 1.75 | Sigma 3.79m | MAE 2.03m | p1=0.33, p2=0.36\n",
      "\n",
      "--- Validation at iteration 370 ---\n",
      "  > Val MSE: 33.9145 | Val ANEES: 5.9205 | Hybrid Score: 53.1194\n",
      "------------------------------------------------------------\n",
      "Iter 380 (Warmup): Loss 8.69 | MSE 8.68 | NLL 1.78 | Sigma 3.80m | MAE 2.00m | p1=0.33, p2=0.35\n",
      "\n",
      "--- Validation at iteration 380 ---\n",
      "  > Val MSE: 16.1196 | Val ANEES: 5.6863 | Hybrid Score: 32.9830\n",
      "------------------------------------------------------------\n",
      "Iter 390 (Warmup): Loss 8.70 | MSE 8.70 | NLL 1.90 | Sigma 3.92m | MAE 1.99m | p1=0.32, p2=0.35\n",
      "\n",
      "--- Validation at iteration 390 ---\n",
      "  > Val MSE: 12.0931 | Val ANEES: 5.6472 | Hybrid Score: 28.5653\n",
      "------------------------------------------------------------\n",
      "Iter 400 (Warmup): Loss 8.97 | MSE 8.97 | NLL 1.73 | Sigma 3.67m | MAE 2.01m | p1=0.32, p2=0.35\n",
      "\n",
      "--- Validation at iteration 400 ---\n",
      "  > Val MSE: 11.5270 | Val ANEES: 5.7929 | Hybrid Score: 29.4561\n",
      "------------------------------------------------------------\n",
      "Iter 410 (Warmup): Loss 9.06 | MSE 9.06 | NLL 1.64 | Sigma 3.57m | MAE 2.04m | p1=0.32, p2=0.35\n",
      "\n",
      "--- Validation at iteration 410 ---\n",
      "  > Val MSE: 12.8152 | Val ANEES: 6.0904 | Hybrid Score: 33.7195\n",
      "------------------------------------------------------------\n",
      "Iter 420 (Warmup): Loss 9.54 | MSE 9.54 | NLL 15.27 | Sigma 3.89m | MAE 2.07m | p1=0.32, p2=0.35\n",
      "\n",
      "--- Validation at iteration 420 ---\n",
      "  > Val MSE: 13.7396 | Val ANEES: 5.9455 | Hybrid Score: 33.1951\n",
      "------------------------------------------------------------\n",
      "Iter 430 (Warmup): Loss 8.53 | MSE 8.53 | NLL 1.65 | Sigma 3.48m | MAE 1.97m | p1=0.32, p2=0.34\n",
      "\n",
      "--- Validation at iteration 430 ---\n",
      "  > Val MSE: 14.7799 | Val ANEES: 5.8447 | Hybrid Score: 33.2273\n",
      "------------------------------------------------------------\n",
      "Iter 440 (Warmup): Loss 10.88 | MSE 10.88 | NLL 1.79 | Sigma 3.65m | MAE 2.10m | p1=0.31, p2=0.34\n",
      "\n",
      "--- Validation at iteration 440 ---\n",
      "  > Val MSE: 14.2089 | Val ANEES: 6.3432 | Hybrid Score: 37.6404\n",
      "------------------------------------------------------------\n",
      "Iter 450 (Warmup): Loss 7.61 | MSE 7.61 | NLL 28.86 | Sigma 3.40m | MAE 1.91m | p1=0.31, p2=0.34\n",
      "\n",
      "--- Validation at iteration 450 ---\n",
      "  > Val MSE: 13.4351 | Val ANEES: 6.1244 | Hybrid Score: 34.6788\n",
      "------------------------------------------------------------\n",
      "Iter 460 (Warmup): Loss 10.29 | MSE 10.29 | NLL 1.69 | Sigma 3.68m | MAE 2.03m | p1=0.31, p2=0.34\n",
      "\n",
      "--- Validation at iteration 460 ---\n",
      "  > Val MSE: 19.3163 | Val ANEES: 6.3674 | Hybrid Score: 42.9899\n",
      "------------------------------------------------------------\n",
      "Iter 470 (Warmup): Loss 8.21 | MSE 8.21 | NLL 15.37 | Sigma 3.43m | MAE 1.97m | p1=0.31, p2=0.34\n",
      "\n",
      "--- Validation at iteration 470 ---\n",
      "  > Val MSE: 18.7708 | Val ANEES: 6.9077 | Hybrid Score: 47.8479\n",
      "------------------------------------------------------------\n",
      "Iter 480 (Warmup): Loss 9.94 | MSE 9.94 | NLL 18.74 | Sigma 3.58m | MAE 2.00m | p1=0.31, p2=0.34\n",
      "\n",
      "--- Validation at iteration 480 ---\n",
      "  > Val MSE: 24.4569 | Val ANEES: 6.3928 | Hybrid Score: 48.3853\n",
      "------------------------------------------------------------\n",
      "Iter 490 (Warmup): Loss 8.93 | MSE 8.93 | NLL 15.31 | Sigma 3.58m | MAE 1.94m | p1=0.30, p2=0.33\n",
      "\n",
      "--- Validation at iteration 490 ---\n",
      "  > Val MSE: 26.7450 | Val ANEES: 6.6024 | Hybrid Score: 52.7686\n",
      "------------------------------------------------------------\n",
      "Iter 500 (Warmup): Loss 8.69 | MSE 8.69 | NLL 1.77 | Sigma 3.50m | MAE 1.93m | p1=0.30, p2=0.33\n",
      "\n",
      "--- Validation at iteration 500 ---\n",
      "  > Val MSE: 18.0434 | Val ANEES: 6.8633 | Hybrid Score: 46.6768\n",
      "------------------------------------------------------------\n",
      "Iter 510 (Warmup): Loss 7.98 | MSE 7.98 | NLL 28.90 | Sigma 3.53m | MAE 1.90m | p1=0.30, p2=0.33\n",
      "\n",
      "--- Validation at iteration 510 ---\n",
      "  > Val MSE: 17.0756 | Val ANEES: 6.7257 | Hybrid Score: 44.3328\n",
      "------------------------------------------------------------\n",
      "Iter 520 (Warmup): Loss 7.30 | MSE 7.30 | NLL 18.31 | Sigma 3.47m | MAE 1.82m | p1=0.30, p2=0.33\n",
      "\n",
      "--- Validation at iteration 520 ---\n",
      "  > Val MSE: 22.6282 | Val ANEES: 6.8625 | Hybrid Score: 51.2529\n",
      "------------------------------------------------------------\n",
      "Iter 530 (Warmup): Loss 8.26 | MSE 8.26 | NLL 15.42 | Sigma 3.37m | MAE 1.90m | p1=0.30, p2=0.33\n",
      "\n",
      "--- Validation at iteration 530 ---\n",
      "  > Val MSE: 24.5917 | Val ANEES: 6.9325 | Hybrid Score: 53.9164\n",
      "------------------------------------------------------------\n",
      "Iter 540 (Warmup): Loss 7.34 | MSE 7.34 | NLL 1.90 | Sigma 3.44m | MAE 1.87m | p1=0.29, p2=0.33\n",
      "\n",
      "--- Validation at iteration 540 ---\n",
      "  > Val MSE: 18.1501 | Val ANEES: 7.2291 | Hybrid Score: 50.4414\n",
      "------------------------------------------------------------\n",
      "Iter 550 (Warmup): Loss 7.98 | MSE 7.98 | NLL 1.91 | Sigma 3.31m | MAE 1.88m | p1=0.29, p2=0.32\n",
      "\n",
      "--- Validation at iteration 550 ---\n",
      "  > Val MSE: 20.7070 | Val ANEES: 7.4065 | Hybrid Score: 54.7724\n",
      "------------------------------------------------------------\n",
      "Iter 560 (Warmup): Loss 8.79 | MSE 8.79 | NLL 1.82 | Sigma 3.44m | MAE 1.97m | p1=0.29, p2=0.32\n",
      "\n",
      "--- Validation at iteration 560 ---\n",
      "  > Val MSE: 12.2222 | Val ANEES: 7.2236 | Hybrid Score: 44.4582\n",
      "------------------------------------------------------------\n",
      "Iter 570 (Warmup): Loss 9.14 | MSE 9.14 | NLL 15.44 | Sigma 3.40m | MAE 2.00m | p1=0.29, p2=0.32\n",
      "\n",
      "--- Validation at iteration 570 ---\n",
      "  > Val MSE: 20.2725 | Val ANEES: 7.8196 | Hybrid Score: 58.4682\n",
      "------------------------------------------------------------\n",
      "Iter 580 (Warmup): Loss 7.89 | MSE 7.89 | NLL 1.70 | Sigma 3.23m | MAE 1.87m | p1=0.29, p2=0.32\n",
      "\n",
      "--- Validation at iteration 580 ---\n",
      "  > Val MSE: 16.4545 | Val ANEES: 7.0680 | Hybrid Score: 47.1345\n",
      "------------------------------------------------------------\n",
      "Iter 590 (Warmup): Loss 7.35 | MSE 7.35 | NLL 1.84 | Sigma 3.35m | MAE 1.88m | p1=0.29, p2=0.32\n",
      "\n",
      "--- Validation at iteration 590 ---\n",
      "  > Val MSE: 13.7467 | Val ANEES: 7.1808 | Hybrid Score: 45.5551\n",
      "------------------------------------------------------------\n",
      "Iter 600 (Warmup): Loss 7.04 | MSE 7.04 | NLL 1.77 | Sigma 3.23m | MAE 1.86m | p1=0.28, p2=0.32\n",
      "\n",
      "--- Validation at iteration 600 ---\n",
      "  > Val MSE: 18.7213 | Val ANEES: 7.5837 | Hybrid Score: 54.5585\n",
      "------------------------------------------------------------\n",
      "Iter 610 (Warmup): Loss 7.26 | MSE 7.26 | NLL 1.77 | Sigma 3.19m | MAE 1.85m | p1=0.28, p2=0.32\n",
      "\n",
      "--- Validation at iteration 610 ---\n",
      "  > Val MSE: 11.8938 | Val ANEES: 7.3580 | Hybrid Score: 45.4735\n",
      "------------------------------------------------------------\n",
      "Iter 620 (Warmup): Loss 7.51 | MSE 7.51 | NLL 15.42 | Sigma 3.31m | MAE 1.88m | p1=0.28, p2=0.31\n",
      "\n",
      "--- Validation at iteration 620 ---\n",
      "  > Val MSE: 11.5326 | Val ANEES: 7.7041 | Hybrid Score: 48.5737\n",
      "------------------------------------------------------------\n",
      "Iter 630 (Warmup): Loss 7.88 | MSE 7.88 | NLL 1.71 | Sigma 3.41m | MAE 1.87m | p1=0.28, p2=0.31\n",
      "\n",
      "--- Validation at iteration 630 ---\n",
      "  > Val MSE: 13.0988 | Val ANEES: 7.2295 | Hybrid Score: 45.3943\n",
      "------------------------------------------------------------\n",
      "Iter 640 (Warmup): Loss 8.63 | MSE 8.63 | NLL 1.74 | Sigma 3.18m | MAE 1.87m | p1=0.28, p2=0.31\n",
      "\n",
      "--- Validation at iteration 640 ---\n",
      "  > Val MSE: 12.4967 | Val ANEES: 7.7738 | Hybrid Score: 50.2349\n",
      "------------------------------------------------------------\n",
      "Iter 650 (Warmup): Loss 8.05 | MSE 8.04 | NLL 1.86 | Sigma 3.35m | MAE 1.85m | p1=0.27, p2=0.31\n",
      "\n",
      "--- Validation at iteration 650 ---\n",
      "  > Val MSE: 13.9619 | Val ANEES: 7.7957 | Hybrid Score: 51.9188\n",
      "------------------------------------------------------------\n",
      "Iter 660 (Warmup): Loss 6.27 | MSE 6.27 | NLL 15.31 | Sigma 3.11m | MAE 1.73m | p1=0.27, p2=0.31\n",
      "\n",
      "--- Validation at iteration 660 ---\n",
      "  > Val MSE: 12.7274 | Val ANEES: 8.0395 | Hybrid Score: 53.1227\n",
      "------------------------------------------------------------\n",
      "Iter 670 (Warmup): Loss 6.59 | MSE 6.58 | NLL 42.46 | Sigma 3.10m | MAE 1.75m | p1=0.27, p2=0.31\n",
      "\n",
      "--- Validation at iteration 670 ---\n",
      "  > Val MSE: 14.3175 | Val ANEES: 8.3140 | Hybrid Score: 57.4578\n",
      "------------------------------------------------------------\n",
      "Iter 680 (Warmup): Loss 9.78 | MSE 9.77 | NLL 1.93 | Sigma 3.41m | MAE 1.91m | p1=0.27, p2=0.30\n",
      "\n",
      "--- Validation at iteration 680 ---\n",
      "  > Val MSE: 18.2479 | Val ANEES: 7.4878 | Hybrid Score: 53.1262\n",
      "------------------------------------------------------------\n",
      "Iter 690 (Warmup): Loss 7.08 | MSE 7.08 | NLL 42.59 | Sigma 3.12m | MAE 1.80m | p1=0.27, p2=0.30\n",
      "\n",
      "--- Validation at iteration 690 ---\n",
      "  > Val MSE: 9.7276 | Val ANEES: 7.8678 | Hybrid Score: 48.4054\n",
      "------------------------------------------------------------\n",
      "Iter 700 (Warmup): Loss 6.22 | MSE 6.22 | NLL 15.50 | Sigma 3.06m | MAE 1.74m | p1=0.27, p2=0.30\n",
      "\n",
      "--- Validation at iteration 700 ---\n",
      "  > Val MSE: 12.4643 | Val ANEES: 8.4821 | Hybrid Score: 57.2852\n",
      "------------------------------------------------------------\n",
      "Iter 710 (Warmup): Loss 5.76 | MSE 5.76 | NLL 1.82 | Sigma 3.04m | MAE 1.71m | p1=0.26, p2=0.30\n",
      "\n",
      "--- Validation at iteration 710 ---\n",
      "  > Val MSE: 9.3153 | Val ANEES: 7.9309 | Hybrid Score: 48.6244\n",
      "------------------------------------------------------------\n",
      "Iter 720 (Warmup): Loss 6.38 | MSE 6.38 | NLL 1.84 | Sigma 3.00m | MAE 1.76m | p1=0.26, p2=0.30\n",
      "\n",
      "--- Validation at iteration 720 ---\n",
      "  > Val MSE: 10.3518 | Val ANEES: 8.3689 | Hybrid Score: 54.0411\n",
      "------------------------------------------------------------\n",
      "Iter 730 (Warmup): Loss 6.00 | MSE 5.99 | NLL 28.98 | Sigma 2.96m | MAE 1.73m | p1=0.26, p2=0.30\n",
      "\n",
      "--- Validation at iteration 730 ---\n",
      "  > Val MSE: 12.8260 | Val ANEES: 8.3645 | Hybrid Score: 56.4711\n",
      "------------------------------------------------------------\n",
      "Iter 740 (Warmup): Loss 6.81 | MSE 6.81 | NLL 29.01 | Sigma 3.13m | MAE 1.78m | p1=0.26, p2=0.29\n",
      "\n",
      "--- Validation at iteration 740 ---\n",
      "  > Val MSE: 15.3794 | Val ANEES: 9.0274 | Hybrid Score: 65.6535\n",
      "------------------------------------------------------------\n",
      "Iter 750 (Warmup): Loss 6.19 | MSE 6.19 | NLL 1.76 | Sigma 3.07m | MAE 1.72m | p1=0.26, p2=0.29\n",
      "\n",
      "--- Validation at iteration 750 ---\n",
      "  > Val MSE: 8.5074 | Val ANEES: 7.9013 | Hybrid Score: 47.5201\n",
      "------------------------------------------------------------\n",
      "Iter 760 (Warmup): Loss 6.45 | MSE 6.45 | NLL 18.58 | Sigma 3.02m | MAE 1.75m | p1=0.25, p2=0.29\n",
      "\n",
      "--- Validation at iteration 760 ---\n",
      "  > Val MSE: 10.2751 | Val ANEES: 8.5840 | Hybrid Score: 56.1152\n",
      "------------------------------------------------------------\n",
      "Iter 770 (Warmup): Loss 5.91 | MSE 5.91 | NLL 28.98 | Sigma 3.04m | MAE 1.68m | p1=0.25, p2=0.29\n",
      "\n",
      "--- Validation at iteration 770 ---\n",
      "  > Val MSE: 12.4292 | Val ANEES: 8.8057 | Hybrid Score: 60.4857\n",
      "------------------------------------------------------------\n",
      "Iter 780 (Warmup): Loss 6.52 | MSE 6.52 | NLL 15.38 | Sigma 2.98m | MAE 1.72m | p1=0.25, p2=0.29\n",
      "\n",
      "--- Validation at iteration 780 ---\n",
      "  > Val MSE: 14.1040 | Val ANEES: 8.7794 | Hybrid Score: 61.8980\n",
      "------------------------------------------------------------\n",
      "Iter 790 (Warmup): Loss 6.39 | MSE 6.38 | NLL 29.07 | Sigma 2.93m | MAE 1.72m | p1=0.25, p2=0.29\n",
      "\n",
      "--- Validation at iteration 790 ---\n",
      "  > Val MSE: 9.3731 | Val ANEES: 8.4369 | Hybrid Score: 53.7421\n",
      "------------------------------------------------------------\n",
      "Iter 800 (Warmup): Loss 6.86 | MSE 6.86 | NLL 35.24 | Sigma 3.12m | MAE 1.74m | p1=0.25, p2=0.28\n",
      "\n",
      "--- Validation at iteration 800 ---\n",
      "  > Val MSE: 11.2977 | Val ANEES: 8.6974 | Hybrid Score: 58.2712\n",
      "------------------------------------------------------------\n",
      "Iter 810 (Warmup): Loss 6.85 | MSE 6.85 | NLL 1.91 | Sigma 2.98m | MAE 1.78m | p1=0.25, p2=0.28\n",
      "\n",
      "--- Validation at iteration 810 ---\n",
      "  > Val MSE: 11.4030 | Val ANEES: 10.1601 | Hybrid Score: 73.0045\n",
      "------------------------------------------------------------\n",
      "Iter 820 (Warmup): Loss 5.33 | MSE 5.33 | NLL 29.31 | Sigma 2.74m | MAE 1.62m | p1=0.25, p2=0.28\n",
      "\n",
      "--- Validation at iteration 820 ---\n",
      "  > Val MSE: 8.7156 | Val ANEES: 9.0591 | Hybrid Score: 59.3065\n",
      "------------------------------------------------------------\n",
      "Iter 830 (Warmup): Loss 5.54 | MSE 5.54 | NLL 1.86 | Sigma 2.87m | MAE 1.64m | p1=0.24, p2=0.28\n",
      "\n",
      "--- Validation at iteration 830 ---\n",
      "  > Val MSE: 8.8981 | Val ANEES: 9.3957 | Hybrid Score: 62.8546\n",
      "------------------------------------------------------------\n",
      "Iter 840 (Warmup): Loss 5.42 | MSE 5.42 | NLL 2.12 | Sigma 2.87m | MAE 1.61m | p1=0.24, p2=0.28\n",
      "\n",
      "--- Validation at iteration 840 ---\n",
      "  > Val MSE: 10.2447 | Val ANEES: 9.4693 | Hybrid Score: 64.9375\n",
      "------------------------------------------------------------\n",
      "Iter 850 (Warmup): Loss 6.58 | MSE 6.58 | NLL 15.78 | Sigma 2.93m | MAE 1.75m | p1=0.24, p2=0.28\n",
      "\n",
      "--- Validation at iteration 850 ---\n",
      "  > Val MSE: 9.9537 | Val ANEES: 10.0308 | Hybrid Score: 70.2617\n",
      "------------------------------------------------------------\n",
      "Iter 860 (Warmup): Loss 6.55 | MSE 6.55 | NLL 2.13 | Sigma 2.81m | MAE 1.74m | p1=0.24, p2=0.28\n",
      "\n",
      "--- Validation at iteration 860 ---\n",
      "  > Val MSE: 10.2751 | Val ANEES: 9.6450 | Hybrid Score: 66.7250\n",
      "------------------------------------------------------------\n",
      "Iter 870 (Warmup): Loss 5.44 | MSE 5.44 | NLL 15.63 | Sigma 2.67m | MAE 1.64m | p1=0.24, p2=0.27\n",
      "\n",
      "--- Validation at iteration 870 ---\n",
      "  > Val MSE: 10.9239 | Val ANEES: 10.2064 | Hybrid Score: 72.9882\n",
      "------------------------------------------------------------\n",
      "Iter 880 (Warmup): Loss 6.43 | MSE 6.43 | NLL 35.51 | Sigma 2.85m | MAE 1.71m | p1=0.24, p2=0.27\n",
      "\n",
      "--- Validation at iteration 880 ---\n",
      "  > Val MSE: 8.9931 | Val ANEES: 10.3668 | Hybrid Score: 72.6609\n",
      "------------------------------------------------------------\n",
      "Iter 890 (Warmup): Loss 6.24 | MSE 6.24 | NLL 1.90 | Sigma 2.71m | MAE 1.71m | p1=0.23, p2=0.27\n",
      "\n",
      "--- Validation at iteration 890 ---\n",
      "  > Val MSE: 10.1285 | Val ANEES: 9.9253 | Hybrid Score: 69.3818\n",
      "------------------------------------------------------------\n",
      "Iter 900 (Warmup): Loss 5.44 | MSE 5.44 | NLL 15.50 | Sigma 2.84m | MAE 1.63m | p1=0.23, p2=0.27\n",
      "\n",
      "--- Validation at iteration 900 ---\n",
      "  > Val MSE: 24.4290 | Val ANEES: 9.4202 | Hybrid Score: 78.6310\n",
      "------------------------------------------------------------\n",
      "Iter 910 (Warmup): Loss 5.98 | MSE 5.98 | NLL 1.92 | Sigma 2.78m | MAE 1.68m | p1=0.23, p2=0.27\n",
      "\n",
      "--- Validation at iteration 910 ---\n",
      "  > Val MSE: 12.1008 | Val ANEES: 9.9161 | Hybrid Score: 71.2615\n",
      "------------------------------------------------------------\n",
      "Iter 920 (Warmup): Loss 6.28 | MSE 6.27 | NLL 35.59 | Sigma 2.84m | MAE 1.69m | p1=0.23, p2=0.27\n",
      "\n",
      "--- Validation at iteration 920 ---\n",
      "  > Val MSE: 11.6425 | Val ANEES: 9.6158 | Hybrid Score: 67.8002\n",
      "------------------------------------------------------------\n",
      "Iter 930 (Warmup): Loss 5.39 | MSE 5.39 | NLL 1.98 | Sigma 2.68m | MAE 1.63m | p1=0.23, p2=0.27\n",
      "\n",
      "--- Validation at iteration 930 ---\n",
      "  > Val MSE: 12.5382 | Val ANEES: 9.9407 | Hybrid Score: 71.9452\n",
      "------------------------------------------------------------\n",
      "Iter 940 (Warmup): Loss 5.57 | MSE 5.57 | NLL 15.59 | Sigma 2.74m | MAE 1.64m | p1=0.23, p2=0.26\n",
      "\n",
      "--- Validation at iteration 940 ---\n",
      "  > Val MSE: 11.9041 | Val ANEES: 10.3785 | Hybrid Score: 75.6888\n",
      "------------------------------------------------------------\n",
      "Iter 950 (Warmup): Loss 5.22 | MSE 5.22 | NLL 43.00 | Sigma 2.72m | MAE 1.61m | p1=0.23, p2=0.26\n",
      "\n",
      "--- Validation at iteration 950 ---\n",
      "  > Val MSE: 8.3318 | Val ANEES: 10.8094 | Hybrid Score: 76.4257\n",
      "------------------------------------------------------------\n",
      "Iter 960 (Warmup): Loss 5.62 | MSE 5.61 | NLL 51.92 | Sigma 2.78m | MAE 1.62m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 960 ---\n",
      "  > Val MSE: 9.0821 | Val ANEES: 10.4891 | Hybrid Score: 73.9736\n",
      "------------------------------------------------------------\n",
      "Iter 970 (Warmup): Loss 5.28 | MSE 5.28 | NLL 15.70 | Sigma 2.59m | MAE 1.61m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 970 ---\n",
      "  > Val MSE: 8.4726 | Val ANEES: 10.8958 | Hybrid Score: 77.4308\n",
      "------------------------------------------------------------\n",
      "Iter 980 (Warmup): Loss 5.51 | MSE 5.51 | NLL 15.40 | Sigma 2.79m | MAE 1.60m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 980 ---\n",
      "  > Val MSE: 10.3839 | Val ANEES: 10.5396 | Hybrid Score: 75.7801\n",
      "------------------------------------------------------------\n",
      "Iter 990 (Warmup): Loss 5.55 | MSE 5.54 | NLL 57.21 | Sigma 2.76m | MAE 1.65m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 990 ---\n",
      "  > Val MSE: 10.1644 | Val ANEES: 10.9850 | Hybrid Score: 80.0146\n",
      "------------------------------------------------------------\n",
      "Iter 1000 (Warmup): Loss 5.58 | MSE 5.58 | NLL 35.50 | Sigma 2.79m | MAE 1.65m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1000 ---\n",
      "  > Val MSE: 20.4029 | Val ANEES: 11.3431 | Hybrid Score: 93.8336\n",
      "------------------------------------------------------------\n",
      "Iter 1010 (Optim): Loss 1.71 | MSE 10.12 | NLL 1.71 | Sigma 3.12m | MAE 2.16m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1010 ---\n",
      "  > Val MSE: 12.1454 | Val ANEES: 6.6115 | Hybrid Score: 38.2603\n",
      "------------------------------------------------------------\n",
      "Iter 1020 (Optim): Loss 1.51 | MSE 11.01 | NLL 1.51 | Sigma 3.80m | MAE 2.13m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1020 ---\n",
      "  > Val MSE: 18.1096 | Val ANEES: 3.7974 | Hybrid Score: 20.1360\n",
      "  >>> ‚≠ê New Best Model! (Score: 23.1661 -> 20.1360)\n",
      "------------------------------------------------------------\n",
      "Iter 1030 (Optim): Loss 1.45 | MSE 11.82 | NLL 1.45 | Sigma 3.94m | MAE 2.18m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1030 ---\n",
      "  > Val MSE: 23.6948 | Val ANEES: 3.4072 | Hybrid Score: 29.6232\n",
      "------------------------------------------------------------\n",
      "Iter 1040 (Optim): Loss 1.40 | MSE 11.57 | NLL 1.40 | Sigma 3.67m | MAE 2.04m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1040 ---\n",
      "  > Val MSE: 21.7190 | Val ANEES: 3.2383 | Hybrid Score: 29.3359\n",
      "------------------------------------------------------------\n",
      "Iter 1050 (Optim): Loss 28.51 | MSE 14.08 | NLL 28.51 | Sigma 3.89m | MAE 2.18m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1050 ---\n",
      "  > Val MSE: 24.2670 | Val ANEES: 3.3466 | Hybrid Score: 30.8010\n",
      "------------------------------------------------------------\n",
      "Iter 1060 (Optim): Loss 1.34 | MSE 8.36 | NLL 1.34 | Sigma 3.56m | MAE 1.90m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1060 ---\n",
      "  > Val MSE: 19.4194 | Val ANEES: 3.3301 | Hybrid Score: 26.1188\n",
      "------------------------------------------------------------\n",
      "Iter 1070 (Optim): Loss 14.90 | MSE 9.60 | NLL 14.90 | Sigma 3.60m | MAE 2.02m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1070 ---\n",
      "  > Val MSE: 18.3187 | Val ANEES: 3.4707 | Hybrid Score: 23.6121\n",
      "------------------------------------------------------------\n",
      "Iter 1080 (Optim): Loss 1.44 | MSE 15.31 | NLL 1.43 | Sigma 3.70m | MAE 2.23m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1080 ---\n",
      "  > Val MSE: 15.8839 | Val ANEES: 3.8812 | Hybrid Score: 17.0719\n",
      "  >>> ‚≠ê New Best Model! (Score: 20.1360 -> 17.0719)\n",
      "------------------------------------------------------------\n",
      "Iter 1090 (Optim): Loss 14.95 | MSE 11.52 | NLL 14.95 | Sigma 3.53m | MAE 2.09m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1090 ---\n",
      "  > Val MSE: 15.0021 | Val ANEES: 3.4874 | Hybrid Score: 20.1281\n",
      "------------------------------------------------------------\n",
      "Iter 1100 (Optim): Loss 1.36 | MSE 8.19 | NLL 1.35 | Sigma 3.53m | MAE 1.91m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1100 ---\n",
      "  > Val MSE: 19.2860 | Val ANEES: 3.4170 | Hybrid Score: 25.1161\n",
      "------------------------------------------------------------\n",
      "Iter 1110 (Optim): Loss 1.36 | MSE 10.33 | NLL 1.36 | Sigma 3.67m | MAE 2.06m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1110 ---\n",
      "  > Val MSE: 12.6872 | Val ANEES: 3.4331 | Hybrid Score: 18.3563\n",
      "------------------------------------------------------------\n",
      "Iter 1120 (Optim): Loss 1.35 | MSE 9.80 | NLL 1.35 | Sigma 3.65m | MAE 1.99m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1120 ---\n",
      "  > Val MSE: 12.1498 | Val ANEES: 3.4736 | Hybrid Score: 17.4141\n",
      "------------------------------------------------------------\n",
      "Iter 1130 (Optim): Loss 14.92 | MSE 16.01 | NLL 14.91 | Sigma 3.71m | MAE 2.19m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1130 ---\n",
      "  > Val MSE: 18.8222 | Val ANEES: 3.4097 | Hybrid Score: 24.7249\n",
      "------------------------------------------------------------\n",
      "Iter 1140 (Optim): Loss 42.04 | MSE 15.21 | NLL 42.04 | Sigma 3.56m | MAE 2.25m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1140 ---\n",
      "  > Val MSE: 22.6381 | Val ANEES: 3.8217 | Hybrid Score: 24.4208\n",
      "------------------------------------------------------------\n",
      "Iter 1150 (Optim): Loss 14.96 | MSE 11.02 | NLL 14.96 | Sigma 3.61m | MAE 2.18m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1150 ---\n",
      "  > Val MSE: 19.7288 | Val ANEES: 3.6980 | Hybrid Score: 22.7491\n",
      "------------------------------------------------------------\n",
      "Iter 1160 (Optim): Loss 1.40 | MSE 12.75 | NLL 1.40 | Sigma 3.66m | MAE 2.20m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1160 ---\n",
      "  > Val MSE: 22.9425 | Val ANEES: 3.7634 | Hybrid Score: 25.3090\n",
      "------------------------------------------------------------\n",
      "Iter 1170 (Optim): Loss 14.99 | MSE 17.34 | NLL 14.99 | Sigma 3.80m | MAE 2.33m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1170 ---\n",
      "  > Val MSE: 15.9436 | Val ANEES: 3.9732 | Hybrid Score: 16.2116\n",
      "  >>> ‚≠ê New Best Model! (Score: 17.0719 -> 16.2116)\n",
      "------------------------------------------------------------\n",
      "Iter 1180 (Optim): Loss 1.38 | MSE 14.02 | NLL 1.38 | Sigma 3.62m | MAE 2.16m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1180 ---\n",
      "  > Val MSE: 23.2693 | Val ANEES: 3.6085 | Hybrid Score: 27.1844\n",
      "------------------------------------------------------------\n",
      "Iter 1190 (Optim): Loss 1.37 | MSE 14.73 | NLL 1.37 | Sigma 3.65m | MAE 2.19m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1190 ---\n",
      "  > Val MSE: 18.5287 | Val ANEES: 3.5359 | Hybrid Score: 23.1694\n",
      "------------------------------------------------------------\n",
      "Iter 1200 (Optim): Loss 18.11 | MSE 25.00 | NLL 18.11 | Sigma 3.91m | MAE 2.41m | p1=0.22, p2=0.26\n",
      "\n",
      "--- Validation at iteration 1200 ---\n",
      "  > Val MSE: 14.5180 | Val ANEES: 3.4654 | Hybrid Score: 19.8642\n",
      "------------------------------------------------------------\n",
      "Iter 1210 (Optim): Loss 29.31 | MSE 10.84 | NLL 29.31 | Sigma 3.52m | MAE 2.14m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1210 ---\n",
      "  > Val MSE: 26.0312 | Val ANEES: 3.7827 | Hybrid Score: 28.2042\n",
      "------------------------------------------------------------\n",
      "Iter 1220 (Optim): Loss 14.93 | MSE 17.11 | NLL 14.93 | Sigma 3.52m | MAE 2.15m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1220 ---\n",
      "  > Val MSE: 15.1702 | Val ANEES: 3.5650 | Hybrid Score: 19.5200\n",
      "------------------------------------------------------------\n",
      "Iter 1230 (Optim): Loss 42.00 | MSE 13.40 | NLL 42.00 | Sigma 3.51m | MAE 2.13m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1230 ---\n",
      "  > Val MSE: 14.5798 | Val ANEES: 3.5418 | Hybrid Score: 19.1615\n",
      "------------------------------------------------------------\n",
      "Iter 1240 (Optim): Loss 1.38 | MSE 9.74 | NLL 1.38 | Sigma 3.44m | MAE 2.06m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1240 ---\n",
      "  > Val MSE: 23.7664 | Val ANEES: 3.4595 | Hybrid Score: 29.1715\n",
      "------------------------------------------------------------\n",
      "Iter 1250 (Optim): Loss 28.44 | MSE 9.16 | NLL 28.44 | Sigma 3.49m | MAE 2.00m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1250 ---\n",
      "  > Val MSE: 16.6668 | Val ANEES: 3.3733 | Hybrid Score: 22.9334\n",
      "------------------------------------------------------------\n",
      "Iter 1260 (Optim): Loss 42.86 | MSE 10.77 | NLL 42.86 | Sigma 3.57m | MAE 2.11m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1260 ---\n",
      "  > Val MSE: 22.3428 | Val ANEES: 3.5375 | Hybrid Score: 26.9682\n",
      "------------------------------------------------------------\n",
      "Iter 1270 (Optim): Loss 15.79 | MSE 9.88 | NLL 15.79 | Sigma 3.44m | MAE 2.07m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1270 ---\n",
      "  > Val MSE: 16.8580 | Val ANEES: 3.5790 | Hybrid Score: 21.0682\n",
      "------------------------------------------------------------\n",
      "Iter 1280 (Optim): Loss 18.10 | MSE 15.48 | NLL 18.10 | Sigma 3.71m | MAE 2.32m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1280 ---\n",
      "  > Val MSE: 19.6335 | Val ANEES: 4.1654 | Hybrid Score: 21.2876\n",
      "------------------------------------------------------------\n",
      "Iter 1290 (Optim): Loss 14.91 | MSE 10.20 | NLL 14.91 | Sigma 3.48m | MAE 2.10m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1290 ---\n",
      "  > Val MSE: 14.7100 | Val ANEES: 3.5983 | Hybrid Score: 18.7271\n",
      "------------------------------------------------------------\n",
      "Iter 1300 (Optim): Loss 28.45 | MSE 15.34 | NLL 28.45 | Sigma 3.55m | MAE 2.15m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1300 ---\n",
      "  > Val MSE: 21.7715 | Val ANEES: 3.5670 | Hybrid Score: 26.1019\n",
      "------------------------------------------------------------\n",
      "Iter 1310 (Optim): Loss 42.06 | MSE 15.83 | NLL 42.06 | Sigma 3.99m | MAE 2.28m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1310 ---\n",
      "  > Val MSE: 20.5958 | Val ANEES: 3.5148 | Hybrid Score: 25.4481\n",
      "------------------------------------------------------------\n",
      "Iter 1320 (Optim): Loss 18.05 | MSE 10.55 | NLL 18.05 | Sigma 3.48m | MAE 2.12m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1320 ---\n",
      "  > Val MSE: 19.4762 | Val ANEES: 3.7515 | Hybrid Score: 21.9616\n",
      "------------------------------------------------------------\n",
      "Iter 1330 (Optim): Loss 55.58 | MSE 10.58 | NLL 55.58 | Sigma 3.45m | MAE 2.11m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1330 ---\n",
      "  > Val MSE: 26.3850 | Val ANEES: 3.8970 | Hybrid Score: 27.4150\n",
      "------------------------------------------------------------\n",
      "Iter 1340 (Optim): Loss 42.04 | MSE 13.36 | NLL 42.04 | Sigma 3.73m | MAE 2.25m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1340 ---\n",
      "  > Val MSE: 21.8456 | Val ANEES: 3.7587 | Hybrid Score: 24.2587\n",
      "------------------------------------------------------------\n",
      "Iter 1350 (Optim): Loss 14.90 | MSE 9.83 | NLL 14.90 | Sigma 3.43m | MAE 2.08m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1350 ---\n",
      "  > Val MSE: 22.9321 | Val ANEES: 3.9058 | Hybrid Score: 23.8738\n",
      "------------------------------------------------------------\n",
      "Iter 1360 (Optim): Loss 34.82 | MSE 31.82 | NLL 34.82 | Sigma 4.01m | MAE 2.69m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1360 ---\n",
      "  > Val MSE: 36.1874 | Val ANEES: 4.1591 | Hybrid Score: 37.7781\n",
      "------------------------------------------------------------\n",
      "Iter 1370 (Optim): Loss 28.61 | MSE 20.85 | NLL 28.61 | Sigma 3.86m | MAE 2.69m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1370 ---\n",
      "  > Val MSE: 51.7368 | Val ANEES: 5.9132 | Hybrid Score: 70.8686\n",
      "------------------------------------------------------------\n",
      "Iter 1380 (Optim): Loss 28.60 | MSE 31.63 | NLL 28.59 | Sigma 3.98m | MAE 2.84m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1380 ---\n",
      "  > Val MSE: 49.0561 | Val ANEES: 4.4583 | Hybrid Score: 53.6389\n",
      "------------------------------------------------------------\n",
      "Iter 1390 (Optim): Loss 14.97 | MSE 13.89 | NLL 14.97 | Sigma 3.89m | MAE 2.34m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1390 ---\n",
      "  > Val MSE: 22.6684 | Val ANEES: 3.5430 | Hybrid Score: 27.2380\n",
      "------------------------------------------------------------\n",
      "Iter 1400 (Optim): Loss 18.09 | MSE 11.90 | NLL 18.09 | Sigma 4.05m | MAE 2.24m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1400 ---\n",
      "  > Val MSE: 17.3662 | Val ANEES: 3.2306 | Hybrid Score: 25.0598\n",
      "------------------------------------------------------------\n",
      "Iter 1410 (Optim): Loss 28.46 | MSE 10.63 | NLL 28.46 | Sigma 3.43m | MAE 2.15m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1410 ---\n",
      "  > Val MSE: 17.3304 | Val ANEES: 3.6646 | Hybrid Score: 20.6845\n",
      "------------------------------------------------------------\n",
      "Iter 1420 (Optim): Loss 55.56 | MSE 16.05 | NLL 55.56 | Sigma 3.78m | MAE 2.23m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1420 ---\n",
      "  > Val MSE: 21.6660 | Val ANEES: 3.6462 | Hybrid Score: 25.2036\n",
      "------------------------------------------------------------\n",
      "Iter 1430 (Optim): Loss 14.93 | MSE 16.62 | NLL 14.93 | Sigma 3.83m | MAE 2.34m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1430 ---\n",
      "  > Val MSE: 29.2631 | Val ANEES: 3.6821 | Hybrid Score: 32.4419\n",
      "------------------------------------------------------------\n",
      "Iter 1440 (Optim): Loss 1.37 | MSE 13.51 | NLL 1.37 | Sigma 3.78m | MAE 2.22m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1440 ---\n",
      "  > Val MSE: 19.8062 | Val ANEES: 3.6220 | Hybrid Score: 23.5859\n",
      "------------------------------------------------------------\n",
      "Iter 1450 (Optim): Loss 14.96 | MSE 14.86 | NLL 14.96 | Sigma 3.96m | MAE 2.27m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1450 ---\n",
      "  > Val MSE: 18.2798 | Val ANEES: 4.0374 | Hybrid Score: 18.6537\n",
      "------------------------------------------------------------\n",
      "Iter 1460 (Optim): Loss 43.79 | MSE 17.36 | NLL 43.79 | Sigma 4.04m | MAE 2.44m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1460 ---\n",
      "  > Val MSE: 18.9018 | Val ANEES: 4.0678 | Hybrid Score: 19.5800\n",
      "------------------------------------------------------------\n",
      "Iter 1470 (Optim): Loss 69.14 | MSE 13.75 | NLL 69.14 | Sigma 3.49m | MAE 2.15m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1470 ---\n",
      "  > Val MSE: 23.0732 | Val ANEES: 3.9810 | Hybrid Score: 23.2628\n",
      "------------------------------------------------------------\n",
      "Iter 1480 (Optim): Loss 18.04 | MSE 9.00 | NLL 18.04 | Sigma 3.34m | MAE 2.02m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1480 ---\n",
      "  > Val MSE: 21.5505 | Val ANEES: 3.6556 | Hybrid Score: 24.9947\n",
      "------------------------------------------------------------\n",
      "Iter 1490 (Optim): Loss 28.48 | MSE 16.50 | NLL 28.47 | Sigma 3.84m | MAE 2.27m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1490 ---\n",
      "  > Val MSE: 30.9101 | Val ANEES: 3.6386 | Hybrid Score: 34.5237\n",
      "------------------------------------------------------------\n",
      "Iter 1500 (Optim): Loss 42.83 | MSE 9.25 | NLL 42.83 | Sigma 3.39m | MAE 2.03m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1500 ---\n",
      "  > Val MSE: 23.9894 | Val ANEES: 3.5067 | Hybrid Score: 28.9227\n",
      "------------------------------------------------------------\n",
      "Iter 1510 (Optim): Loss 14.92 | MSE 12.33 | NLL 14.92 | Sigma 3.72m | MAE 2.23m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1510 ---\n",
      "  > Val MSE: 16.5067 | Val ANEES: 3.4855 | Hybrid Score: 21.6513\n",
      "------------------------------------------------------------\n",
      "Iter 1520 (Optim): Loss 18.09 | MSE 11.96 | NLL 18.09 | Sigma 3.76m | MAE 2.27m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1520 ---\n",
      "  > Val MSE: 18.4702 | Val ANEES: 3.6380 | Hybrid Score: 22.0906\n",
      "------------------------------------------------------------\n",
      "Iter 1530 (Optim): Loss 14.98 | MSE 26.51 | NLL 14.97 | Sigma 4.02m | MAE 2.41m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1530 ---\n",
      "  > Val MSE: 17.1730 | Val ANEES: 3.4513 | Hybrid Score: 22.6597\n",
      "------------------------------------------------------------\n",
      "Iter 1540 (Optim): Loss 1.37 | MSE 12.70 | NLL 1.36 | Sigma 3.60m | MAE 2.17m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1540 ---\n",
      "  > Val MSE: 16.6276 | Val ANEES: 3.5363 | Hybrid Score: 21.2650\n",
      "------------------------------------------------------------\n",
      "Iter 1550 (Optim): Loss 69.08 | MSE 13.33 | NLL 69.08 | Sigma 3.51m | MAE 2.17m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1550 ---\n",
      "  > Val MSE: 27.2455 | Val ANEES: 3.8391 | Hybrid Score: 28.8547\n",
      "------------------------------------------------------------\n",
      "Iter 1560 (Optim): Loss 68.08 | MSE 11.18 | NLL 68.08 | Sigma 3.53m | MAE 2.17m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1560 ---\n",
      "  > Val MSE: 24.0434 | Val ANEES: 4.0005 | Hybrid Score: 24.0487\n",
      "------------------------------------------------------------\n",
      "Iter 1570 (Optim): Loss 69.97 | MSE 22.59 | NLL 69.97 | Sigma 3.73m | MAE 2.32m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1570 ---\n",
      "  > Val MSE: 27.2811 | Val ANEES: 3.7897 | Hybrid Score: 29.3844\n",
      "------------------------------------------------------------\n",
      "Iter 1580 (Optim): Loss 28.48 | MSE 11.35 | NLL 28.48 | Sigma 3.62m | MAE 2.16m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1580 ---\n",
      "  > Val MSE: 31.7852 | Val ANEES: 3.5190 | Hybrid Score: 36.5948\n",
      "------------------------------------------------------------\n",
      "Iter 1590 (Optim): Loss 28.49 | MSE 33.37 | NLL 28.49 | Sigma 3.78m | MAE 2.39m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1590 ---\n",
      "  > Val MSE: 19.7846 | Val ANEES: 3.7881 | Hybrid Score: 21.9031\n",
      "------------------------------------------------------------\n",
      "Iter 1600 (Optim): Loss 51.44 | MSE 13.49 | NLL 51.44 | Sigma 3.51m | MAE 2.24m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1600 ---\n",
      "  > Val MSE: 27.3338 | Val ANEES: 3.7533 | Hybrid Score: 29.8009\n",
      "------------------------------------------------------------\n",
      "Iter 1610 (Optim): Loss 29.38 | MSE 16.24 | NLL 29.38 | Sigma 3.95m | MAE 2.40m | p1=0.22, p2=0.25\n",
      "\n",
      "--- Validation at iteration 1610 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   -> Using Standard Hybrid Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_BayesianKalmanNet_TwoPhase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_knet2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_phase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader_phase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_train_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip_grad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mJ_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmse_warmup_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmse_warmup_iters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_parameter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcalibration_parameter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbkn_curriculum_phase\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_len\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(state_knet2\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[0;32mIn[12], line 183\u001b[0m, in \u001b[0;36mtrain_BayesianKalmanNet_TwoPhase\u001b[0;34m(model, train_loader, val_loader, device, total_train_iter, learning_rate, clip_grad, J_samples, validation_period, logging_period, mse_warmup_iters, calibration_parameter, weight_decay_)\u001b[0m\n\u001b[1;32m    181\u001b[0m v_x_hats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, v_seq):\n\u001b[0;32m--> 183\u001b[0m     est, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_meas_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     v_x_hats\u001b[38;5;241m.\u001b[39mappend(est)\n\u001b[1;32m    185\u001b[0m val_ensemble_trajs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack(v_x_hats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/state_NN_models/TAN/StateBayesianKalmanNet.py:83\u001b[0m, in \u001b[0;36mStateBayesianKalmanNetTAN.step\u001b[0;34m(self, y_t)\u001b[0m\n\u001b[1;32m     80\u001b[0m norm_diff_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_modulus(diff_obs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 3. PR≈ÆCHOD S√çT√ç A KOREKCE\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m K_vec, h_new, regs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_state_inno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_innovation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_diff_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_diff_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m K \u001b[38;5;241m=\u001b[39m K_vec\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_dim)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# K = torch.clamp(K, min=-5.0, max=5.0)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/state_NN_models/TAN/DNN_BayesianKalmanNet.py:51\u001b[0m, in \u001b[0;36mDNN_BayesianKalmanNetTAN.forward\u001b[0;34m(self, state_inno, inovation, diff_state, diff_obs, h_prev)\u001b[0m\n\u001b[1;32m     48\u001b[0m out_gru, h_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(activated_input\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), h_prev)\n\u001b[1;32m     49\u001b[0m out_gru_squeezed \u001b[38;5;241m=\u001b[39m out_gru\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m out_final, reg2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete_dropout2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_gru_squeezed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m total_reg \u001b[38;5;241m=\u001b[39m reg1\u001b[38;5;241m+\u001b[39mreg2\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_final, h_new, total_reg\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/skola/KalmanNet-main/state_NN_models/TAN/DNN_BayesianKalmanNet.py:87\u001b[0m, in \u001b[0;36mConcreteDropout.forward\u001b[0;34m(self, x, layer)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     85\u001b[0m     sum_of_squares \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mpow(param, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 87\u001b[0m weights_regularizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_regularizer \u001b[38;5;241m*\u001b[39m sum_of_squares \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m)\n\u001b[1;32m     89\u001b[0m dropout_regularizer \u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(p)\n\u001b[1;32m     90\u001b[0m dropout_regularizer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m p)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:38\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "# --- 1. DEFINICE CURRICULA (UPRAVENO PRO STABILITU) ---\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Stabilizace (Kr√°tk√© sekvence)\n",
    "    # Zde se model nauƒç√≠ z√°klady dynamiky.\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,\n",
    "        'iters': 2000,\n",
    "        'lr': 1e-3,\n",
    "        'lambda_mse': 1.0,    # Na kr√°tk√© sekvenci staƒç√≠ m√°lo\n",
    "        'clip_grad': 1.0,\n",
    "        'use_tbptt': False,\n",
    "        'mse_warmup_iters': 1000 # Celou dobu jedeme v re≈æimu \"Siln√© MSE + Soft NLL\"\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Prodlou≈æen√≠ na 100 (Kritick√° f√°ze)\n",
    "    # Tady doch√°zelo k explozi variance. Mus√≠me b√Ωt p≈ô√≠snƒõj≈°√≠.\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100,\n",
    "        'iters': 3000,        # P≈ôidali jsme iterace\n",
    "        \n",
    "        'lambda_mse': 5.0,   # <--- ZV√ù≈†ENO! (b√Ωvalo 1.0). Dr≈æ√≠ model u zemƒõ.\n",
    "        \n",
    "        'lr': 1e-5,           # Trochu pomalej≈°√≠ uƒçen√≠\n",
    "        'clip_grad': 0.05,\n",
    "        'use_tbptt': False,\n",
    "        'tbptt_w': 50,\n",
    "        'tbptt_k': 5,\n",
    "        \n",
    "        # D√≠ky nov√© funkci se i ve warmupu uƒç√≠ variance.\n",
    "        # Proto chceme warmup del≈°√≠, aby model z√≠skal jistotu v trajektorii.\n",
    "        'mse_warmup_iters': 1500, \n",
    "        \n",
    "        'calibration_parameter': 10.0\n",
    "    },\n",
    "    \n",
    "    # # F√ÅZE 3: Long-term (300)\n",
    "    # {\n",
    "    #     'phase_id': 3,\n",
    "    #     'seq_len': 300,\n",
    "    #     'iters': 2000,\n",
    "    #     'lr': 1e-6,           # Jemn√© doladƒõn√≠\n",
    "    #     'clip_grad': 0.01,\n",
    "        \n",
    "    #     'lambda_mse': 10.0,   # <--- VYSOK√Å KOTVA. Na 300 kroc√≠ch je MSE obrovsk√©.\n",
    "        \n",
    "    #     'use_tbptt': True,\n",
    "    #     'tbptt_w': 50,\n",
    "    #     'tbptt_k': 5,\n",
    "    #     'mse_warmup_iters': 500, # Kr√°tk√Ω re-warmup na nov√© d√©lce\n",
    "    #     'calibration_parameter': 10.0\n",
    "    # }\n",
    "]\n",
    "\n",
    "# --- 2. INICIALIZACE MODELU ---\n",
    "print(\"=== INICIALIZACE BKN MODELU ===\")\n",
    "state_knet2 = TAN.StateBayesianKalmanNetTAN(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=12,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        init_max_dropout=0.6, \n",
    "        init_min_dropout=0.4    \n",
    ").to(device)\n",
    "\n",
    "# --- 3. CURRICULUM LOOP ---\n",
    "for phase in curriculum_schedule:\n",
    "    phase_id = phase['phase_id']\n",
    "    seq_len = phase['seq_len']\n",
    "\n",
    "    if phase_id not in datasets_cache:\n",
    "        print(f\"‚ö†Ô∏è Skipping Phase {phase_id}: Data not in cache.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üöÄ START PHASE {phase_id}: SeqLen {seq_len} | LR {phase['lr']} | Lambda MSE: {phase['lambda_mse']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_loader_phase = datasets_cache[phase_id][0]\n",
    "    val_loader_phase = datasets_cache[phase_id][1]\n",
    "    \n",
    "    if phase['use_tbptt']:\n",
    "        if not hasattr(state_knet2, 'detach_hidden'):\n",
    "            raise AttributeError(\"Modelu chyb√≠ metoda 'detach_hidden()'\")\n",
    "\n",
    "        result = train_BayesianKalmanNet_TBPTT_Windowed(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=7,\n",
    "            tbptt_w=phase.get('tbptt_w', 10),\n",
    "            tbptt_k=phase.get('tbptt_k', 2),\n",
    "            validation_period=20,\n",
    "            logging_period=10,\n",
    "            \n",
    "            # P≈ôed√°v√°me parametry z curricula\n",
    "            mse_warmup_iters=phase['mse_warmup_iters'],\n",
    "            lambda_mse=phase['lambda_mse'], \n",
    "            weight_decay_=1e-3,\n",
    "            calibration_parameter=phase.get('calibration_parameter', 10.0)\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        print(f\"   -> Using Standard Hybrid Training\")\n",
    "        result = train_BayesianKalmanNet_TwoPhase(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=10,\n",
    "            validation_period=10,\n",
    "            logging_period=10,\n",
    "            mse_warmup_iters=phase['mse_warmup_iters'],\n",
    "            weight_decay_=1e-3,\n",
    "            calibration_parameter=phase.get('calibration_parameter', 10.0)\n",
    "        )\n",
    "    \n",
    "    save_path = f\"bkn_curriculum_phase{phase_id}_len{seq_len}.pth\"\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"‚úÖ F√°ze {phase_id} dokonƒçena. Model ulo≈æen do: {save_path}\")\n",
    "\n",
    "print(\"\\nüéâ Cel√Ω tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'best_mse_and_anees_bknet.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import Filters\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from Filters import TAN\n",
    "\n",
    "# === KONFIGURACE ===\n",
    "TEST_DATA_PATH = './generated_data_synthetic_controlled/test_set/test.pt'\n",
    "PLOT_PER_ITERATION = True  # Vykreslovat graf pro ka≈ædou trajektorii?\n",
    "MAX_TEST_SAMPLES = 20        # Kolik trajektori√≠ z test setu vyhodnotit\n",
    "J_EVALUATION = 100           # Poƒçet Monte Carlo vzork≈Ø pro BKN (Ensemble size)\n",
    "\n",
    "print(f\"=== VYHODNOCEN√ç BKN NA TESTOVAC√ç SADƒö (s ANEES) ===\")\n",
    "print(f\"Naƒç√≠t√°m data z: {TEST_DATA_PATH}\")\n",
    "\n",
    "# 1. Naƒçten√≠ Testovac√≠ sady\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Soubor {TEST_DATA_PATH} neexistuje!\")\n",
    "\n",
    "# P≈ôedpokl√°d√°me, ≈æe 'device' je definov√°no\n",
    "if 'device' not in globals():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_data = torch.load(TEST_DATA_PATH, map_location=device)\n",
    "X_test_all = test_data['x']  # Ground Truth [N, Seq, 4]\n",
    "Y_test_all = test_data['y']  # Measurements [N, Seq, 3]\n",
    "\n",
    "n_samples = min(X_test_all.shape[0], MAX_TEST_SAMPLES)\n",
    "print(f\"Poƒçet testovac√≠ch trajektori√≠: {n_samples}\")\n",
    "print(f\"Ensemble size (BKN): {J_EVALUATION}\")\n",
    "print(f\"D√©lka sekvence: {X_test_all.shape[1]}\")\n",
    "print(\"Modely: BKN vs. UKF vs. PF vs. APF\")\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = []\n",
    "agg_mse = {\"BKN\": [], \"UKF\": [], \"PF\": [], \"APF\": []}\n",
    "agg_pos = {\"BKN\": [], \"UKF\": [], \"PF\": [], \"APF\": []}\n",
    "agg_anees = {\"BKN\": [], \"UKF\": [], \"PF\": [], \"APF\": []} # Nov√Ω list pro ANEES\n",
    "\n",
    "# Ujist√≠me se, ≈æe BKN je v eval m√≥du \n",
    "state_knet2.eval() \n",
    "\n",
    "# --- POMOCN√Å FUNKCE PRO ANEES ---\n",
    "def calculate_anees(gt, est, P):\n",
    "    \"\"\"\n",
    "    Vypoƒç√≠t√° Average Normalized Estimation Error Squared.\n",
    "    gt: Ground Truth [T, Dim] (NumPy)\n",
    "    est: Odhad [T, Dim] (NumPy)\n",
    "    P: Kovarianƒçn√≠ matice [T, Dim, Dim] (NumPy)\n",
    "    \"\"\"\n",
    "    T = min(len(gt), len(est), len(P))\n",
    "    anees_vals = []\n",
    "    \n",
    "    # O≈ô√≠znut√≠ na stejnou d√©lku\n",
    "    gt = gt[:T]\n",
    "    est = est[:T]\n",
    "    P = P[:T]\n",
    "    \n",
    "    for t in range(T):\n",
    "        e_t = gt[t] - est[t] # Chyba v ƒçase t\n",
    "        P_t = P[t]\n",
    "        \n",
    "        try:\n",
    "            # Inverze kovariance\n",
    "            # P≈ôiƒçteme mal√© epsilon na diagon√°lu pro numerickou stabilitu, pokud je singul√°rn√≠\n",
    "            if np.linalg.cond(P_t) > 1e10:\n",
    "                P_t = P_t + np.eye(P_t.shape[0]) * 1e-6\n",
    "                \n",
    "            P_inv = np.linalg.inv(P_t)\n",
    "            \n",
    "            # Mahalanobisova vzd√°lenost: e^T * P^-1 * e\n",
    "            anees_t = e_t.T @ P_inv @ e_t\n",
    "            anees_vals.append(anees_t)\n",
    "        except np.linalg.LinAlgError:\n",
    "            anees_vals.append(np.nan)\n",
    "            \n",
    "    return np.nanmean(anees_vals)\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA ---\n",
    "for i in tqdm(range(n_samples), desc=\"Evaluace\"):\n",
    "    \n",
    "    # A) P≈ô√≠prava dat\n",
    "    x_gt_tensor = X_test_all[i].to(device)\n",
    "    y_obs_tensor = Y_test_all[i].to(device)\n",
    "    \n",
    "    x_gt = x_gt_tensor.cpu().numpy()\n",
    "    seq_len = x_gt.shape[0]\n",
    "    true_init_state = x_gt_tensor[0] \n",
    "    \n",
    "    # --- B) BKN (Ensemble) ---\n",
    "    with torch.no_grad():\n",
    "        init_batch = true_init_state.unsqueeze(0).repeat(J_EVALUATION, 1)\n",
    "        state_knet2.reset(batch_size=J_EVALUATION, initial_state=init_batch)\n",
    "        \n",
    "        bkn_preds = []\n",
    "        y_input_batch = y_obs_tensor.unsqueeze(0).repeat(J_EVALUATION, 1, 1)\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input_batch[:, t, :]\n",
    "            x_est, _ = state_knet2.step(y_t) \n",
    "            bkn_preds.append(x_est)\n",
    "            \n",
    "        if len(bkn_preds) > 0:\n",
    "            bkn_preds_tensor = torch.stack(bkn_preds, dim=1) # [J, Seq-1, 4]\n",
    "            full_bkn_ensemble = torch.cat([init_batch.unsqueeze(1), bkn_preds_tensor], dim=1) # [J, Seq, 4]\n",
    "            \n",
    "            # Mean Estimate\n",
    "            x_est_mean = full_bkn_ensemble.mean(dim=0)\n",
    "            x_est_bkn = x_est_mean.cpu().numpy()\n",
    "            \n",
    "            # --- V√ùPOƒåET KOVARIANCE PRO BKN ---\n",
    "            # P = 1/(J-1) * sum((x_j - x_mean) * (x_j - x_mean)^T)\n",
    "            # Vycentrov√°n√≠\n",
    "            residuals = full_bkn_ensemble - x_est_mean.unsqueeze(0) # [J, Seq, 4]\n",
    "            # Permute pro batch matmul: [Seq, J, 4] a [Seq, 4, J]\n",
    "            residuals = residuals.permute(1, 2, 0) # [Seq, 4, J]\n",
    "            \n",
    "            # Batch matrix multiplication: (Seq, 4, J) @ (Seq, J, 4) -> (Seq, 4, 4)\n",
    "            P_bkn_tensor = torch.bmm(residuals, residuals.transpose(1, 2)) / (J_EVALUATION - 1)\n",
    "            # P≈ôiƒçten√≠ process noise/stabilitu (voliteln√©, BKN variance je epistemick√°)\n",
    "            P_bkn = P_bkn_tensor.cpu().numpy()\n",
    "            \n",
    "        else:\n",
    "            x_est_bkn = x_gt\n",
    "            P_bkn = np.eye(4)[np.newaxis, :, :].repeat(len(x_gt), axis=0)\n",
    "\n",
    "    # --- C) Klasick√© Filtry ---\n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(y_seq=y_obs_tensor, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "    # Z√≠sk√°n√≠ P pro UKF (zkus√≠me r≈Øzn√© kl√≠ƒçe)\n",
    "    P_ukf = ukf_res.get('P_filtered', ukf_res.get('P', None))\n",
    "    if P_ukf is not None: P_ukf = P_ukf.cpu().numpy()\n",
    "\n",
    "    # PF\n",
    "    pf = TAN.ParticleFilterTAN(system_model, num_particles=1000) \n",
    "    pf_res = pf.process_sequence(y_seq=y_obs_tensor, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "    P_pf = pf_res.get('P_filtered', pf_res.get('P', None))\n",
    "    if P_pf is not None: P_pf = P_pf.cpu().numpy()\n",
    "\n",
    "    # APF\n",
    "    apf = TAN.AuxiliaryParticleFilterTAN(system_model, num_particles=2000) \n",
    "    apf_res = apf.process_sequence(y_seq=y_obs_tensor, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_apf = apf_res['x_filtered'].cpu().numpy()\n",
    "    P_apf = apf_res.get('P_filtered', apf_res.get('P', None))\n",
    "    if P_apf is not None: P_apf = P_apf.cpu().numpy()\n",
    "    \n",
    "    # --- D) V√Ωpoƒçet chyb a ANEES ---\n",
    "    min_len = min(len(x_gt), len(x_est_bkn), len(x_est_ukf))\n",
    "    \n",
    "    def calc_metrics(est, gt, P_mat):\n",
    "        diff = est[:min_len] - gt[:min_len]\n",
    "        mse = np.mean(np.sum(diff[:, :2]**2, axis=1)) \n",
    "        pos_err = np.mean(np.sqrt(diff[:, 0]**2 + diff[:, 1]**2))\n",
    "        \n",
    "        anees = np.nan\n",
    "        if P_mat is not None:\n",
    "            anees = calculate_anees(gt[:min_len], est[:min_len], P_mat[:min_len])\n",
    "            \n",
    "        return mse, pos_err, anees\n",
    "\n",
    "    # Calculate for all\n",
    "    mse_bkn, pos_bkn, anees_bkn = calc_metrics(x_est_bkn, x_gt, P_bkn)\n",
    "    mse_ukf, pos_ukf, anees_ukf = calc_metrics(x_est_ukf, x_gt, P_ukf)\n",
    "    mse_pf, pos_pf, anees_pf = calc_metrics(x_est_pf, x_gt, P_pf)\n",
    "    mse_apf, pos_apf, anees_apf = calc_metrics(x_est_apf, x_gt, P_apf)\n",
    "    \n",
    "    # Ulo≈æen√≠\n",
    "    agg_mse[\"BKN\"].append(mse_bkn); agg_pos[\"BKN\"].append(pos_bkn); agg_anees[\"BKN\"].append(anees_bkn)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf); agg_pos[\"UKF\"].append(pos_ukf); agg_anees[\"UKF\"].append(anees_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf);   agg_pos[\"PF\"].append(pos_pf);   agg_anees[\"PF\"].append(anees_pf)\n",
    "    agg_mse[\"APF\"].append(mse_apf); agg_pos[\"APF\"].append(pos_apf); agg_anees[\"APF\"].append(anees_apf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"BKN_PosErr\": pos_bkn, \"BKN_ANEES\": anees_bkn,\n",
    "        \"UKF_PosErr\": pos_ukf, \"UKF_ANEES\": anees_ukf,\n",
    "        \"PF_PosErr\": pos_pf,   \"PF_ANEES\": anees_pf,\n",
    "        \"APF_PosErr\": pos_apf, \"APF_ANEES\": anees_apf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        plt.plot(x_est_bkn[:, 0], x_est_bkn[:, 1], 'g-', linewidth=2, label=f'BKN (Err: {pos_bkn:.1f}m, ANEES: {anees_bkn:.1f})')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_ukf:.1f}m, ANEES: {anees_ukf:.1f})')\n",
    "        # Pro p≈ôehlednost vykresl√≠me jen BKN a UKF, p≈ô√≠padnƒõ odkomentujte PF/APF\n",
    "        # plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.6, label='PF')\n",
    "        plt.title(f\"Test Trajectory {i+1}\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY (Pozice v metrech | ANEES - ide√°l ~4.0)\")\n",
    "print(\"=\"*120)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"BKN_PosErr\", \"BKN_ANEES\", \"UKF_PosErr\", \"UKF_ANEES\", \"PF_PosErr\", \"APF_PosErr\"]])\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({n_samples} trajektori√≠)\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "def get_stats(key):\n",
    "    return (np.nanmean(agg_mse[key]), np.nanstd(agg_mse[key]), \n",
    "            np.nanmean(agg_pos[key]), np.nanstd(agg_pos[key]),\n",
    "            np.nanmean(agg_anees[key]), np.nanstd(agg_anees[key]))\n",
    "\n",
    "bkn_s = get_stats(\"BKN\")\n",
    "ukf_s = get_stats(\"UKF\")\n",
    "pf_s = get_stats(\"PF\")\n",
    "apf_s = get_stats(\"APF\")\n",
    "\n",
    "# Form√°tov√°n√≠ tabulky\n",
    "header = f\"{'Model':<10} | {'Pos Error [m] (Mean ¬± Std)':<30} | {'ANEES (Mean ¬± Std)':<30}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "print(f\"{'BKN':<10} | {bkn_s[2]:.2f} ¬± {bkn_s[3]:.2f} m {'':<14} | {bkn_s[4]:.2f} ¬± {bkn_s[5]:.2f}\")\n",
    "print(f\"{'UKF':<10} | {ukf_s[2]:.2f} ¬± {ukf_s[3]:.2f} m {'':<14} | {ukf_s[4]:.2f} ¬± {ukf_s[5]:.2f}\")\n",
    "print(f\"{'PF':<10} | {pf_s[2]:.2f} ¬± {pf_s[3]:.2f} m {'':<14} | {pf_s[4]:.2f} ¬± {pf_s[5]:.2f}\")\n",
    "print(f\"{'APF':<10} | {apf_s[2]:.2f} ¬± {apf_s[3]:.2f} m {'':<14} | {apf_s[4]:.2f} ¬± {apf_s[5]:.2f}\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Grafick√© porovn√°n√≠ (Boxplot Position Error)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([agg_pos[\"BKN\"], agg_pos[\"UKF\"], agg_pos[\"PF\"]], labels=['BKN', 'UKF', 'PF'], patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "plt.title(\"Position Error [m]\")\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Grafick√© porovn√°n√≠ (Boxplot ANEES)\n",
    "plt.subplot(1, 2, 2)\n",
    "# Filtrujeme NaN pro boxplot\n",
    "anees_data = [\n",
    "    [x for x in agg_anees[\"BKN\"] if not np.isnan(x)],\n",
    "    [x for x in agg_anees[\"UKF\"] if not np.isnan(x)],\n",
    "    [x for x in agg_anees[\"PF\"] if not np.isnan(x)]\n",
    "]\n",
    "plt.boxplot(anees_data, labels=['BKN', 'UKF', 'PF'], patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "plt.axhline(y=4.0, color='r', linestyle='--', label='Ideal (4.0)')\n",
    "plt.title(\"ANEES (Consistency)\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
