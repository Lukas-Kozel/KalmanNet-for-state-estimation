{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: c:\\Users\\PC1\\Desktop\\Diplomka\\KalmanNet\\data\\data.mat\n",
      "Project root added: c:\\Users\\PC1\\Desktop\\Diplomka\\KalmanNet\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 1\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[3.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import TAN\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [323.7707824707031, -13.519903182983398, -29.721908569335938]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=128 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 3: Seq=300 | Batch=64 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_synthetic_controlled'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-4,             \n",
    "        'batch_size': 128\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3ad555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_BayesianKalmanNet_Hybrid(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad,\n",
    "    J_samples, validation_period, logging_period,\n",
    "    warmup_iterations=0, weight_decay_=1e-5,\n",
    "    lambda_mse=100.0  # <--- NOV√ù PARAMETR: Kotva pro MSE\n",
    "):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    \n",
    "    # Scheduler: Pokud se loss zasekne, sn√≠≈æ√≠me LR (pom√°h√° stabilizovat konvergenci)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer, mode='min', factor=0.5, patience=50\n",
    "    # )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    score_at_best = {\"val_nll\": 0.0, \"val_mse\": 0.0}\n",
    "    best_iter_count = 0\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START Hybrid Training: Loss = NLL + {lambda_mse} * MSE\")\n",
    "    print(f\"    Logging period: {logging_period} iterations\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            if torch.isnan(x_true_batch).any():\n",
    "                print(f\"!!! SKIP BATCH iter {train_iter_count}: NaN found in x_true (Ground Truth) !!!\")\n",
    "                continue\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            # --- Training ---\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, seq_len, _ = x_true_batch.shape\n",
    "            \n",
    "            all_trajectories_for_ensemble = []\n",
    "            all_regs_for_ensemble = []\n",
    "\n",
    "            # 1. Ensemble Forward Pass\n",
    "            for j in range(J_samples):\n",
    "                model.reset(batch_size=batch_size, initial_state=x_true_batch[:, 0, :])\n",
    "                current_trajectory_x_hats = []\n",
    "                current_trajectory_regs = []\n",
    "                for t in range(1, seq_len):\n",
    "                    y_t = y_meas_batch[:, t, :]\n",
    "                    x_filtered_t, reg_t = model.step(y_t)\n",
    "                    if torch.isnan(x_filtered_t).any():\n",
    "                            raise ValueError(f\"NaN in x_filtered_t at sample {j}, step {t}\")\n",
    "                    current_trajectory_x_hats.append(x_filtered_t)\n",
    "                    current_trajectory_regs.append(reg_t)\n",
    "                all_trajectories_for_ensemble.append(torch.stack(current_trajectory_x_hats, dim=1))\n",
    "                all_regs_for_ensemble.append(torch.sum(torch.stack(current_trajectory_regs)))\n",
    "\n",
    "            # 2. Statistiky Ensemble\n",
    "            ensemble_trajectories = torch.stack(all_trajectories_for_ensemble, dim=0)\n",
    "            x_hat_sequence = ensemble_trajectories.mean(dim=0)\n",
    "            \n",
    "            # Epistemick√° variance (ƒçist√Ω rozptyl s√≠tƒõ)\n",
    "            # P≈ôiƒç√≠t√°me 1e-9 jen proti dƒõlen√≠ nulou, nen√≠ to \"noise floor\"\n",
    "            cov_diag_sequence = ensemble_trajectories.var(dim=0) + 1e-9 \n",
    "            \n",
    "            regularization_loss = torch.stack(all_regs_for_ensemble).mean()\n",
    "            target_sequence = x_true_batch[:, 1:, :]\n",
    "            \n",
    "            # --- 3. V√ùPOƒåET HYBRIDN√ç LOSS ---\n",
    "            \n",
    "            # A) MSE ƒå√°st (P≈ôesnost)\n",
    "            mse_loss = F.mse_loss(x_hat_sequence, target_sequence)\n",
    "            \n",
    "            # B) NLL ƒå√°st (Konzistence)\n",
    "            # 0.5 * (log(var) + (target - pred)^2 / var)\n",
    "            cov_diag_clamped = torch.clamp(cov_diag_sequence, min=1e-4, max=1e6)\n",
    "            error_sq = (x_hat_sequence - target_sequence) ** 2\n",
    "            nll_term = 0.5 * (torch.log(cov_diag_clamped) + error_sq / cov_diag_clamped)\n",
    "            nll_loss = nll_term.mean()\n",
    "            mean_var = cov_diag_sequence.mean()\n",
    "            var_penalty = torch.relu(mean_var - 100.0) * 0.01\n",
    "            \n",
    "            # C) Celkov√° Loss (Hybrid)\n",
    "            # Zde je ta magie: I kdy≈æ NLL chce ut√©ct s varianc√≠, lambda_mse * mse ho dr≈æ√≠ zp√°tky\n",
    "            weighted_mse = lambda_mse * mse_loss\n",
    "            loss = nll_loss + weighted_mse + regularization_loss * 1.0 + var_penalty\n",
    "            \n",
    "            if torch.isnan(loss): \n",
    "                print(\"Collapse detected (NaN loss)\"); done = True; break\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # --- DIAGNOSTIC LOGGING (Gradients) ---\n",
    "            # Zaznamen√°me statistiky gradient≈Ø p≈ôed o≈ô√≠znut√≠m (clippingem)\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                total_norm = 0.0\n",
    "                max_grad = 0.0\n",
    "                min_grad = float('inf')\n",
    "                nan_grad_detected = False\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2).item()\n",
    "                        total_norm += param_norm ** 2\n",
    "                        p_max = p.grad.data.abs().max().item()\n",
    "                        p_min = p.grad.data.abs().min().item()\n",
    "                        if p_max > max_grad: max_grad = p_max\n",
    "                        if p_min < min_grad: min_grad = p_min\n",
    "                        if torch.isnan(p.grad).any():\n",
    "                            nan_grad_detected = True\n",
    "                total_norm = total_norm ** 0.5\n",
    "                \n",
    "                if nan_grad_detected:\n",
    "                     print(f\"!!! WARNING: NaN gradient detected at iter {train_iter_count} !!!\")\n",
    "\n",
    "            if clip_grad > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            train_iter_count += 1\n",
    "            \n",
    "            # --- Logging ---\n",
    "            diff = x_hat_sequence - target_sequence\n",
    "            mean_error = diff.abs().mean().item()\n",
    "            min_variance = cov_diag_sequence.min().item()\n",
    "            max_variance = cov_diag_sequence.max().item()\n",
    "            mean_variance = cov_diag_sequence.mean().item()\n",
    "\n",
    "            if train_iter_count % logging_period == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Zjist√≠me dropout pravdƒõpodobnosti (jen pro info)\n",
    "                    p1 = torch.sigmoid(model.dnn.concrete_dropout1.p_logit).item()\n",
    "                    p2 = torch.sigmoid(model.dnn.concrete_dropout2.p_logit).item()\n",
    "                \n",
    "                print(f\"--- Iter [{train_iter_count}/{total_train_iter}] ---\")\n",
    "                print(f\"    Total Loss:     {loss.item():.4f}\")\n",
    "                print(f\"    MSE (Raw):      {mse_loss.item():.6f}\")\n",
    "                print(f\"    MSE (Weighted): {weighted_mse.item():.4f} (lambda={lambda_mse})\")\n",
    "                print(f\"    NLL Component:  {nll_loss.item():.4f}\")\n",
    "                print(f\"    Reg Loss:       {regularization_loss.item():.6f}\")\n",
    "                print(f\"    Variance stats: Min={min_variance:.2e}, Max={max_variance:.2e}, Mean={mean_variance:.2e}\")\n",
    "                print(f\"    Mean Error L1:  {mean_error:.4f}\")\n",
    "                print(f\"    Grad Norm:      {total_norm:.4f} (Max abs grad: {max_grad:.4f})\")\n",
    "                print(f\"    Dropout probs:  p1={p1:.4f}, p2={p2:.4f}\")\n",
    "                \n",
    "                # Check pro \"Variance collapse\"\n",
    "                if mean_variance < 1e-8:\n",
    "                    print(\"    !!! WARNING: Variance is extremely low (Collapse risk) !!!\")\n",
    "\n",
    "            # --- Validation step ---\n",
    "            if train_iter_count > 0 and train_iter_count % validation_period == 0:\n",
    "                # Step scheduleru podle tr√©novac√≠ loss (nebo validace, pokud bys to p≈ôedƒõlal)\n",
    "                # scheduler.step(loss)\n",
    "                \n",
    "                print(f\"\\n--- Validation at iteration {train_iter_count} ---\")\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                all_val_x_true_cpu, all_val_x_hat_cpu, all_val_P_hat_cpu = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_true_val_batch, y_meas_val_batch in val_loader:\n",
    "                        val_batch_size, val_seq_len, _ = x_true_val_batch.shape\n",
    "                        x_true_val_batch = x_true_val_batch.to(device)\n",
    "                        y_meas_val_batch = y_meas_val_batch.to(device)\n",
    "                        val_ensemble_trajectories = []\n",
    "                        for j in range(J_samples):\n",
    "                            model.reset(batch_size=val_batch_size, initial_state=x_true_val_batch[:, 0, :])\n",
    "                            val_current_x_hats = []\n",
    "                            for t in range(1, val_seq_len):\n",
    "                                y_t_val = y_meas_val_batch[:, t, :]\n",
    "                                x_filtered_t, _ = model.step(y_t_val)\n",
    "                                val_current_x_hats.append(x_filtered_t)\n",
    "                            val_ensemble_trajectories.append(torch.stack(val_current_x_hats, dim=1))\n",
    "                        \n",
    "                        # Agregace validace\n",
    "                        val_ensemble = torch.stack(val_ensemble_trajectories, dim=0)\n",
    "                        val_preds_seq = val_ensemble.mean(dim=0)\n",
    "                        \n",
    "                        val_target_seq = x_true_val_batch[:, 1:, :]\n",
    "                        val_mse_list.append(F.mse_loss(val_preds_seq, val_target_seq).item())\n",
    "                        \n",
    "                        # P≈ô√≠prava pro ANEES\n",
    "                        initial_state_val = x_true_val_batch[:, 0, :].unsqueeze(1)\n",
    "                        full_x_hat = torch.cat([initial_state_val, val_preds_seq], dim=1)\n",
    "                        \n",
    "                        # Epistemick√° variance\n",
    "                        val_covs_diag = val_ensemble.var(dim=0) + 1e-9\n",
    "                        \n",
    "                        # Vytvo≈ôen√≠ diagon√°ln√≠ch matic P\n",
    "                        # (Zjednodu≈°en√° konstrukce pro ANEES calc)\n",
    "                        # Pro p≈ôesn√© ANEES bychom mƒõli dƒõlat outer product, \n",
    "                        # ale diagon√°la z var() je dobr√° aproximace pro BKN\n",
    "                        val_covs_full = torch.zeros(val_batch_size, val_seq_len-1, 4, 4, device=device)\n",
    "                        for b in range(val_batch_size):\n",
    "                            for t in range(val_seq_len-1):\n",
    "                                val_covs_full[b, t] = torch.diag(val_covs_diag[b, t])\n",
    "\n",
    "                        P0 = model.system_model.P0.unsqueeze(0).repeat(val_batch_size, 1, 1).unsqueeze(1)\n",
    "                        full_P_hat = torch.cat([P0, val_covs_full], dim=1)\n",
    "                        \n",
    "                        all_val_x_true_cpu.append(x_true_val_batch.cpu())\n",
    "                        all_val_x_hat_cpu.append(full_x_hat.cpu())\n",
    "                        all_val_P_hat_cpu.append(full_P_hat.cpu())\n",
    "\n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                final_x_true_list = torch.cat(all_val_x_true_cpu, dim=0)\n",
    "                final_x_hat_list = torch.cat(all_val_x_hat_cpu, dim=0)\n",
    "                final_P_hat_list = torch.cat(all_val_P_hat_cpu, dim=0)\n",
    "                \n",
    "                # V√Ωpoƒçet ANEES\n",
    "                try:\n",
    "                    avg_val_anees = trainer.calculate_anees_vectorized(final_x_true_list, final_x_hat_list, final_P_hat_list)\n",
    "                except Exception as e:\n",
    "                    print(f\"  !!! Error calculating ANEES: {e}\")\n",
    "                    avg_val_anees = float('nan')\n",
    "                \n",
    "                print(f\"  Average MSE: {avg_val_mse:.4f}, Average ANEES: {avg_val_anees:.4f}\")\n",
    "                \n",
    "                # Ukl√°d√°n√≠ modelu:\n",
    "                if not np.isnan(avg_val_anees) and avg_val_anees < best_val_anees and avg_val_anees > 0:\n",
    "                    print(f\"  >>> New best VALIDATION ANEES! Saving model. (Old: {best_val_anees:.4f} -> New: {avg_val_anees:.4f}) <<<\")\n",
    "                    best_val_anees = avg_val_anees\n",
    "                    best_iter_count = train_iter_count\n",
    "                    score_at_best['val_mse'] = avg_val_mse\n",
    "                    best_model_state = deepcopy(model.state_dict())\n",
    "                print(\"-\" * 50)\n",
    "                model.train()\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    if best_model_state:\n",
    "        print(f\"Loading best model from iteration {best_iter_count} with ANEES {best_val_anees:.4f}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(\"No best model was saved; returning last state.\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_anees\": best_val_anees,\n",
    "        \"best_val_nll\": score_at_best['val_nll'],\n",
    "        \"best_val_mse\": score_at_best['val_mse'],\n",
    "        \"best_iter\": best_iter_count,\n",
    "        \"final_model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0fe051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_BayesianKalmanNet_TBPTT(\n",
    "    model, train_loader, val_loader, device,\n",
    "    total_train_iter, learning_rate, clip_grad=1.0,\n",
    "    J_samples=5, tbptt_steps=20, # D√©lka okna (krok≈Ø), po kter√Ωch dƒõl√°me update\n",
    "    validation_period=50, logging_period=10,\n",
    "    weight_decay_=1e-5, lambda_mse=1000.0\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=100\n",
    "    )\n",
    "\n",
    "    best_val_anees = float('inf')\n",
    "    best_model_state = None\n",
    "    train_iter_count = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"üöÄ START TBPTT Training (Window={tbptt_steps}): Loss = NLL + {lambda_mse} * MSE\")\n",
    "\n",
    "    while not done:\n",
    "        model.train()\n",
    "        for x_true_batch, y_meas_batch in train_loader:\n",
    "            if train_iter_count >= total_train_iter: done = True; break\n",
    "            \n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "            y_meas_batch = y_meas_batch.to(device)\n",
    "            \n",
    "            batch_size, seq_len, dim_x = x_true_batch.shape\n",
    "            \n",
    "            # === VEKTORIZOVAN√ù ENSEMBLE ===\n",
    "            # M√≠sto cyklu p≈ôes J udƒõl√°me \"Super Batch\": [Batch * J, Seq, Dim]\n",
    "            # T√≠m p√°dem model zpracuje v≈°echny vzorky nar√°z paralelnƒõ.\n",
    "            x_true_super = x_true_batch.repeat_interleave(J_samples, dim=0)\n",
    "            y_meas_super = y_meas_batch.repeat_interleave(J_samples, dim=0)\n",
    "            super_batch_size = x_true_super.shape[0]\n",
    "\n",
    "            # 1. Reset na zaƒç√°tku sekvence (t=0)\n",
    "            # Inicializujeme pro v≈°echny vzorky najednou\n",
    "            model.reset(batch_size=super_batch_size, initial_state=x_true_super[:, 0, :])\n",
    "\n",
    "            # 2. TBPTT Smyƒçka p≈ôes okna\n",
    "            # Kr√°j√≠me sekvenci na kousky d√©lky 'tbptt_steps'\n",
    "            for t_start in range(1, seq_len, tbptt_steps):\n",
    "                t_end = min(t_start + tbptt_steps, seq_len)\n",
    "                current_window_len = t_end - t_start\n",
    "                if current_window_len <= 0: continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Ulo≈æi≈°tƒõ pro predikce v oknƒõ\n",
    "                window_x_preds = []\n",
    "                window_regs = []\n",
    "\n",
    "                # A) Forward pass oknem\n",
    "                for t in range(t_start, t_end):\n",
    "                    y_t = y_meas_super[:, t, :] # [Batch*J, 3]\n",
    "                    x_est, reg = model.step(y_t)\n",
    "                    \n",
    "                    window_x_preds.append(x_est)\n",
    "                    window_regs.append(reg)\n",
    "                \n",
    "                # B) Zpracov√°n√≠ v√Ωsledk≈Ø okna\n",
    "                # Stackneme ƒças: [Batch*J, Window, 4]\n",
    "                preds_super = torch.stack(window_x_preds, dim=1)\n",
    "                regs_super = torch.stack(window_regs) # [Window, Batch*J]\n",
    "                \n",
    "                # Rozbal√≠me zpƒõt na [Batch, J, Window, 4] pro statistiku\n",
    "                preds_reshaped = preds_super.view(batch_size, J_samples, current_window_len, dim_x)\n",
    "                \n",
    "                # Statistiky Ensemble (p≈ôes dimenzi J=1)\n",
    "                x_hat_seq = preds_reshaped.mean(dim=1) # [Batch, Window, 4]\n",
    "                cov_diag_seq = preds_reshaped.var(dim=1) + 1e-4 # [Batch, Window, 4]\n",
    "                \n",
    "                # Ground Truth pro toto okno\n",
    "                target_seq = x_true_batch[:, t_start:t_end, :]\n",
    "\n",
    "                # C) V√Ωpoƒçet Loss (pouze pro toto okno!)\n",
    "                mse_loss = F.mse_loss(x_hat_seq, target_seq)\n",
    "                \n",
    "                cov_clamped = torch.clamp(cov_diag_seq, min=1e-4, max=1e6)\n",
    "                error_sq = (x_hat_seq - target_seq) ** 2\n",
    "                nll_term = 0.5 * (torch.log(cov_clamped) + error_sq / cov_clamped)\n",
    "                nll_loss = nll_term.mean()\n",
    "                \n",
    "                # Normalizovan√° regularizace na okno\n",
    "                reg_loss = regs_super.mean() / current_window_len \n",
    "                \n",
    "                # Variance penalty (voliteln√©, proti alibismu)\n",
    "                var_penalty = torch.relu(cov_diag_seq.mean() - 100.0) * 0.01\n",
    "\n",
    "                loss = nll_loss + (lambda_mse * mse_loss) + reg_loss + var_penalty\n",
    "                \n",
    "                # D) Backward & Update\n",
    "                loss.backward()\n",
    "                if clip_grad > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # E) !!! DETACH HIDDEN STATE !!!\n",
    "                # Toto je podstata TBPTT. Utneme gradienty, ale nech√°me hodnoty stav≈Ø\n",
    "                # pro dal≈°√≠ okno.\n",
    "                model.detach_hidden()\n",
    "                \n",
    "                train_iter_count += 1 # Poƒç√≠t√°me ka≈æd√Ω update (okno) jako iteraci\n",
    "\n",
    "                # --- Logging (uvnit≈ô okna, aby to bylo ƒçast√©) ---\n",
    "                if train_iter_count % logging_period == 0:\n",
    "                    print(f\"Iter {train_iter_count} (TBPTT): Loss {loss.item():.4f} | MSE {mse_loss.item():.2f} | VarMean {cov_diag_seq.mean().item():.1f}\")\n",
    "\n",
    "            # --- Validation step (Na konci batchi, ne okna) ---\n",
    "            if train_iter_count % validation_period == 0:  # Zjednodu≈°en√° podm√≠nka\n",
    "                model.eval()\n",
    "                val_mse_list = []\n",
    "                # Pro validaci mus√≠me bohu≈æel iterovat po staru nebo taky vektorizovanƒõ\n",
    "                # Zde zjednodu≈°enƒõ celou sekvenci nar√°z (bez TBPTT, jen inference)\n",
    "                with torch.no_grad():\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                        b_val, s_val, _ = x_val.shape\n",
    "                        \n",
    "                        # Vektorizovan√Ω validace\n",
    "                        x_val_super = x_val.repeat_interleave(J_samples, dim=0)\n",
    "                        y_val_super = y_val.repeat_interleave(J_samples, dim=0)\n",
    "                        \n",
    "                        model.reset(batch_size=b_val*J_samples, initial_state=x_val_super[:,0,:])\n",
    "                        preds = []\n",
    "                        for t in range(1, s_val):\n",
    "                            est, _ = model.step(y_val_super[:, t, :])\n",
    "                            preds.append(est)\n",
    "                        \n",
    "                        preds_stack = torch.stack(preds, dim=1).view(b_val, J_samples, s_val-1, 4)\n",
    "                        mean_preds = preds_stack.mean(dim=1)\n",
    "                        val_mse_list.append(F.mse_loss(mean_preds, x_val[:, 1:, :]).item())\n",
    "                \n",
    "                avg_val_mse = np.mean(val_mse_list)\n",
    "                print(f\"\\n--- VALIDATION: MSE {avg_val_mse:.4f} ---\")\n",
    "                \n",
    "                if avg_val_mse < 200000 and avg_val_mse > 0: # Simple guard\n",
    "                     best_model_state = deepcopy(model.state_dict())\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "    print(\"TBPTT Training completed.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return {\"final_model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE BKN MODELU ===\n",
      "INFO: Aplikuji upravenou inicializaci pro BKN.\n",
      "DEBUG: V√Ωstupn√≠ vrstva inicializov√°na konzervativnƒõ (interval -0.1 a≈æ 0.1).\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 1: SeqLen 10 | LR 0.001 | TBPTT: False\n",
      "============================================================\n",
      "   -> Using Standard Hybrid Training\n",
      "üöÄ START Hybrid Training: Loss = NLL + 1.0 * MSE\n",
      "    Logging period: 10 iterations\n",
      "--- Iter [10/500] ---\n",
      "    Total Loss:     231.9101\n",
      "    MSE (Raw):      217.272110\n",
      "    MSE (Weighted): 217.2721 (lambda=1.0)\n",
      "    NLL Component:  14.6290\n",
      "    Reg Loss:       0.008999\n",
      "    Variance stats: Min=1.00e-09, Max=9.80e+03, Mean=4.29e+01\n",
      "    Mean Error L1:  8.2209\n",
      "    Grad Norm:      14116.9275 (Max abs grad: 3844.8779)\n",
      "    Dropout probs:  p1=0.2830, p2=0.1220\n",
      "\n",
      "--- Validation at iteration 10 ---\n",
      "  Average MSE: 240.6018, Average ANEES: 59.0878\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: inf -> New: 59.0878) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [20/500] ---\n",
      "    Total Loss:     70.2482\n",
      "    MSE (Raw):      58.628323\n",
      "    MSE (Weighted): 58.6283 (lambda=1.0)\n",
      "    NLL Component:  11.6108\n",
      "    Reg Loss:       0.009011\n",
      "    Variance stats: Min=1.00e-09, Max=1.67e+03, Mean=1.81e+01\n",
      "    Mean Error L1:  4.8056\n",
      "    Grad Norm:      208.0348 (Max abs grad: 40.0783)\n",
      "    Dropout probs:  p1=0.2840, p2=0.1226\n",
      "\n",
      "--- Validation at iteration 20 ---\n",
      "  Average MSE: 58.5806, Average ANEES: 34.8247\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 59.0878 -> New: 34.8247) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [30/500] ---\n",
      "    Total Loss:     48.6116\n",
      "    MSE (Raw):      35.905090\n",
      "    MSE (Weighted): 35.9051 (lambda=1.0)\n",
      "    NLL Component:  12.6975\n",
      "    Reg Loss:       0.009024\n",
      "    Variance stats: Min=1.00e-09, Max=3.88e+03, Mean=2.18e+01\n",
      "    Mean Error L1:  3.7934\n",
      "    Grad Norm:      108.9948 (Max abs grad: 30.0578)\n",
      "    Dropout probs:  p1=0.2849, p2=0.1234\n",
      "\n",
      "--- Validation at iteration 30 ---\n",
      "  Average MSE: 44.2908, Average ANEES: 21.1338\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 34.8247 -> New: 21.1338) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [40/500] ---\n",
      "    Total Loss:     32.1979\n",
      "    MSE (Raw):      27.685869\n",
      "    MSE (Weighted): 27.6859 (lambda=1.0)\n",
      "    NLL Component:  4.5030\n",
      "    Reg Loss:       0.009039\n",
      "    Variance stats: Min=1.00e-09, Max=9.09e+03, Mean=2.12e+01\n",
      "    Mean Error L1:  3.2747\n",
      "    Grad Norm:      99.3298 (Max abs grad: 20.8926)\n",
      "    Dropout probs:  p1=0.2861, p2=0.1238\n",
      "\n",
      "--- Validation at iteration 40 ---\n",
      "  Average MSE: 32.0771, Average ANEES: 15.2339\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 21.1338 -> New: 15.2339) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [50/500] ---\n",
      "    Total Loss:     37.0205\n",
      "    MSE (Raw):      32.111500\n",
      "    MSE (Weighted): 32.1115 (lambda=1.0)\n",
      "    NLL Component:  4.9000\n",
      "    Reg Loss:       0.009052\n",
      "    Variance stats: Min=1.00e-09, Max=7.20e+03, Mean=2.54e+01\n",
      "    Mean Error L1:  3.3695\n",
      "    Grad Norm:      92.5475 (Max abs grad: 49.0008)\n",
      "    Dropout probs:  p1=0.2875, p2=0.1239\n",
      "\n",
      "--- Validation at iteration 50 ---\n",
      "  Average MSE: 31.9023, Average ANEES: 15.4094\n",
      "--------------------------------------------------\n",
      "--- Iter [60/500] ---\n",
      "    Total Loss:     74.0165\n",
      "    MSE (Raw):      26.860359\n",
      "    MSE (Weighted): 26.8604 (lambda=1.0)\n",
      "    NLL Component:  47.1470\n",
      "    Reg Loss:       0.009063\n",
      "    Variance stats: Min=1.00e-09, Max=7.70e+03, Mean=2.50e+01\n",
      "    Mean Error L1:  3.2107\n",
      "    Grad Norm:      27.9654 (Max abs grad: 7.7158)\n",
      "    Dropout probs:  p1=0.2885, p2=0.1240\n",
      "\n",
      "--- Validation at iteration 60 ---\n",
      "  Average MSE: 47.7260, Average ANEES: 14.6636\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 15.2339 -> New: 14.6636) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [70/500] ---\n",
      "    Total Loss:     32.7205\n",
      "    MSE (Raw):      29.796394\n",
      "    MSE (Weighted): 29.7964 (lambda=1.0)\n",
      "    NLL Component:  2.9151\n",
      "    Reg Loss:       0.009067\n",
      "    Variance stats: Min=1.00e-09, Max=1.92e+04, Mean=3.04e+01\n",
      "    Mean Error L1:  3.2012\n",
      "    Grad Norm:      32.4522 (Max abs grad: 9.2570)\n",
      "    Dropout probs:  p1=0.2891, p2=0.1241\n",
      "\n",
      "--- Validation at iteration 70 ---\n",
      "  Average MSE: 25.3910, Average ANEES: 13.0364\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 14.6636 -> New: 13.0364) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [80/500] ---\n",
      "    Total Loss:     29.2384\n",
      "    MSE (Raw):      23.312263\n",
      "    MSE (Weighted): 23.3123 (lambda=1.0)\n",
      "    NLL Component:  5.9171\n",
      "    Reg Loss:       0.009071\n",
      "    Variance stats: Min=1.00e-09, Max=7.87e+02, Mean=1.77e+01\n",
      "    Mean Error L1:  3.1136\n",
      "    Grad Norm:      64.6391 (Max abs grad: 16.6382)\n",
      "    Dropout probs:  p1=0.2895, p2=0.1242\n",
      "\n",
      "--- Validation at iteration 80 ---\n",
      "  Average MSE: 39.7364, Average ANEES: 13.7932\n",
      "--------------------------------------------------\n",
      "--- Iter [90/500] ---\n",
      "    Total Loss:     29.0670\n",
      "    MSE (Raw):      24.710077\n",
      "    MSE (Weighted): 24.7101 (lambda=1.0)\n",
      "    NLL Component:  4.3479\n",
      "    Reg Loss:       0.009071\n",
      "    Variance stats: Min=1.00e-09, Max=1.14e+04, Mean=2.17e+01\n",
      "    Mean Error L1:  3.0814\n",
      "    Grad Norm:      50.2331 (Max abs grad: 14.0887)\n",
      "    Dropout probs:  p1=0.2896, p2=0.1240\n",
      "\n",
      "--- Validation at iteration 90 ---\n",
      "  Average MSE: 47.4549, Average ANEES: 13.6383\n",
      "--------------------------------------------------\n",
      "--- Iter [100/500] ---\n",
      "    Total Loss:     29.7585\n",
      "    MSE (Raw):      25.197212\n",
      "    MSE (Weighted): 25.1972 (lambda=1.0)\n",
      "    NLL Component:  4.5523\n",
      "    Reg Loss:       0.009072\n",
      "    Variance stats: Min=1.00e-09, Max=1.20e+04, Mean=2.72e+01\n",
      "    Mean Error L1:  3.0743\n",
      "    Grad Norm:      44.3019 (Max abs grad: 14.8455)\n",
      "    Dropout probs:  p1=0.2898, p2=0.1239\n",
      "\n",
      "--- Validation at iteration 100 ---\n",
      "  Average MSE: 28.7680, Average ANEES: 12.8497\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 13.0364 -> New: 12.8497) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [110/500] ---\n",
      "    Total Loss:     31.6005\n",
      "    MSE (Raw):      27.269356\n",
      "    MSE (Weighted): 27.2694 (lambda=1.0)\n",
      "    NLL Component:  4.3221\n",
      "    Reg Loss:       0.009070\n",
      "    Variance stats: Min=1.00e-09, Max=7.34e+03, Mean=2.54e+01\n",
      "    Mean Error L1:  3.0854\n",
      "    Grad Norm:      137.9392 (Max abs grad: 44.4216)\n",
      "    Dropout probs:  p1=0.2897, p2=0.1240\n",
      "\n",
      "--- Validation at iteration 110 ---\n",
      "  Average MSE: 26.8250, Average ANEES: 13.9413\n",
      "--------------------------------------------------\n",
      "--- Iter [120/500] ---\n",
      "    Total Loss:     28.6391\n",
      "    MSE (Raw):      21.494524\n",
      "    MSE (Weighted): 21.4945 (lambda=1.0)\n",
      "    NLL Component:  7.1355\n",
      "    Reg Loss:       0.009070\n",
      "    Variance stats: Min=1.00e-09, Max=1.18e+04, Mean=2.15e+01\n",
      "    Mean Error L1:  2.9687\n",
      "    Grad Norm:      231.2918 (Max abs grad: 97.8562)\n",
      "    Dropout probs:  p1=0.2898, p2=0.1240\n",
      "\n",
      "--- Validation at iteration 120 ---\n",
      "  Average MSE: 31.8421, Average ANEES: 14.0197\n",
      "--------------------------------------------------\n",
      "--- Iter [130/500] ---\n",
      "    Total Loss:     26.8160\n",
      "    MSE (Raw):      22.463015\n",
      "    MSE (Weighted): 22.4630 (lambda=1.0)\n",
      "    NLL Component:  4.3439\n",
      "    Reg Loss:       0.009072\n",
      "    Variance stats: Min=1.00e-09, Max=1.21e+04, Mean=2.55e+01\n",
      "    Mean Error L1:  2.9613\n",
      "    Grad Norm:      21.7176 (Max abs grad: 5.4220)\n",
      "    Dropout probs:  p1=0.2899, p2=0.1244\n",
      "\n",
      "--- Validation at iteration 130 ---\n",
      "  Average MSE: 31.2026, Average ANEES: 12.8521\n",
      "--------------------------------------------------\n",
      "--- Iter [140/500] ---\n",
      "    Total Loss:     21.8561\n",
      "    MSE (Raw):      18.061872\n",
      "    MSE (Weighted): 18.0619 (lambda=1.0)\n",
      "    NLL Component:  3.7852\n",
      "    Reg Loss:       0.009077\n",
      "    Variance stats: Min=1.00e-09, Max=1.44e+04, Mean=2.23e+01\n",
      "    Mean Error L1:  2.7532\n",
      "    Grad Norm:      33.2885 (Max abs grad: 14.3629)\n",
      "    Dropout probs:  p1=0.2903, p2=0.1245\n",
      "\n",
      "--- Validation at iteration 140 ---\n",
      "  Average MSE: 37.4158, Average ANEES: 11.9663\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 12.8497 -> New: 11.9663) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [150/500] ---\n",
      "    Total Loss:     41.1433\n",
      "    MSE (Raw):      23.175972\n",
      "    MSE (Weighted): 23.1760 (lambda=1.0)\n",
      "    NLL Component:  17.9583\n",
      "    Reg Loss:       0.009077\n",
      "    Variance stats: Min=1.00e-09, Max=1.42e+03, Mean=1.83e+01\n",
      "    Mean Error L1:  3.0144\n",
      "    Grad Norm:      47.0572 (Max abs grad: 22.8953)\n",
      "    Dropout probs:  p1=0.2902, p2=0.1245\n",
      "\n",
      "--- Validation at iteration 150 ---\n",
      "  Average MSE: 25.2946, Average ANEES: 11.8665\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 11.9663 -> New: 11.8665) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [160/500] ---\n",
      "    Total Loss:     23.2037\n",
      "    MSE (Raw):      19.645863\n",
      "    MSE (Weighted): 19.6459 (lambda=1.0)\n",
      "    NLL Component:  3.5488\n",
      "    Reg Loss:       0.009076\n",
      "    Variance stats: Min=1.00e-09, Max=8.61e+03, Mean=2.27e+01\n",
      "    Mean Error L1:  2.8464\n",
      "    Grad Norm:      29.8247 (Max abs grad: 8.5461)\n",
      "    Dropout probs:  p1=0.2902, p2=0.1245\n",
      "\n",
      "--- Validation at iteration 160 ---\n",
      "  Average MSE: 28.1619, Average ANEES: 11.7524\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 11.8665 -> New: 11.7524) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [170/500] ---\n",
      "    Total Loss:     26.3986\n",
      "    MSE (Raw):      22.467838\n",
      "    MSE (Weighted): 22.4678 (lambda=1.0)\n",
      "    NLL Component:  3.9217\n",
      "    Reg Loss:       0.009080\n",
      "    Variance stats: Min=1.00e-09, Max=1.42e+04, Mean=2.93e+01\n",
      "    Mean Error L1:  2.9665\n",
      "    Grad Norm:      42.2797 (Max abs grad: 14.5835)\n",
      "    Dropout probs:  p1=0.2905, p2=0.1247\n",
      "\n",
      "--- Validation at iteration 170 ---\n",
      "  Average MSE: 37.1086, Average ANEES: 11.2512\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 11.7524 -> New: 11.2512) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [180/500] ---\n",
      "    Total Loss:     31.0757\n",
      "    MSE (Raw):      26.565058\n",
      "    MSE (Weighted): 26.5651 (lambda=1.0)\n",
      "    NLL Component:  4.5016\n",
      "    Reg Loss:       0.009075\n",
      "    Variance stats: Min=1.00e-09, Max=9.55e+03, Mean=3.18e+01\n",
      "    Mean Error L1:  2.9172\n",
      "    Grad Norm:      26.3332 (Max abs grad: 7.8959)\n",
      "    Dropout probs:  p1=0.2901, p2=0.1245\n",
      "\n",
      "--- Validation at iteration 180 ---\n",
      "  Average MSE: 37.7880, Average ANEES: 11.0928\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 11.2512 -> New: 11.0928) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [190/500] ---\n",
      "    Total Loss:     24.6174\n",
      "    MSE (Raw):      20.460421\n",
      "    MSE (Weighted): 20.4604 (lambda=1.0)\n",
      "    NLL Component:  4.1479\n",
      "    Reg Loss:       0.009071\n",
      "    Variance stats: Min=1.00e-09, Max=1.19e+04, Mean=2.38e+01\n",
      "    Mean Error L1:  2.8116\n",
      "    Grad Norm:      47.4061 (Max abs grad: 14.9841)\n",
      "    Dropout probs:  p1=0.2900, p2=0.1243\n",
      "\n",
      "--- Validation at iteration 190 ---\n",
      "  Average MSE: 28.4644, Average ANEES: 11.3326\n",
      "--------------------------------------------------\n",
      "--- Iter [200/500] ---\n",
      "    Total Loss:     24.5175\n",
      "    MSE (Raw):      20.128452\n",
      "    MSE (Weighted): 20.1285 (lambda=1.0)\n",
      "    NLL Component:  4.3800\n",
      "    Reg Loss:       0.009070\n",
      "    Variance stats: Min=1.00e-09, Max=1.64e+03, Mean=2.13e+01\n",
      "    Mean Error L1:  2.8860\n",
      "    Grad Norm:      26.9935 (Max abs grad: 5.7056)\n",
      "    Dropout probs:  p1=0.2900, p2=0.1242\n",
      "\n",
      "--- Validation at iteration 200 ---\n",
      "  Average MSE: 39.3245, Average ANEES: 10.9415\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 11.0928 -> New: 10.9415) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [210/500] ---\n",
      "    Total Loss:     22.0226\n",
      "    MSE (Raw):      18.641941\n",
      "    MSE (Weighted): 18.6419 (lambda=1.0)\n",
      "    NLL Component:  3.3716\n",
      "    Reg Loss:       0.009065\n",
      "    Variance stats: Min=1.00e-09, Max=1.51e+04, Mean=2.55e+01\n",
      "    Mean Error L1:  2.8003\n",
      "    Grad Norm:      28.3707 (Max abs grad: 8.6558)\n",
      "    Dropout probs:  p1=0.2897, p2=0.1242\n",
      "\n",
      "--- Validation at iteration 210 ---\n",
      "  Average MSE: 34.4296, Average ANEES: 10.5996\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 10.9415 -> New: 10.5996) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [220/500] ---\n",
      "    Total Loss:     35.1706\n",
      "    MSE (Raw):      26.251980\n",
      "    MSE (Weighted): 26.2520 (lambda=1.0)\n",
      "    NLL Component:  8.9096\n",
      "    Reg Loss:       0.009059\n",
      "    Variance stats: Min=1.00e-09, Max=1.06e+04, Mean=2.91e+01\n",
      "    Mean Error L1:  2.9168\n",
      "    Grad Norm:      32.2342 (Max abs grad: 12.4176)\n",
      "    Dropout probs:  p1=0.2894, p2=0.1241\n",
      "\n",
      "--- Validation at iteration 220 ---\n",
      "  Average MSE: 26.6088, Average ANEES: 10.6664\n",
      "--------------------------------------------------\n",
      "--- Iter [230/500] ---\n",
      "    Total Loss:     33.6398\n",
      "    MSE (Raw):      23.445841\n",
      "    MSE (Weighted): 23.4458 (lambda=1.0)\n",
      "    NLL Component:  10.1849\n",
      "    Reg Loss:       0.009052\n",
      "    Variance stats: Min=1.00e-09, Max=1.31e+04, Mean=2.92e+01\n",
      "    Mean Error L1:  3.0147\n",
      "    Grad Norm:      49.0035 (Max abs grad: 18.1280)\n",
      "    Dropout probs:  p1=0.2888, p2=0.1236\n",
      "\n",
      "--- Validation at iteration 230 ---\n",
      "  Average MSE: 32.9505, Average ANEES: 10.5474\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 10.5996 -> New: 10.5474) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [240/500] ---\n",
      "    Total Loss:     22.8223\n",
      "    MSE (Raw):      18.728111\n",
      "    MSE (Weighted): 18.7281 (lambda=1.0)\n",
      "    NLL Component:  4.0852\n",
      "    Reg Loss:       0.009045\n",
      "    Variance stats: Min=1.00e-09, Max=3.07e+03, Mean=1.99e+01\n",
      "    Mean Error L1:  2.7182\n",
      "    Grad Norm:      35.9825 (Max abs grad: 15.8126)\n",
      "    Dropout probs:  p1=0.2883, p2=0.1233\n",
      "\n",
      "--- Validation at iteration 240 ---\n",
      "  Average MSE: 26.0458, Average ANEES: 10.4147\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 10.5474 -> New: 10.4147) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [250/500] ---\n",
      "    Total Loss:     21.8351\n",
      "    MSE (Raw):      17.219250\n",
      "    MSE (Weighted): 17.2192 (lambda=1.0)\n",
      "    NLL Component:  4.6068\n",
      "    Reg Loss:       0.009041\n",
      "    Variance stats: Min=1.00e-09, Max=4.05e+03, Mean=2.03e+01\n",
      "    Mean Error L1:  2.7102\n",
      "    Grad Norm:      50.4514 (Max abs grad: 18.2652)\n",
      "    Dropout probs:  p1=0.2878, p2=0.1230\n",
      "\n",
      "--- Validation at iteration 250 ---\n",
      "  Average MSE: 29.1611, Average ANEES: 9.7676\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 10.4147 -> New: 9.7676) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [260/500] ---\n",
      "    Total Loss:     25.1803\n",
      "    MSE (Raw):      20.489161\n",
      "    MSE (Weighted): 20.4892 (lambda=1.0)\n",
      "    NLL Component:  4.6822\n",
      "    Reg Loss:       0.009032\n",
      "    Variance stats: Min=1.00e-09, Max=8.25e+03, Mean=2.14e+01\n",
      "    Mean Error L1:  2.8530\n",
      "    Grad Norm:      45.8940 (Max abs grad: 18.9895)\n",
      "    Dropout probs:  p1=0.2871, p2=0.1225\n",
      "\n",
      "--- Validation at iteration 260 ---\n",
      "  Average MSE: 23.1651, Average ANEES: 11.0256\n",
      "--------------------------------------------------\n",
      "--- Iter [270/500] ---\n",
      "    Total Loss:     19.8464\n",
      "    MSE (Raw):      14.936417\n",
      "    MSE (Weighted): 14.9364 (lambda=1.0)\n",
      "    NLL Component:  4.9010\n",
      "    Reg Loss:       0.009021\n",
      "    Variance stats: Min=1.00e-09, Max=2.30e+03, Mean=1.82e+01\n",
      "    Mean Error L1:  2.5772\n",
      "    Grad Norm:      40.3724 (Max abs grad: 15.5832)\n",
      "    Dropout probs:  p1=0.2861, p2=0.1221\n",
      "\n",
      "--- Validation at iteration 270 ---\n",
      "  Average MSE: 25.9496, Average ANEES: 9.5889\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.7676 -> New: 9.5889) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [280/500] ---\n",
      "    Total Loss:     23.1160\n",
      "    MSE (Raw):      17.622755\n",
      "    MSE (Weighted): 17.6228 (lambda=1.0)\n",
      "    NLL Component:  5.4843\n",
      "    Reg Loss:       0.009016\n",
      "    Variance stats: Min=1.00e-09, Max=8.81e+02, Mean=1.97e+01\n",
      "    Mean Error L1:  2.7208\n",
      "    Grad Norm:      56.5641 (Max abs grad: 19.9833)\n",
      "    Dropout probs:  p1=0.2856, p2=0.1218\n",
      "\n",
      "--- Validation at iteration 280 ---\n",
      "  Average MSE: 31.1540, Average ANEES: 10.1501\n",
      "--------------------------------------------------\n",
      "--- Iter [290/500] ---\n",
      "    Total Loss:     33.2707\n",
      "    MSE (Raw):      26.991133\n",
      "    MSE (Weighted): 26.9911 (lambda=1.0)\n",
      "    NLL Component:  6.2706\n",
      "    Reg Loss:       0.009015\n",
      "    Variance stats: Min=1.00e-09, Max=1.38e+04, Mean=3.55e+01\n",
      "    Mean Error L1:  2.9028\n",
      "    Grad Norm:      21.0336 (Max abs grad: 6.8942)\n",
      "    Dropout probs:  p1=0.2855, p2=0.1216\n",
      "\n",
      "--- Validation at iteration 290 ---\n",
      "  Average MSE: 28.4798, Average ANEES: 10.0473\n",
      "--------------------------------------------------\n",
      "--- Iter [300/500] ---\n",
      "    Total Loss:     18.7069\n",
      "    MSE (Raw):      14.462457\n",
      "    MSE (Weighted): 14.4625 (lambda=1.0)\n",
      "    NLL Component:  4.2354\n",
      "    Reg Loss:       0.009007\n",
      "    Variance stats: Min=1.00e-09, Max=1.68e+03, Mean=1.75e+01\n",
      "    Mean Error L1:  2.5173\n",
      "    Grad Norm:      60.8674 (Max abs grad: 20.5477)\n",
      "    Dropout probs:  p1=0.2847, p2=0.1213\n",
      "\n",
      "--- Validation at iteration 300 ---\n",
      "  Average MSE: 20.5668, Average ANEES: 10.2932\n",
      "--------------------------------------------------\n",
      "--- Iter [310/500] ---\n",
      "    Total Loss:     30.2092\n",
      "    MSE (Raw):      24.110773\n",
      "    MSE (Weighted): 24.1108 (lambda=1.0)\n",
      "    NLL Component:  6.0894\n",
      "    Reg Loss:       0.008997\n",
      "    Variance stats: Min=1.00e-09, Max=1.48e+04, Mean=2.66e+01\n",
      "    Mean Error L1:  2.6670\n",
      "    Grad Norm:      51.3344 (Max abs grad: 25.3591)\n",
      "    Dropout probs:  p1=0.2837, p2=0.1208\n",
      "\n",
      "--- Validation at iteration 310 ---\n",
      "  Average MSE: 21.4937, Average ANEES: 10.4063\n",
      "--------------------------------------------------\n",
      "--- Iter [320/500] ---\n",
      "    Total Loss:     27.6515\n",
      "    MSE (Raw):      22.417494\n",
      "    MSE (Weighted): 22.4175 (lambda=1.0)\n",
      "    NLL Component:  5.2250\n",
      "    Reg Loss:       0.008986\n",
      "    Variance stats: Min=1.00e-09, Max=9.73e+03, Mean=2.88e+01\n",
      "    Mean Error L1:  2.8124\n",
      "    Grad Norm:      44.4713 (Max abs grad: 19.2664)\n",
      "    Dropout probs:  p1=0.2828, p2=0.1204\n",
      "\n",
      "--- Validation at iteration 320 ---\n",
      "  Average MSE: 23.4530, Average ANEES: 9.5602\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.5889 -> New: 9.5602) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [330/500] ---\n",
      "    Total Loss:     26.2628\n",
      "    MSE (Raw):      20.718018\n",
      "    MSE (Weighted): 20.7180 (lambda=1.0)\n",
      "    NLL Component:  5.5358\n",
      "    Reg Loss:       0.008982\n",
      "    Variance stats: Min=1.00e-09, Max=6.28e+03, Mean=2.23e+01\n",
      "    Mean Error L1:  2.7895\n",
      "    Grad Norm:      44.9329 (Max abs grad: 15.0083)\n",
      "    Dropout probs:  p1=0.2827, p2=0.1197\n",
      "\n",
      "--- Validation at iteration 330 ---\n",
      "  Average MSE: 26.4599, Average ANEES: 10.0573\n",
      "--------------------------------------------------\n",
      "--- Iter [340/500] ---\n",
      "    Total Loss:     29.3572\n",
      "    MSE (Raw):      22.991066\n",
      "    MSE (Weighted): 22.9911 (lambda=1.0)\n",
      "    NLL Component:  6.3571\n",
      "    Reg Loss:       0.008980\n",
      "    Variance stats: Min=1.00e-09, Max=2.05e+04, Mean=2.93e+01\n",
      "    Mean Error L1:  2.8534\n",
      "    Grad Norm:      58.5192 (Max abs grad: 21.9446)\n",
      "    Dropout probs:  p1=0.2826, p2=0.1192\n",
      "\n",
      "--- Validation at iteration 340 ---\n",
      "  Average MSE: 23.5235, Average ANEES: 9.3247\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.5602 -> New: 9.3247) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [350/500] ---\n",
      "    Total Loss:     33.1271\n",
      "    MSE (Raw):      17.710001\n",
      "    MSE (Weighted): 17.7100 (lambda=1.0)\n",
      "    NLL Component:  15.4081\n",
      "    Reg Loss:       0.008975\n",
      "    Variance stats: Min=1.00e-09, Max=1.31e+04, Mean=2.34e+01\n",
      "    Mean Error L1:  2.5859\n",
      "    Grad Norm:      39.5054 (Max abs grad: 17.8496)\n",
      "    Dropout probs:  p1=0.2823, p2=0.1187\n",
      "\n",
      "--- Validation at iteration 350 ---\n",
      "  Average MSE: 17.0909, Average ANEES: 10.1621\n",
      "--------------------------------------------------\n",
      "--- Iter [360/500] ---\n",
      "    Total Loss:     21.8250\n",
      "    MSE (Raw):      13.512784\n",
      "    MSE (Weighted): 13.5128 (lambda=1.0)\n",
      "    NLL Component:  8.3033\n",
      "    Reg Loss:       0.008970\n",
      "    Variance stats: Min=1.00e-09, Max=1.35e+03, Mean=1.75e+01\n",
      "    Mean Error L1:  2.4349\n",
      "    Grad Norm:      45.0607 (Max abs grad: 17.3622)\n",
      "    Dropout probs:  p1=0.2819, p2=0.1186\n",
      "\n",
      "--- Validation at iteration 360 ---\n",
      "  Average MSE: 18.1855, Average ANEES: 9.7612\n",
      "--------------------------------------------------\n",
      "--- Iter [370/500] ---\n",
      "    Total Loss:     26.9808\n",
      "    MSE (Raw):      21.198046\n",
      "    MSE (Weighted): 21.1980 (lambda=1.0)\n",
      "    NLL Component:  5.7738\n",
      "    Reg Loss:       0.008972\n",
      "    Variance stats: Min=1.00e-09, Max=8.31e+03, Mean=2.23e+01\n",
      "    Mean Error L1:  2.7962\n",
      "    Grad Norm:      57.5002 (Max abs grad: 16.8247)\n",
      "    Dropout probs:  p1=0.2820, p2=0.1186\n",
      "\n",
      "--- Validation at iteration 370 ---\n",
      "  Average MSE: 16.5152, Average ANEES: 9.6721\n",
      "--------------------------------------------------\n",
      "--- Iter [380/500] ---\n",
      "    Total Loss:     22.5010\n",
      "    MSE (Raw):      17.383677\n",
      "    MSE (Weighted): 17.3837 (lambda=1.0)\n",
      "    NLL Component:  5.1084\n",
      "    Reg Loss:       0.008971\n",
      "    Variance stats: Min=1.00e-09, Max=7.62e+03, Mean=2.24e+01\n",
      "    Mean Error L1:  2.6750\n",
      "    Grad Norm:      58.5481 (Max abs grad: 20.2951)\n",
      "    Dropout probs:  p1=0.2819, p2=0.1188\n",
      "\n",
      "--- Validation at iteration 380 ---\n",
      "  Average MSE: 22.1645, Average ANEES: 9.5267\n",
      "--------------------------------------------------\n",
      "--- Iter [390/500] ---\n",
      "    Total Loss:     31.9491\n",
      "    MSE (Raw):      19.818563\n",
      "    MSE (Weighted): 19.8186 (lambda=1.0)\n",
      "    NLL Component:  12.1215\n",
      "    Reg Loss:       0.008969\n",
      "    Variance stats: Min=1.00e-09, Max=1.98e+04, Mean=3.18e+01\n",
      "    Mean Error L1:  2.6683\n",
      "    Grad Norm:      84.4182 (Max abs grad: 46.6926)\n",
      "    Dropout probs:  p1=0.2816, p2=0.1188\n",
      "\n",
      "--- Validation at iteration 390 ---\n",
      "  Average MSE: 25.0591, Average ANEES: 9.6053\n",
      "--------------------------------------------------\n",
      "--- Iter [400/500] ---\n",
      "    Total Loss:     23.0911\n",
      "    MSE (Raw):      18.494526\n",
      "    MSE (Weighted): 18.4945 (lambda=1.0)\n",
      "    NLL Component:  4.5876\n",
      "    Reg Loss:       0.008964\n",
      "    Variance stats: Min=1.00e-09, Max=9.38e+03, Mean=2.11e+01\n",
      "    Mean Error L1:  2.7053\n",
      "    Grad Norm:      52.1430 (Max abs grad: 19.0372)\n",
      "    Dropout probs:  p1=0.2811, p2=0.1187\n",
      "\n",
      "--- Validation at iteration 400 ---\n",
      "  Average MSE: 22.5583, Average ANEES: 9.6334\n",
      "--------------------------------------------------\n",
      "--- Iter [410/500] ---\n",
      "    Total Loss:     32.1093\n",
      "    MSE (Raw):      18.208689\n",
      "    MSE (Weighted): 18.2087 (lambda=1.0)\n",
      "    NLL Component:  13.8917\n",
      "    Reg Loss:       0.008958\n",
      "    Variance stats: Min=1.00e-09, Max=1.01e+04, Mean=2.32e+01\n",
      "    Mean Error L1:  2.6258\n",
      "    Grad Norm:      37.1803 (Max abs grad: 14.3606)\n",
      "    Dropout probs:  p1=0.2807, p2=0.1187\n",
      "\n",
      "--- Validation at iteration 410 ---\n",
      "  Average MSE: 22.8836, Average ANEES: 9.1134\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.3247 -> New: 9.1134) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [420/500] ---\n",
      "    Total Loss:     21.3897\n",
      "    MSE (Raw):      15.553471\n",
      "    MSE (Weighted): 15.5535 (lambda=1.0)\n",
      "    NLL Component:  5.8272\n",
      "    Reg Loss:       0.008952\n",
      "    Variance stats: Min=1.00e-09, Max=6.86e+03, Mean=2.09e+01\n",
      "    Mean Error L1:  2.5801\n",
      "    Grad Norm:      50.2845 (Max abs grad: 24.5140)\n",
      "    Dropout probs:  p1=0.2802, p2=0.1187\n",
      "\n",
      "--- Validation at iteration 420 ---\n",
      "  Average MSE: 21.3395, Average ANEES: 9.7079\n",
      "--------------------------------------------------\n",
      "--- Iter [430/500] ---\n",
      "    Total Loss:     33.0173\n",
      "    MSE (Raw):      20.524857\n",
      "    MSE (Weighted): 20.5249 (lambda=1.0)\n",
      "    NLL Component:  12.4835\n",
      "    Reg Loss:       0.008947\n",
      "    Variance stats: Min=1.00e-09, Max=1.62e+04, Mean=3.03e+01\n",
      "    Mean Error L1:  2.5461\n",
      "    Grad Norm:      40.9268 (Max abs grad: 17.0445)\n",
      "    Dropout probs:  p1=0.2798, p2=0.1186\n",
      "\n",
      "--- Validation at iteration 430 ---\n",
      "  Average MSE: 22.9758, Average ANEES: 10.6405\n",
      "--------------------------------------------------\n",
      "--- Iter [440/500] ---\n",
      "    Total Loss:     23.1760\n",
      "    MSE (Raw):      13.986617\n",
      "    MSE (Weighted): 13.9866 (lambda=1.0)\n",
      "    NLL Component:  9.1804\n",
      "    Reg Loss:       0.008940\n",
      "    Variance stats: Min=1.00e-09, Max=7.63e+02, Mean=1.82e+01\n",
      "    Mean Error L1:  2.4987\n",
      "    Grad Norm:      160.0181 (Max abs grad: 67.6636)\n",
      "    Dropout probs:  p1=0.2793, p2=0.1184\n",
      "\n",
      "--- Validation at iteration 440 ---\n",
      "  Average MSE: 26.9428, Average ANEES: 9.3413\n",
      "--------------------------------------------------\n",
      "--- Iter [450/500] ---\n",
      "    Total Loss:     20.5987\n",
      "    MSE (Raw):      15.082377\n",
      "    MSE (Weighted): 15.0824 (lambda=1.0)\n",
      "    NLL Component:  5.5074\n",
      "    Reg Loss:       0.008932\n",
      "    Variance stats: Min=1.00e-09, Max=2.81e+03, Mean=2.05e+01\n",
      "    Mean Error L1:  2.5627\n",
      "    Grad Norm:      57.0011 (Max abs grad: 22.2191)\n",
      "    Dropout probs:  p1=0.2785, p2=0.1182\n",
      "\n",
      "--- Validation at iteration 450 ---\n",
      "  Average MSE: 20.5267, Average ANEES: 9.0480\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.1134 -> New: 9.0480) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [460/500] ---\n",
      "    Total Loss:     26.7473\n",
      "    MSE (Raw):      19.531742\n",
      "    MSE (Weighted): 19.5317 (lambda=1.0)\n",
      "    NLL Component:  7.2067\n",
      "    Reg Loss:       0.008921\n",
      "    Variance stats: Min=1.00e-09, Max=2.51e+04, Mean=3.13e+01\n",
      "    Mean Error L1:  2.6107\n",
      "    Grad Norm:      49.6742 (Max abs grad: 16.6104)\n",
      "    Dropout probs:  p1=0.2774, p2=0.1182\n",
      "\n",
      "--- Validation at iteration 460 ---\n",
      "  Average MSE: 18.1993, Average ANEES: 9.2194\n",
      "--------------------------------------------------\n",
      "--- Iter [470/500] ---\n",
      "    Total Loss:     23.7755\n",
      "    MSE (Raw):      17.702671\n",
      "    MSE (Weighted): 17.7027 (lambda=1.0)\n",
      "    NLL Component:  6.0639\n",
      "    Reg Loss:       0.008909\n",
      "    Variance stats: Min=1.00e-09, Max=1.97e+04, Mean=2.40e+01\n",
      "    Mean Error L1:  2.6451\n",
      "    Grad Norm:      67.7969 (Max abs grad: 27.3785)\n",
      "    Dropout probs:  p1=0.2764, p2=0.1182\n",
      "\n",
      "--- Validation at iteration 470 ---\n",
      "  Average MSE: 24.2685, Average ANEES: 9.9593\n",
      "--------------------------------------------------\n",
      "--- Iter [480/500] ---\n",
      "    Total Loss:     25.3691\n",
      "    MSE (Raw):      18.903133\n",
      "    MSE (Weighted): 18.9031 (lambda=1.0)\n",
      "    NLL Component:  6.4571\n",
      "    Reg Loss:       0.008903\n",
      "    Variance stats: Min=1.00e-09, Max=2.61e+04, Mean=2.89e+01\n",
      "    Mean Error L1:  2.4570\n",
      "    Grad Norm:      60.4844 (Max abs grad: 23.0159)\n",
      "    Dropout probs:  p1=0.2760, p2=0.1178\n",
      "\n",
      "--- Validation at iteration 480 ---\n",
      "  Average MSE: 18.7899, Average ANEES: 9.4537\n",
      "--------------------------------------------------\n",
      "--- Iter [490/500] ---\n",
      "    Total Loss:     22.1743\n",
      "    MSE (Raw):      13.324931\n",
      "    MSE (Weighted): 13.3249 (lambda=1.0)\n",
      "    NLL Component:  8.8404\n",
      "    Reg Loss:       0.008900\n",
      "    Variance stats: Min=1.00e-09, Max=1.53e+03, Mean=1.91e+01\n",
      "    Mean Error L1:  2.3981\n",
      "    Grad Norm:      67.1175 (Max abs grad: 32.6689)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 490 ---\n",
      "  Average MSE: 19.1624, Average ANEES: 8.9400\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 9.0480 -> New: 8.9400) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [500/500] ---\n",
      "    Total Loss:     28.8792\n",
      "    MSE (Raw):      18.941101\n",
      "    MSE (Weighted): 18.9411 (lambda=1.0)\n",
      "    NLL Component:  9.9292\n",
      "    Reg Loss:       0.008897\n",
      "    Variance stats: Min=1.00e-09, Max=1.87e+04, Mean=2.60e+01\n",
      "    Mean Error L1:  2.5152\n",
      "    Grad Norm:      106.8934 (Max abs grad: 45.2787)\n",
      "    Dropout probs:  p1=0.2756, p2=0.1170\n",
      "\n",
      "--- Validation at iteration 500 ---\n",
      "  Average MSE: 21.9840, Average ANEES: 9.6006\n",
      "--------------------------------------------------\n",
      "\n",
      "Training completed.\n",
      "Loading best model from iteration 490 with ANEES 8.9400\n",
      "‚úÖ F√°ze 1 dokonƒçena. Model ulo≈æen do: bkn_curriculum_phase1_len10.pth\n",
      "\n",
      "============================================================\n",
      "üöÄ START PHASE 2: SeqLen 100 | LR 1e-05 | TBPTT: False\n",
      "============================================================\n",
      "   -> Using Standard Hybrid Training\n",
      "üöÄ START Hybrid Training: Loss = NLL + 1.0 * MSE\n",
      "    Logging period: 10 iterations\n",
      "--- Iter [10/1000] ---\n",
      "    Total Loss:     1205.5852\n",
      "    MSE (Raw):      1177.570557\n",
      "    MSE (Weighted): 1177.5706 (lambda=1.0)\n",
      "    NLL Component:  2.9218\n",
      "    Reg Loss:       0.097896\n",
      "    Variance stats: Min=1.00e-09, Max=2.90e+06, Mean=2.60e+03\n",
      "    Mean Error L1:  8.1855\n",
      "    Grad Norm:      53.5902 (Max abs grad: 21.1119)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 10 ---\n",
      "  Average MSE: 3155.5788, Average ANEES: 7.8422\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: inf -> New: 7.8422) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [20/1000] ---\n",
      "    Total Loss:     787.9039\n",
      "    MSE (Raw):      755.245056\n",
      "    MSE (Weighted): 755.2451 (lambda=1.0)\n",
      "    NLL Component:  3.4825\n",
      "    Reg Loss:       0.097896\n",
      "    Variance stats: Min=1.00e-09, Max=2.14e+06, Mean=3.01e+03\n",
      "    Mean Error L1:  8.4226\n",
      "    Grad Norm:      91.3604 (Max abs grad: 32.2248)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 20 ---\n",
      "  Average MSE: 3608.1172, Average ANEES: 8.1786\n",
      "--------------------------------------------------\n",
      "--- Iter [30/1000] ---\n",
      "    Total Loss:     657.3945\n",
      "    MSE (Raw):      630.735657\n",
      "    MSE (Weighted): 630.7357 (lambda=1.0)\n",
      "    NLL Component:  2.7136\n",
      "    Reg Loss:       0.097895\n",
      "    Variance stats: Min=1.00e-09, Max=2.17e+06, Mean=2.48e+03\n",
      "    Mean Error L1:  7.2037\n",
      "    Grad Norm:      34.3481 (Max abs grad: 8.3270)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 30 ---\n",
      "  Average MSE: 3674.2281, Average ANEES: 8.1004\n",
      "--------------------------------------------------\n",
      "--- Iter [40/1000] ---\n",
      "    Total Loss:     172.0704\n",
      "    MSE (Raw):      166.351471\n",
      "    MSE (Weighted): 166.3515 (lambda=1.0)\n",
      "    NLL Component:  2.7755\n",
      "    Reg Loss:       0.097895\n",
      "    Variance stats: Min=1.00e-09, Max=4.31e+05, Mean=3.85e+02\n",
      "    Mean Error L1:  6.3395\n",
      "    Grad Norm:      67.5741 (Max abs grad: 23.2788)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 40 ---\n",
      "  Average MSE: 1542.9771, Average ANEES: 7.9791\n",
      "--------------------------------------------------\n",
      "--- Iter [50/1000] ---\n",
      "    Total Loss:     4589.8579\n",
      "    MSE (Raw):      4539.935547\n",
      "    MSE (Weighted): 4539.9355 (lambda=1.0)\n",
      "    NLL Component:  2.6830\n",
      "    Reg Loss:       0.097894\n",
      "    Variance stats: Min=1.00e-09, Max=2.70e+06, Mean=4.81e+03\n",
      "    Mean Error L1:  10.3186\n",
      "    Grad Norm:      15.6866 (Max abs grad: 6.5243)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 50 ---\n",
      "  Average MSE: 3507.5448, Average ANEES: 8.0540\n",
      "--------------------------------------------------\n",
      "--- Iter [60/1000] ---\n",
      "    Total Loss:     3002.6565\n",
      "    MSE (Raw):      2867.917236\n",
      "    MSE (Weighted): 2867.9172 (lambda=1.0)\n",
      "    NLL Component:  2.6305\n",
      "    Reg Loss:       0.097893\n",
      "    Variance stats: Min=1.00e-09, Max=4.31e+06, Mean=1.33e+04\n",
      "    Mean Error L1:  11.7030\n",
      "    Grad Norm:      15.3572 (Max abs grad: 5.5457)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 60 ---\n",
      "  Average MSE: 999.4472, Average ANEES: 8.0129\n",
      "--------------------------------------------------\n",
      "--- Iter [70/1000] ---\n",
      "    Total Loss:     1475.6189\n",
      "    MSE (Raw):      1442.318848\n",
      "    MSE (Weighted): 1442.3188 (lambda=1.0)\n",
      "    NLL Component:  2.8841\n",
      "    Reg Loss:       0.097893\n",
      "    Variance stats: Min=1.00e-09, Max=1.31e+06, Mean=3.13e+03\n",
      "    Mean Error L1:  9.1610\n",
      "    Grad Norm:      93.8359 (Max abs grad: 45.6297)\n",
      "    Dropout probs:  p1=0.2758, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 70 ---\n",
      "  Average MSE: 2273.0675, Average ANEES: 7.7193\n",
      "  >>> New best VALIDATION ANEES! Saving model. (Old: 7.8422 -> New: 7.7193) <<<\n",
      "--------------------------------------------------\n",
      "--- Iter [80/1000] ---\n",
      "    Total Loss:     9036.1416\n",
      "    MSE (Raw):      8956.829102\n",
      "    MSE (Weighted): 8956.8291 (lambda=1.0)\n",
      "    NLL Component:  7.4585\n",
      "    Reg Loss:       0.097892\n",
      "    Variance stats: Min=1.00e-09, Max=3.59e+06, Mean=7.28e+03\n",
      "    Mean Error L1:  14.3013\n",
      "    Grad Norm:      12.3599 (Max abs grad: 3.4259)\n",
      "    Dropout probs:  p1=0.2757, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 80 ---\n",
      "  Average MSE: 1243.7019, Average ANEES: 7.7591\n",
      "--------------------------------------------------\n",
      "--- Iter [90/1000] ---\n",
      "    Total Loss:     293.6586\n",
      "    MSE (Raw):      282.157501\n",
      "    MSE (Weighted): 282.1575 (lambda=1.0)\n",
      "    NLL Component:  2.8577\n",
      "    Reg Loss:       0.097892\n",
      "    Variance stats: Min=1.00e-09, Max=8.19e+05, Mean=9.55e+02\n",
      "    Mean Error L1:  6.8782\n",
      "    Grad Norm:      274.0990 (Max abs grad: 109.0690)\n",
      "    Dropout probs:  p1=0.2757, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 90 ---\n",
      "  Average MSE: 2557.2306, Average ANEES: 7.7661\n",
      "--------------------------------------------------\n",
      "--- Iter [100/1000] ---\n",
      "    Total Loss:     6771.3188\n",
      "    MSE (Raw):      6657.096191\n",
      "    MSE (Weighted): 6657.0962 (lambda=1.0)\n",
      "    NLL Component:  2.7862\n",
      "    Reg Loss:       0.097891\n",
      "    Variance stats: Min=1.00e-09, Max=4.32e+06, Mean=1.12e+04\n",
      "    Mean Error L1:  14.1384\n",
      "    Grad Norm:      34.9078 (Max abs grad: 11.5888)\n",
      "    Dropout probs:  p1=0.2757, p2=0.1174\n",
      "\n",
      "--- Validation at iteration 100 ---\n",
      "  Average MSE: 4206.1133, Average ANEES: 7.7531\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "# P≈ôedpokl√°d√°m, ≈æe funkce train_BayesianKalmanNet_TBPTT je definov√°na (z p≈ôedchoz√≠ho kroku)\n",
    "# from utils import train_BayesianKalmanNet_TBPTT  <-- pokud ji m√°te v souboru\n",
    "\n",
    "# --- 1. DEFINICE CURRICULA ---\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Stabilizace (Kr√°tk√© sekvence - staƒç√≠ standardn√≠ tr√©nink)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,\n",
    "        'iters': 500,\n",
    "        'lr': 1e-3,\n",
    "        'lambda_mse': 1.0,\n",
    "        'clip_grad': 1.0,\n",
    "        'use_tbptt': False,   # Tady TBPTT nepot≈ôebujeme\n",
    "        'tbptt_steps': None\n",
    "    },\n",
    "    # F√ÅZE 2: Prodlou≈æen√≠ (St≈ôedn√≠ sekvence - ZAP√çN√ÅME TBPTT)\n",
    "    # TBPTT n√°m umo≈æn√≠ tr√©novat na d√©lce 100, ani≈æ by explodovala variance\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100,\n",
    "        'iters': 1000,        # V√≠ce iterac√≠, proto≈æe TBPTT dƒõl√° men≈°√≠ kroky\n",
    "        'lr': 1e-5,           # Opatrn√© uƒçen√≠\n",
    "        'lambda_mse': 1.0, # St√°le dr≈æ√≠me prioritu na p≈ôesnost\n",
    "        'clip_grad': 0.25,\n",
    "        'use_tbptt': False,    # <--- ZAPNUTO\n",
    "        'tbptt_steps': 6     # Uƒç√≠me se na oknech d√©lky 20\n",
    "    },\n",
    "    # {\n",
    "    #     'phase_id': 3,\n",
    "    #     'seq_len': 300,\n",
    "    #     'iters': 1000,        # V√≠ce iterac√≠, proto≈æe TBPTT dƒõl√° men≈°√≠ kroky\n",
    "    #     'lr': 1e-6,           # Opatrn√© uƒçen√≠\n",
    "    #     'lambda_mse': 100.0, # St√°le dr≈æ√≠me prioritu na p≈ôesnost\n",
    "    #     'clip_grad': 0.01,\n",
    "    #     'use_tbptt': True,    # <--- ZAPNUTO\n",
    "    #     'tbptt_steps': 6     # Uƒç√≠me se na oknech d√©lky 20\n",
    "    # }\n",
    "]\n",
    "\n",
    "# --- 2. INICIALIZACE MODELU ---\n",
    "print(\"=== INICIALIZACE BKN MODELU ===\")\n",
    "# Pou≈æ√≠v√°me v√°≈° osvƒõdƒçen√Ω setup s vy≈°≈°√≠m hidden size\n",
    "state_knet2 = TAN.StateBayesianKalmanNetTAN(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=6,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        init_max_dropout=0.3, # Zdrav√Ω dropout pro exploration\n",
    "        init_min_dropout=0.1    \n",
    ").to(device)\n",
    "\n",
    "# --- 3. CURRICULUM LOOP ---\n",
    "for phase in curriculum_schedule:\n",
    "    phase_id = phase['phase_id']\n",
    "    seq_len = phase['seq_len']\n",
    "\n",
    "    # Kontrola dostupnosti dat\n",
    "    if phase_id not in datasets_cache:\n",
    "        print(f\"‚ö†Ô∏è Skipping Phase {phase_id}: Data not in cache.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üöÄ START PHASE {phase_id}: SeqLen {seq_len} | LR {phase['lr']} | TBPTT: {phase['use_tbptt']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_loader_phase = datasets_cache[phase_id][0]\n",
    "    val_loader_phase = datasets_cache[phase_id][1]\n",
    "    \n",
    "    # Rozhodov√°n√≠ podle typu tr√©ninku\n",
    "    if phase['use_tbptt']:\n",
    "        # === A) TBPTT Tr√©nink (pro dlouh√© sekvence) ===\n",
    "        print(f\"   -> Using TBPTT with window size {phase['tbptt_steps']}\")\n",
    "        \n",
    "        # Zkontrolujeme, zda model m√° metodu detach_hidden (prevence p√°du)\n",
    "        if not hasattr(state_knet2, 'detach_hidden'):\n",
    "            raise AttributeError(\"Modelu chyb√≠ metoda 'detach_hidden()'! P≈ôidejte ji do t≈ô√≠dy StateBayesianKalmanNetTAN.\")\n",
    "\n",
    "        result = train_BayesianKalmanNet_TBPTT(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=5,               # V TBPTT m≈Ø≈æeme m√≠t men≈°√≠ J, proto≈æe se pr≈Ømƒõruje ƒçastƒõji\n",
    "            tbptt_steps=phase['tbptt_steps'],\n",
    "            validation_period=50,       # Validace ka≈æd√Ωch 50 oken\n",
    "            logging_period=10,\n",
    "            lambda_mse=phase['lambda_mse']\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # === B) Standardn√≠ Hybrid Tr√©nink (pro kr√°tk√© sekvence) ===\n",
    "        print(f\"   -> Using Standard Hybrid Training\")\n",
    "        result = train_BayesianKalmanNet_Hybrid(\n",
    "            model=state_knet2,\n",
    "            train_loader=train_loader_phase,\n",
    "            val_loader=val_loader_phase,\n",
    "            device=device,\n",
    "            total_train_iter=phase['iters'],\n",
    "            learning_rate=phase['lr'],\n",
    "            clip_grad=phase['clip_grad'],\n",
    "            J_samples=5,\n",
    "            validation_period=10,\n",
    "            logging_period=10,\n",
    "            warmup_iterations=0,\n",
    "            weight_decay_=1e-5,\n",
    "            lambda_mse=phase['lambda_mse']\n",
    "        )\n",
    "    \n",
    "    # Ulo≈æen√≠ a kontrola\n",
    "    save_path = f\"bkn_curriculum_phase{phase_id}_len{seq_len}.pth\"\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"‚úÖ F√°ze {phase_id} dokonƒçena. Model ulo≈æen do: {save_path}\")\n",
    "    \n",
    "    # Jednoduch√° kontrola proti divergenci (pokud best_val neexistuje nebo je inf)\n",
    "    # Pozn: TBPTT funkce vrac√≠ slovn√≠k, hybrid taky, ujist√≠me se, ≈æe kl√≠ƒçe sed√≠\n",
    "    # TBPTT moment√°lnƒõ vrac√≠ jen {'final_model': model}, pro pokroƒçilou kontrolu by to chtƒõlo vr√°tit i loss.\n",
    "    # Ale pro teƒè to nech√°me bƒõ≈æet d√°l.\n",
    "\n",
    "print(\"\\nüéâ Cel√Ω tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'most_consistent_and_accurate_bknet.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ec4ec",
   "metadata": {},
   "source": [
    "# Test na realne trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d79a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === KONFIGURACE MC ===\n",
    "MC_ITERATIONS = 2  \n",
    "J_SAMPLES = 75      \n",
    "PLOT_PER_ITERATION = True  # True = vykresl√≠ grafy (prvn√≠ a posledn√≠ bƒõh), False = ≈æ√°dn√© grafy\n",
    "\n",
    "# P≈ô√≠prava dat\n",
    "real_traj_np = souradniceGNSS[:2, :].T \n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "\n",
    "# --- POMOCN√Å FUNKCE ---\n",
    "def get_reference_test_set(system, real_traj_tensor):\n",
    "    device = system.Ex0.device\n",
    "    hB_np = mat_data['hB']\n",
    "    real_hB_tensor = torch.from_numpy(hB_np).float().to(device).view(-1)\n",
    "\n",
    "    pos_full = real_traj_tensor.clone().to(device)\n",
    "    deltas = pos_full[1:] - pos_full[:-1] \n",
    "    last_vel = deltas[-1:]\n",
    "    velocities = torch.cat([deltas, last_vel], dim=0) \n",
    "    \n",
    "    x_traj_flat = torch.cat([pos_full, velocities], dim=1) \n",
    "    y_traj_flat = system.measure(x_traj_flat) \n",
    "    \n",
    "    seq_len = x_traj_flat.shape[0]\n",
    "    y_traj_flat[:, 0] = real_hB_tensor[:seq_len] \n",
    "    \n",
    "    x_ref = x_traj_flat.unsqueeze(0) \n",
    "    y_ref = y_traj_flat.unsqueeze(0) \n",
    "    \n",
    "    return x_ref, y_ref\n",
    "\n",
    "print(f\"=== SPU≈†TƒöN√ç MONTE CARLO SIMULACE ({MC_ITERATIONS} bƒõh≈Ø) ===\")\n",
    "print(f\"Modely: BKN (J={J_SAMPLES}) vs. UKF vs. PF\")\n",
    "\n",
    "# 1. P≈ô√≠prava Ground Truth\n",
    "x_ref_tensor_static, _ = get_reference_test_set(system_model, real_traj_tensor)\n",
    "x_gt = x_ref_tensor_static.squeeze().cpu().numpy()\n",
    "seq_len = x_gt.shape[0]\n",
    "\n",
    "# 2. Inicializace metrik\n",
    "detailed_results = []\n",
    "agg_metrics = {\n",
    "    \"BKN\": {\"mse\": [], \"pos\": []}, \n",
    "    \"UKF\": {\"mse\": [], \"pos\": []}, \n",
    "    \"PF\":  {\"mse\": [], \"pos\": []}\n",
    "}\n",
    "\n",
    "state_knet2.train() \n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA ---\n",
    "for i in tqdm(range(MC_ITERATIONS), desc=\"Simulace\"):\n",
    "    \n",
    "    # A) Nov√© mƒõ≈ôen√≠ (n√°hodn√Ω ≈°um)\n",
    "    _, y_ref_tensor = get_reference_test_set(system_model, real_traj_tensor)\n",
    "    y_meas = y_ref_tensor \n",
    "    \n",
    "    # B) Inference: BKN Ensemble\n",
    "    with torch.no_grad():\n",
    "        initial_state = x_ref_tensor_static[:, 0, :] \n",
    "        ensemble_preds = []\n",
    "        \n",
    "        for j in range(J_SAMPLES):\n",
    "            state_knet2.reset(batch_size=1, initial_state=initial_state)\n",
    "            trajectory_preds = []\n",
    "            for t in range(1, seq_len):\n",
    "                y_t = y_meas[:, t, :]\n",
    "                x_est, _ = state_knet2.step(y_t)\n",
    "                trajectory_preds.append(x_est)\n",
    "            \n",
    "            trajectory_tensor = torch.stack(trajectory_preds, dim=1) \n",
    "            full_traj = torch.cat([initial_state.unsqueeze(1), trajectory_tensor], dim=1)\n",
    "            ensemble_preds.append(full_traj)\n",
    "            \n",
    "        ensemble_stack = torch.stack(ensemble_preds, dim=0) \n",
    "        x_est_bkn_tensor = ensemble_stack.mean(dim=0).squeeze(0) \n",
    "        cov_diag_bkn = ensemble_stack.var(dim=0).squeeze(0) + 1e-9 \n",
    "        \n",
    "        x_est_bkn = x_est_bkn_tensor.cpu().numpy()\n",
    "        std_bkn = torch.sqrt(cov_diag_bkn).cpu().numpy() \n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    y_for_filters = y_ref_tensor.squeeze(0) \n",
    "    true_init_state = x_ref_tensor_static[0, 0, :] \n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(y_seq=y_for_filters, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=5000) \n",
    "    pf_res = pf.process_sequence(y_seq=y_for_filters, Ex0=true_init_state, P0=system_model.P0)\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # D) V√Ωpoƒçet chyb\n",
    "    def calc_errors(x_est, x_gt):\n",
    "        diff = x_est - x_gt\n",
    "        mse = np.mean(diff**2)\n",
    "        pos_err = np.mean(np.sqrt(diff[:, 0]**2 + diff[:, 1]**2))\n",
    "        return mse, pos_err\n",
    "\n",
    "    mse_bkn, pos_bkn = calc_errors(x_est_bkn, x_gt)\n",
    "    mse_ukf, pos_ukf = calc_errors(x_est_ukf, x_gt)\n",
    "    mse_pf, pos_pf = calc_errors(x_est_pf, x_gt)\n",
    "    \n",
    "    agg_metrics[\"BKN\"][\"mse\"].append(mse_bkn); agg_metrics[\"BKN\"][\"pos\"].append(pos_bkn)\n",
    "    agg_metrics[\"UKF\"][\"mse\"].append(mse_ukf); agg_metrics[\"UKF\"][\"pos\"].append(pos_ukf)\n",
    "    agg_metrics[\"PF\"][\"mse\"].append(mse_pf);   agg_metrics[\"PF\"][\"pos\"].append(pos_pf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"BKN_MSE\": mse_bkn, \"UKF_MSE\": mse_ukf, \"PF_MSE\": mse_pf,\n",
    "        \"BKN_Pos\": pos_bkn, \"UKF_Pos\": pos_ukf, \"PF_Pos\": pos_pf\n",
    "    })\n",
    "    \n",
    "    # E) VYKRESLEN√ç (Upraveno pro X i Y s neurƒçitost√≠)\n",
    "    # Vykresl√≠me jen prvn√≠ a posledn√≠ bƒõh, abychom nezahltili v√Ωstup, pokud je PLOT_PER_ITERATION True\n",
    "    if PLOT_PER_ITERATION and (i == 0 or i == MC_ITERATIONS-1):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(24, 6)) # 3 grafy vedle sebe\n",
    "        \n",
    "        # 1. Graf: Trajektorie v mapƒõ (XY)\n",
    "        ax[0].plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='GT')\n",
    "        ax[0].plot(x_est_bkn[:, 0], x_est_bkn[:, 1], 'g-', linewidth=2, label=f'BKN (Err: {pos_bkn:.1f}m)')\n",
    "        ax[0].plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_ukf:.1f}m)')\n",
    "        ax[0].set_title(f\"Run {i+1}: Map Trajectory (XY)\")\n",
    "        ax[0].set_xlabel(\"X [m]\")\n",
    "        ax[0].set_ylabel(\"Y [m]\")\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "        ax[0].axis('equal')\n",
    "        \n",
    "        # Spoleƒçn√° ƒçasov√° osa\n",
    "        time_steps = np.arange(seq_len)\n",
    "\n",
    "        # 2. Graf: V√Ωvoj X v ƒçase s neurƒçitost√≠\n",
    "        ax[1].plot(time_steps, x_gt[:, 0], 'k-', label='GT X')\n",
    "        ax[1].plot(time_steps, x_est_bkn[:, 0], 'g-', label='BKN X')\n",
    "        # P√°smo neurƒçitosti X (+/- 3 sigma)\n",
    "        ax[1].fill_between(time_steps, \n",
    "                           x_est_bkn[:, 0] - 3*std_bkn[:, 0], \n",
    "                           x_est_bkn[:, 0] + 3*std_bkn[:, 0], \n",
    "                           color='green', alpha=0.2, label='Uncertainty (3$\\sigma$)')\n",
    "        ax[1].set_title(\"BKN Uncertainty: X-axis\")\n",
    "        ax[1].set_ylabel(\"Position X [m]\")\n",
    "        ax[1].set_xlabel(\"Time Step\")\n",
    "        ax[1].legend()\n",
    "        ax[1].grid(True)\n",
    "\n",
    "        # 3. Graf: V√Ωvoj Y v ƒçase s neurƒçitost√≠\n",
    "        ax[2].plot(time_steps, x_gt[:, 1], 'k-', label='GT Y')\n",
    "        ax[2].plot(time_steps, x_est_bkn[:, 1], 'g-', label='BKN Y')\n",
    "        # P√°smo neurƒçitosti Y (+/- 3 sigma)\n",
    "        ax[2].fill_between(time_steps, \n",
    "                           x_est_bkn[:, 1] - 3*std_bkn[:, 1], \n",
    "                           x_est_bkn[:, 1] + 3*std_bkn[:, 1], \n",
    "                           color='green', alpha=0.2, label='Uncertainty (3$\\sigma$)')\n",
    "        ax[2].set_title(\"BKN Uncertainty: Y-axis\")\n",
    "        ax[2].set_ylabel(\"Position Y [m]\")\n",
    "        ax[2].set_xlabel(\"Time Step\")\n",
    "        ax[2].legend()\n",
    "        ax[2].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({MC_ITERATIONS} bƒõh≈Ø)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats_str(key):\n",
    "    m_mse, s_mse = np.mean(agg_metrics[key][\"mse\"]), np.std(agg_metrics[key][\"mse\"])\n",
    "    m_pos, s_pos = np.mean(agg_metrics[key][\"pos\"]), np.std(agg_metrics[key][\"pos\"])\n",
    "    return f\"{m_mse:.1f} ¬± {s_mse:.1f} | {m_pos:.2f} ¬± {s_pos:.2f} m\"\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE':<25} | {'Pos Error':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'BKN (Ensemble)':<15} | {get_stats_str('BKN')}\")\n",
    "print(f\"{'UKF':<15} | {get_stats_str('UKF')}\")\n",
    "print(f\"{'PF':<15} | {get_stats_str('PF')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_metrics[\"BKN\"][\"pos\"], agg_metrics[\"UKF\"][\"pos\"], agg_metrics[\"PF\"][\"pos\"]], \n",
    "            labels=['BKN', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({MC_ITERATIONS} runs)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Pro hezkou tabulku\n",
    "import Filters\n",
    "from tqdm import tqdm\n",
    "real_traj_np = souradniceGNSS[:2, :].T \n",
    "\n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "train_source_tensor = real_traj_tensor[:, :]\n",
    "# --- POMOCN√Å FUNKCE PRO GENEROW√ÅN√ç DAT ---\n",
    "def get_reference_test_set(system, real_traj_tensor, reverse=False):\n",
    "    # O≈ô√≠znut√≠ trajektorie (pokud je pot≈ôeba)\n",
    "    # real_traj_tensor = real_traj_tensor[:1050,:] \n",
    "    \n",
    "    device = system.Ex0.device\n",
    "    \n",
    "    # P≈ôedpoklad: mat_data je glob√°ln√≠ promƒõnn√° s naƒçten√Ωm .mat souborem\n",
    "    # hB_np = mat_data['hB']\n",
    "    # real_hB_tensor = torch.from_numpy(hB_np).float().to(device).view(-1)\n",
    "\n",
    "    pos_full = real_traj_tensor.clone().to(device)\n",
    "    \n",
    "    deltas = pos_full[1:] - pos_full[:-1] \n",
    "    last_vel = deltas[-1:]\n",
    "    velocities = torch.cat([deltas, last_vel], dim=0) # [T, 2]\n",
    "    \n",
    "    x_traj_flat = torch.cat([pos_full, velocities], dim=1) # [T, 4]\n",
    "    \n",
    "    # Generov√°n√≠ mƒõ≈ôen√≠ (s n√°hodn√Ωm ≈°umem uvnit≈ô system.measure)\n",
    "    y_traj_flat = system.measure(x_traj_flat) # [T, 3]\n",
    "    \n",
    "    # Nahrazen√≠ barometru re√°ln√Ωmi daty (pokud je to ≈æ√°douc√≠)\n",
    "    seq_len = x_traj_flat.shape[0]\n",
    "    # Pokud chce≈° simulovat ƒçistƒõ syntetick√Ω ≈°um barometru, tento ≈ô√°dek zakomentuj:\n",
    "    # y_traj_flat[:, 0] = real_hB_tensor[:seq_len] \n",
    "    \n",
    "    x_ref = x_traj_flat.unsqueeze(0) # [1, T, 4]\n",
    "    y_ref = y_traj_flat.unsqueeze(0) # [1, T, 3]\n",
    "    \n",
    "    return x_ref, y_ref\n",
    "\n",
    "\n",
    "# --- KONFIGURACE MC ---\n",
    "MC_ITERATIONS = 10  # Nastav rozumn√© ƒç√≠slo (pro 100 graf≈Ø by to zahltilo notebook)\n",
    "PLOT_PER_ITERATION = True # Zda vykreslovat grafy pro ka≈æd√Ω bƒõh\n",
    "\n",
    "print(f\"=== SPU≈†TƒöN√ç MONTE CARLO SIMULACE ({MC_ITERATIONS} bƒõh≈Ø) ===\")\n",
    "print(\"Modely: KalmanNet vs. UKF vs. PF\")\n",
    "\n",
    "# 1. P≈ô√≠prava Ground Truth (GT)\n",
    "real_traj_tensor = torch.from_numpy(real_traj_np).float().to(device)\n",
    "# Z√≠sk√°me GT stavy (X) jen jednou, proto≈æe trajektorie je fixn√≠\n",
    "# Mƒõ≈ôen√≠ (Y) se bude mƒõnit v ka≈æd√© iteraci kv≈Øli ≈°umu\n",
    "x_ref_tensor_static, _ = get_reference_test_set(system_model, real_traj_tensor)\n",
    "x_gt = x_ref_tensor_static.squeeze().cpu().numpy()\n",
    "seq_len = x_gt.shape[0]\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = [] # Seznam slovn√≠k≈Ø pro DataFrame\n",
    "agg_mse = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "agg_pos = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "\n",
    "# Ujist√≠me se, ≈æe KNet je v eval m√≥du\n",
    "state_knet2.eval()\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA ---\n",
    "for i in tqdm(range(MC_ITERATIONS), desc=\"Simulace\"):\n",
    "    \n",
    "    # A) Generov√°n√≠ nov√©ho mƒõ≈ôen√≠ (s nov√Ωm n√°hodn√Ωm ≈°umem)\n",
    "    # Vol√°me funkci znovu, abychom dostali Y s jinou realizac√≠ ≈°umu (pokud system.measure ≈°um√≠)\n",
    "    _, y_ref_tensor = get_reference_test_set(system_model, real_traj_tensor)\n",
    "    \n",
    "    # B) Inference: KalmanNet\n",
    "    with torch.no_grad():\n",
    "        initial_state = x_ref_tensor_static[:, 0, :] # [1, 4]\n",
    "        state_knet2.reset(batch_size=1, initial_state=initial_state)\n",
    "        \n",
    "        knet_preds = []\n",
    "        y_input = y_ref_tensor \n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input[:, t, :]\n",
    "            x_est = state_knet2.step(y_t)\n",
    "            knet_preds.append(x_est)\n",
    "            \n",
    "        knet_preds_tensor = torch.stack(knet_preds, dim=1)\n",
    "        full_knet_est = torch.cat([initial_state.unsqueeze(1), knet_preds_tensor], dim=1)\n",
    "        x_est_knet = full_knet_est.squeeze().cpu().numpy()\n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    y_for_filters = y_ref_tensor.squeeze(0) \n",
    "    \n",
    "    # !!! KL√çƒåOV√Å OPRAVA: Pou≈æijeme SKUTEƒåN√ù startovn√≠ bod trajektorie !!!\n",
    "    true_init_state = x_ref_tensor_static[0, 0, :] \n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(\n",
    "        y_seq=y_for_filters,\n",
    "        Ex0=true_init_state, # Spr√°vn√Ω start\n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=10000) # Poƒçet ƒç√°stic dle v√Ωkonu\n",
    "    pf_res = pf.process_sequence(\n",
    "        y_seq=y_for_filters,\n",
    "        Ex0=true_init_state, # Spr√°vn√Ω start\n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    \n",
    "    # D) V√Ωpoƒçet chyb pro tento bƒõh\n",
    "    # KNet\n",
    "    diff_knet = x_est_knet - x_gt\n",
    "    mse_knet = np.mean(diff_knet**2)\n",
    "    pos_err_knet = np.mean(np.sqrt(diff_knet[:, 0]**2 + diff_knet[:, 1]**2))\n",
    "    \n",
    "    # UKF\n",
    "    diff_ukf = x_est_ukf - x_gt\n",
    "    mse_ukf = np.mean(diff_ukf**2)\n",
    "    pos_err_ukf = np.mean(np.sqrt(diff_ukf[:, 0]**2 + diff_ukf[:, 1]**2))\n",
    "    \n",
    "    # PF\n",
    "    diff_pf = x_est_pf - x_gt\n",
    "    mse_pf = np.mean(diff_pf**2)\n",
    "    pos_err_pf = np.mean(np.sqrt(diff_pf[:, 0]**2 + diff_pf[:, 1]**2))\n",
    "    \n",
    "    # Ulo≈æen√≠ do agreg√°toru\n",
    "    agg_mse[\"KNet\"].append(mse_knet)\n",
    "    agg_pos[\"KNet\"].append(pos_err_knet)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf)\n",
    "    agg_pos[\"UKF\"].append(pos_err_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf)\n",
    "    agg_pos[\"PF\"].append(pos_err_pf)\n",
    "\n",
    "    # Ulo≈æen√≠ do detailn√≠ho seznamu\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"KNet_MSE\": mse_knet,\n",
    "        \"UKF_MSE\": mse_ukf,\n",
    "        \"PF_MSE\": mse_pf,\n",
    "        \"KNet_PosErr\": pos_err_knet,\n",
    "        \"UKF_PosErr\": pos_err_ukf,\n",
    "        \"PF_PosErr\": pos_err_pf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠ grafu pro TENTO bƒõh\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        \n",
    "        plt.plot(x_est_knet[:, 0], x_est_knet[:, 1], 'g-', linewidth=1.5, label=f'KalmanNet (MSE: {mse_knet:.1f})')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (MSE: {mse_ukf:.1f})')\n",
    "        plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.8, label=f'PF (MSE: {mse_pf:.1f})')\n",
    "        \n",
    "        plt.title(f\"Run {i+1}/{MC_ITERATIONS}: Trajectory Comparison\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "\n",
    "# 1. Detailn√≠ tabulka v≈°ech bƒõh≈Ø\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY PO JEDNOTLIV√ùCH BƒöZ√çCH\")\n",
    "print(\"=\"*80)\n",
    "# Form√°tov√°n√≠ tabulky pro hezƒç√≠ v√Ωpis\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"KNet_MSE\", \"UKF_MSE\", \"PF_MSE\", \"KNet_PosErr\", \"UKF_PosErr\", \"PF_PosErr\"]])\n",
    "\n",
    "# 2. Souhrnn√° statistika\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({MC_ITERATIONS} bƒõh≈Ø)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats(key):\n",
    "    return np.mean(agg_mse[key]), np.std(agg_mse[key]), np.mean(agg_pos[key]), np.std(agg_pos[key])\n",
    "\n",
    "knet_stats = get_stats(\"KNet\")\n",
    "ukf_stats = get_stats(\"UKF\")\n",
    "pf_stats = get_stats(\"PF\")\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE (Mean ¬± Std)':<25} | {'Pos Error (Mean ¬± Std)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'KalmanNet':<15} | {knet_stats[0]:.1f} ¬± {knet_stats[1]:.1f} | {knet_stats[2]:.2f} ¬± {knet_stats[3]:.2f} m\")\n",
    "print(f\"{'UKF':<15} | {ukf_stats[0]:.1f} ¬± {ukf_stats[1]:.1f} | {ukf_stats[2]:.2f} ¬± {ukf_stats[3]:.2f} m\")\n",
    "print(f\"{'PF':<15} | {pf_stats[0]:.1f} ¬± {pf_stats[1]:.1f} | {pf_stats[2]:.2f} ¬± {pf_stats[3]:.2f} m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3. Fin√°ln√≠ Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_pos[\"KNet\"], agg_pos[\"UKF\"], agg_pos[\"PF\"]], labels=['KalmanNet', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({MC_ITERATIONS} runs)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
