{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15959426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'hB', 'souradniceGNSS', 'souradniceX', 'souradniceY', 'souradniceZ'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Robust path finding for data.mat\n",
    "current_path = Path.cwd()\n",
    "possible_data_paths = [\n",
    "    current_path / 'data' / 'data.mat',\n",
    "    current_path.parent / 'data' / 'data.mat',\n",
    "    current_path.parent.parent / 'data' / 'data.mat',\n",
    "    # Fallback absolute path\n",
    "    Path('/home/luky/skola/KalmanNet-for-state-estimation/data/data.mat')\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for p in possible_data_paths:\n",
    "    if p.exists():\n",
    "        dataset_path = p\n",
    "        break\n",
    "\n",
    "if dataset_path is None or not dataset_path.exists():\n",
    "    print(\"Warning: data.mat not found automatically.\")\n",
    "    dataset_path = Path('data/data.mat')\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Add project root to sys.path (2 levels up from debug/test)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")\n",
    "\n",
    "mat_data = loadmat(dataset_path)\n",
    "print(mat_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94742705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luky/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import trainer\n",
    "from utils import utils\n",
    "from Systems import DynamicSystem\n",
    "import Filters\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f00243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of 1D X axis: (2500,)\n",
      "Dimensions of 1D Y axis: (2500,)\n",
      "Dimensions of 2D elevation data Z: (2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "mat_data = loadmat(dataset_path)\n",
    "\n",
    "souradniceX_mapa = mat_data['souradniceX']\n",
    "souradniceY_mapa = mat_data['souradniceY']\n",
    "souradniceZ_mapa = mat_data['souradniceZ']\n",
    "souradniceGNSS = mat_data['souradniceGNSS'] \n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "\n",
    "print(f\"Dimensions of 1D X axis: {x_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 1D Y axis: {y_axis_unique.shape}\")\n",
    "print(f\"Dimensions of 2D elevation data Z: {souradniceZ_mapa.shape}\")\n",
    "\n",
    "terMap_interpolator = RegularGridInterpolator(\n",
    "    (y_axis_unique, x_axis_unique),\n",
    "    souradniceZ_mapa,\n",
    "    bounds_error=False, \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "def terMap(px, py):\n",
    "    # Query bilinear interpolation over the terrain map\n",
    "    points_to_query = np.column_stack((py, px))\n",
    "    return terMap_interpolator(points_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1487547.1250, 6395520.5000,       0.0000,       0.0000])\n",
      "INFO: DynamicSystemTAN inicializov√°n s hranicemi mapy:\n",
      "  X: [1476611.42, 1489541.47]\n",
      "  Y: [6384032.63, 6400441.34]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Systems import DynamicSystemTAN\n",
    "\n",
    "state_dim = 4\n",
    "obs_dim = 3\n",
    "dT = 1\n",
    "q = 1\n",
    "\n",
    "F = torch.tensor([[1.0, 0.0, dT, 0.0],\n",
    "                   [0.0, 1.0, 0.0, dT],\n",
    "                   [0.0, 0.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "Q = q* torch.tensor([[dT**3/3, 0.0, dT**2/2, 0.0],\n",
    "                   [0.0, dT**3/3, 0.0, dT**2/2],\n",
    "                   [dT**2/2, 0.0, dT, 0.0],\n",
    "                   [0.0, dT**2/2, 0.0, dT]])\n",
    "R = torch.tensor([[3.0**2, 0.0, 0.0],\n",
    "                   [0.0, 1.0**2, 0.0],\n",
    "                   [0.0, 0.0, 1.0**2]])\n",
    "\n",
    "initial_velocity_np = souradniceGNSS[:2, 1] - souradniceGNSS[:2, 0]\n",
    "# initial_velocity_np = torch.from_numpy()\n",
    "initial_velocity = torch.from_numpy(np.array([0,0]))\n",
    "\n",
    "initial_position = torch.from_numpy(souradniceGNSS[:2, 0])\n",
    "x_0 = torch.cat([\n",
    "    initial_position,\n",
    "    initial_velocity\n",
    "]).float()\n",
    "print(x_0)\n",
    "\n",
    "P_0 = torch.tensor([[25.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 25.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.5]])\n",
    "import torch.nn.functional as func\n",
    "\n",
    "def h_nl_differentiable(x: torch.Tensor, map_tensor, x_min, x_max, y_min, y_max) -> torch.Tensor:\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    px = x[:, 0]\n",
    "    py = x[:, 1]\n",
    "\n",
    "    px_norm = 2.0 * (px - x_min) / (x_max - x_min) - 1.0\n",
    "    py_norm = 2.0 * (py - y_min) / (y_max - y_min) - 1.0\n",
    "\n",
    "    sampling_grid = torch.stack((px_norm, py_norm), dim=1).view(batch_size, 1, 1, 2)\n",
    "\n",
    "    vyska_terenu_batch = func.grid_sample(\n",
    "        map_tensor.expand(batch_size, -1, -1, -1),\n",
    "        sampling_grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='border',\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    vyska_terenu = vyska_terenu_batch.view(batch_size)\n",
    "\n",
    "    eps = 1e-12\n",
    "    vx_w, vy_w = x[:, 2], x[:, 3]\n",
    "    norm_v_w = torch.sqrt(vx_w**2 + vy_w**2).clamp(min=eps)\n",
    "    cos_psi = vx_w / norm_v_w\n",
    "    sin_psi = vy_w / norm_v_w\n",
    "\n",
    "    vx_b = cos_psi * vx_w - sin_psi * vy_w \n",
    "    vy_b = sin_psi * vx_w + cos_psi * vy_w\n",
    "\n",
    "    result = torch.stack([vyska_terenu, vx_b, vy_b], dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "x_axis_unique = souradniceX_mapa[0, :]\n",
    "y_axis_unique = souradniceY_mapa[:, 0]\n",
    "terMap_tensor = torch.from_numpy(souradniceZ_mapa).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_min, x_max = x_axis_unique.min(), x_axis_unique.max()\n",
    "y_min, y_max = y_axis_unique.min(), y_axis_unique.max()\n",
    "\n",
    "h_wrapper = lambda x: h_nl_differentiable(\n",
    "    x, \n",
    "    map_tensor=terMap_tensor, \n",
    "    x_min=x_min, \n",
    "    x_max=x_max, \n",
    "    y_min=y_min, \n",
    "    y_max=y_max\n",
    ")\n",
    "\n",
    "system_model = DynamicSystemTAN(\n",
    "    state_dim=state_dim,\n",
    "    obs_dim=obs_dim,\n",
    "    Q=Q.float(),\n",
    "    R=R.float(),\n",
    "    Ex0=x_0.float(),\n",
    "    P0=P_0.float(),\n",
    "    F=F.float(),\n",
    "    h=h_wrapper,\n",
    "    x_axis_unique=x_axis_unique, \n",
    "    y_axis_unique=y_axis_unique,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from state_NN_models import StateKalmanNet \n",
    "from state_NN_models import StateKalmanNet_arch2\n",
    "from state_NN_models import KalmanFormer\n",
    "from utils import trainer \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\n",
      "üì• Naƒç√≠t√°m F√°zi 1: Seq=10 | Batch=256 ...\n",
      "   üîé Uk√°zka RAW dat (y): [323.7707824707031, -13.519903182983398, -29.721908569335938]\n",
      "üì• Naƒç√≠t√°m F√°zi 2: Seq=100 | Batch=128 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 3: Seq=200 | Batch=64 ...\n",
      "üì• Naƒç√≠t√°m F√°zi 4: Seq=300 | Batch=64 ...\n",
      "\n",
      "‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from utils import trainer # P≈ôedpokl√°d√°m, ≈æe toto m√°≈°\n",
    "\n",
    "# === 1. ZJEDNODU≈†EN√ù DATA MANAGER (BEZ NORMALIZACE) ===\n",
    "class NavigationDataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Jen dr≈æ√°k na cestu k dat≈Øm. ≈Ω√°dn√° statistika, ≈æ√°dn√° normalizace.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_dataloader(self, seq_len, split='train', shuffle=True, batch_size=32):\n",
    "        # Sestaven√≠ cesty: ./generated_data/len_100/train.pt\n",
    "        path = os.path.join(self.data_dir, f'len_{seq_len}', f'{split}.pt')\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset nenalezen: {path}\")\n",
    "            \n",
    "        # Naƒçten√≠ tenzor≈Ø\n",
    "        data = torch.load(path)\n",
    "        x = data['x'] # Stav [Batch, Seq, DimX]\n",
    "        y = data['y'] # Mƒõ≈ôen√≠ [Batch, Seq, DimY] - RAW DATA\n",
    "        \n",
    "        # Vytvo≈ôen√≠ datasetu\n",
    "        dataset = TensorDataset(x, y)\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# === 2. KONFIGURACE CURRICULA ===\n",
    "DATA_DIR = './generated_data_synthetic_controlled'\n",
    "\n",
    "# Inicializace mana≈æera (teƒè je to jen wrapper pro naƒç√≠t√°n√≠ soubor≈Ø)\n",
    "data_manager = NavigationDataManager(DATA_DIR)\n",
    "\n",
    "# Definice f√°z√≠ (zde ≈ô√≠d√≠≈°, jak se tr√©nink vyv√≠j√≠)\n",
    "curriculum_schedule = [\n",
    "    # F√ÅZE 1: Warm-up (Kr√°tk√© sekvence)\n",
    "    {\n",
    "        'phase_id': 1,\n",
    "        'seq_len': 10,          \n",
    "        'epochs': 500,           \n",
    "        'lr': 1e-3, \n",
    "        'batch_size': 256\n",
    "    },\n",
    "    \n",
    "    # F√ÅZE 2: Stabilizace (St≈ôedn√≠ d√©lka)\n",
    "    {\n",
    "        'phase_id': 2,\n",
    "        'seq_len': 100, \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-4,             \n",
    "        'batch_size': 128\n",
    "    },\n",
    "       # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 3,\n",
    "        'seq_len': 200,         \n",
    "        'epochs': 200, \n",
    "        'lr': 5e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    },\n",
    "    # F√ÅZE 3: Long-term Reality (Pln√° d√©lka)\n",
    "    {\n",
    "        'phase_id': 4,\n",
    "        'seq_len': 300,         \n",
    "        'epochs': 200, \n",
    "        'lr': 1e-5,             \n",
    "        'batch_size': 64       # Men≈°√≠ batch kv≈Øli pamƒõti GPU u dlouh√Ωch sekvenc√≠\n",
    "    }\n",
    "]\n",
    "\n",
    "# === 3. NAƒå√çT√ÅN√ç DO PAMƒöTI (CACHING) ===\n",
    "print(\"\\n=== NAƒå√çT√ÅN√ç RAW DAT Z DISKU (BEZ EXT. NORMALIZACE) ===\")\n",
    "datasets_cache = {} \n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    seq_len = phase['seq_len']\n",
    "    bs = phase['batch_size']\n",
    "    \n",
    "    print(f\"üì• Naƒç√≠t√°m F√°zi {phase['phase_id']}: Seq={seq_len} | Batch={bs} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Pou≈æit√≠ DataManageru\n",
    "        train_loader = data_manager.get_dataloader(seq_len=seq_len, split='train', shuffle=True, batch_size=bs)\n",
    "        val_loader = data_manager.get_dataloader(seq_len=seq_len, split='val', shuffle=False, batch_size=bs)\n",
    "        \n",
    "        # Ulo≈æen√≠ do cache\n",
    "        datasets_cache[phase['phase_id']] = (train_loader, val_loader)\n",
    "        \n",
    "        # Rychl√° kontrola pro jistotu\n",
    "        x_ex, y_ex = next(iter(train_loader))\n",
    "        if phase['phase_id'] == 1:\n",
    "            print(f\"   üîé Uk√°zka RAW dat (y): {y_ex[0, 0, :].tolist()}\") \n",
    "            # Mƒõl bys vidƒõt velk√° ƒç√≠sla (nap≈ô. 250.0) a mal√° (0.2), ne ~0.0\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è CHYBA: {e}\")\n",
    "        # raise e # Odkomentuj, pokud chce≈°, aby to spadlo p≈ôi chybƒõ\n",
    "\n",
    "print(\"\\n‚úÖ Data p≈ôipravena. Normalizaci ≈ôe≈°√≠ model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZACE NOV√âHO MODELU ===\n",
      "DEBUG: Layer 'output_final_linear.0' initialized near zero (Start K=0).\n",
      "Model inicializov√°n. Poƒçet parametr≈Ø: 212436\n",
      "=== SPU≈†TƒöN√ç TR√âNINKU ===\n",
      "\n",
      "--- PHASE 1: SeqLen 10 ---\n",
      "-> Using Standard Training (with Init Noise)\n",
      "Start training on cpu...\n",
      "Epoch [1/500] | Train Loss: 715.1402 | Val Loss: 330.3586 -> New Best!\n",
      "Epoch [2/500] | Train Loss: 257.3266 | Val Loss: 259.4979 -> New Best!\n",
      "Epoch [3/500] | Train Loss: 174.0171 | Val Loss: 124.9143 -> New Best!\n",
      "Epoch [4/500] | Train Loss: 59.0978 | Val Loss: 44.5967 -> New Best!\n",
      "Epoch [5/500] | Train Loss: 19.6627 | Val Loss: 17.6683 | PosMSE: 33.9, VelMSE: 5.4 -> New Best!\n",
      "Epoch [6/500] | Train Loss: 14.5094 | Val Loss: 14.7949 -> New Best!\n",
      "Epoch [8/500] | Train Loss: 10.5419 | Val Loss: 11.0060 -> New Best!\n",
      "Epoch [9/500] | Train Loss: 9.5733 | Val Loss: 10.4534 -> New Best!\n",
      "Epoch [10/500] | Train Loss: 8.8642 | Val Loss: 27.0497 | PosMSE: 14.3, VelMSE: 3.4\n",
      "Epoch [12/500] | Train Loss: 7.5662 | Val Loss: 9.8956 -> New Best!\n",
      "Epoch [15/500] | Train Loss: 6.1406 | Val Loss: 8.1924 | PosMSE: 9.4, VelMSE: 2.9 -> New Best!\n",
      "Epoch [16/500] | Train Loss: 5.8249 | Val Loss: 7.9313 -> New Best!\n",
      "Epoch [17/500] | Train Loss: 5.4411 | Val Loss: 7.5644 -> New Best!\n",
      "Epoch [18/500] | Train Loss: 5.1690 | Val Loss: 7.3352 -> New Best!\n",
      "Epoch [19/500] | Train Loss: 4.9184 | Val Loss: 7.0141 -> New Best!\n",
      "Epoch [20/500] | Train Loss: 4.7022 | Val Loss: 7.1349 | PosMSE: 6.7, VelMSE: 2.7\n",
      "Epoch [21/500] | Train Loss: 4.5854 | Val Loss: 6.7994 -> New Best!\n",
      "Epoch [23/500] | Train Loss: 4.4131 | Val Loss: 6.7788 -> New Best!\n",
      "Epoch [25/500] | Train Loss: 4.1254 | Val Loss: 6.4617 | PosMSE: 5.6, VelMSE: 2.7 -> New Best!\n",
      "Epoch [29/500] | Train Loss: 3.7854 | Val Loss: 6.3798 -> New Best!\n",
      "Epoch [30/500] | Train Loss: 3.6412 | Val Loss: 6.9252 | PosMSE: 4.7, VelMSE: 2.6\n",
      "Epoch [32/500] | Train Loss: 3.4915 | Val Loss: 6.2591 -> New Best!\n",
      "Epoch [33/500] | Train Loss: 3.4401 | Val Loss: 6.2297 -> New Best!\n",
      "Epoch [35/500] | Train Loss: 3.3145 | Val Loss: 6.2247 | PosMSE: 4.1, VelMSE: 2.6 -> New Best!\n",
      "Epoch [38/500] | Train Loss: 3.1918 | Val Loss: 6.1460 -> New Best!\n",
      "Epoch [40/500] | Train Loss: 3.0781 | Val Loss: 6.0500 | PosMSE: 3.6, VelMSE: 2.5 -> New Best!\n",
      "Epoch [45/500] | Train Loss: 2.9077 | Val Loss: 6.1407 | PosMSE: 3.3, VelMSE: 2.5\n",
      "Epoch [50/500] | Train Loss: 2.7016 | Val Loss: 6.2424 | PosMSE: 2.9, VelMSE: 2.5\n",
      "Epoch [55/500] | Train Loss: 2.5695 | Val Loss: 6.3495 | PosMSE: 2.7, VelMSE: 2.5\n",
      "Epoch [57/500] | Train Loss: 2.5219 | Val Loss: 5.9799 -> New Best!\n",
      "Epoch [60/500] | Train Loss: 2.4922 | Val Loss: 6.5188 | PosMSE: 2.5, VelMSE: 2.4\n",
      "Epoch [65/500] | Train Loss: 2.3685 | Val Loss: 6.1619 | PosMSE: 2.3, VelMSE: 2.4\n",
      "Epoch [70/500] | Train Loss: 2.3815 | Val Loss: 6.2384 | PosMSE: 2.4, VelMSE: 2.4\n",
      "Epoch [75/500] | Train Loss: 2.2741 | Val Loss: 6.1257 | PosMSE: 2.2, VelMSE: 2.4\n",
      "Epoch [80/500] | Train Loss: 2.1624 | Val Loss: 6.2332 | PosMSE: 2.0, VelMSE: 2.4\n",
      "Epoch [85/500] | Train Loss: 2.1316 | Val Loss: 6.0586 | PosMSE: 1.9, VelMSE: 2.4\n",
      "\n",
      "Early stopping triggered after 87 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 5.979928\n",
      "Phase 1 completed. Model saved.\n",
      "\n",
      "--- PHASE 2: SeqLen 100 ---\n",
      "-> Using TBPTT Sliding Window (k=40, w=20)\n",
      "INFO: Detected from model attribute that it returns covariance: False\n",
      "INFO: Starting training with TBPTT(k=40, w=40)\n",
      "Epoch [1/200] | Train Loss: 494.1092 | Val Loss: 51.9507 -> New Best!\n",
      "Epoch [2/200] | Train Loss: 449.8236 | Val Loss: 50.5907 -> New Best!\n",
      "Epoch [5/200] | Train Loss: 438.3049 | Val Loss: 52.1870 | Val MSE: 52.19\n",
      "Epoch [6/200] | Train Loss: 430.9848 | Val Loss: 49.3002 -> New Best!\n",
      "Epoch [7/200] | Train Loss: 420.9014 | Val Loss: 48.3503 -> New Best!\n",
      "Epoch [9/200] | Train Loss: 411.6831 | Val Loss: 46.9574 -> New Best!\n",
      "Epoch [10/200] | Train Loss: 411.5507 | Val Loss: 48.0107 | Val MSE: 48.01\n",
      "Epoch [15/200] | Train Loss: 389.1487 | Val Loss: 46.2526 | Val MSE: 46.25 -> New Best!\n",
      "Epoch [20/200] | Train Loss: 601.6649 | Val Loss: 47.2359 | Val MSE: 47.24\n",
      "Epoch [21/200] | Train Loss: 376.5595 | Val Loss: 46.1498 -> New Best!\n",
      "Epoch [25/200] | Train Loss: 386.5602 | Val Loss: 47.4206 | Val MSE: 47.42\n",
      "Epoch [28/200] | Train Loss: 372.3164 | Val Loss: 45.7445 -> New Best!\n",
      "Epoch [30/200] | Train Loss: 583.3208 | Val Loss: 47.7218 | Val MSE: 47.72\n",
      "Epoch [34/200] | Train Loss: 356.6487 | Val Loss: 45.4574 -> New Best!\n",
      "Epoch [35/200] | Train Loss: 363.0242 | Val Loss: 46.1717 | Val MSE: 46.17\n",
      "Epoch [40/200] | Train Loss: 352.8904 | Val Loss: 46.1202 | Val MSE: 46.12\n",
      "Epoch [41/200] | Train Loss: 343.4250 | Val Loss: 45.2426 -> New Best!\n",
      "Epoch [45/200] | Train Loss: 347.2819 | Val Loss: 46.8339 | Val MSE: 46.83\n",
      "Epoch [50/200] | Train Loss: 343.3277 | Val Loss: 46.5375 | Val MSE: 46.54\n",
      "Epoch [55/200] | Train Loss: 329.5193 | Val Loss: 49.6424 | Val MSE: 49.64\n",
      "Epoch [60/200] | Train Loss: 324.7915 | Val Loss: 46.7300 | Val MSE: 46.73\n",
      "Epoch [65/200] | Train Loss: 313.6516 | Val Loss: 47.0430 | Val MSE: 47.04\n",
      "Epoch [70/200] | Train Loss: 315.5511 | Val Loss: 50.1806 | Val MSE: 50.18\n",
      "\n",
      "Early stopping triggered after 71 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 45.242590\n",
      "Phase 2 completed. Model saved.\n",
      "\n",
      "--- PHASE 3: SeqLen 200 ---\n",
      "-> Using TBPTT Sliding Window (k=40, w=20)\n",
      "INFO: Detected from model attribute that it returns covariance: False\n",
      "INFO: Starting training with TBPTT(k=40, w=40)\n",
      "Epoch [1/200] | Train Loss: 6487.4593 | Val Loss: 90.0331 -> New Best!\n",
      "Epoch [2/200] | Train Loss: 6502.5923 | Val Loss: 88.3075 -> New Best!\n",
      "Epoch [4/200] | Train Loss: 3532.6238 | Val Loss: 82.9143 -> New Best!\n",
      "Epoch [5/200] | Train Loss: 3603.8268 | Val Loss: 82.1714 | Val MSE: 82.17 -> New Best!\n",
      "Epoch [10/200] | Train Loss: 71.6370 | Val Loss: 82.5482 | Val MSE: 82.55\n",
      "Epoch [11/200] | Train Loss: 70.4324 | Val Loss: 80.1358 -> New Best!\n",
      "Epoch [15/200] | Train Loss: 65.9430 | Val Loss: 4316.1496 | Val MSE: 4316.15\n",
      "Epoch [20/200] | Train Loss: 63.1233 | Val Loss: 83.8129 | Val MSE: 83.81\n",
      "Epoch [25/200] | Train Loss: 58.2340 | Val Loss: 82.6870 | Val MSE: 82.69\n",
      "Epoch [30/200] | Train Loss: 56.8111 | Val Loss: 84.1973 | Val MSE: 84.20\n",
      "Epoch [31/200] | Train Loss: 55.8820 | Val Loss: 79.3348 -> New Best!\n",
      "Epoch [35/200] | Train Loss: 54.3629 | Val Loss: 86.5113 | Val MSE: 86.51\n",
      "Epoch [40/200] | Train Loss: 52.4613 | Val Loss: 82.6046 | Val MSE: 82.60\n",
      "Epoch [45/200] | Train Loss: 51.5664 | Val Loss: 78.9116 | Val MSE: 78.91 -> New Best!\n",
      "Epoch [50/200] | Train Loss: 48.0403 | Val Loss: 82.4805 | Val MSE: 82.48\n",
      "Epoch [55/200] | Train Loss: 46.6671 | Val Loss: 80.5251 | Val MSE: 80.53\n",
      "Epoch [60/200] | Train Loss: 46.3735 | Val Loss: 81.2523 | Val MSE: 81.25\n",
      "Epoch [65/200] | Train Loss: 46.4781 | Val Loss: 85.1947 | Val MSE: 85.19\n",
      "Epoch [70/200] | Train Loss: 45.2527 | Val Loss: 85.2749 | Val MSE: 85.27\n",
      "Epoch [75/200] | Train Loss: 42.8814 | Val Loss: 86.9980 | Val MSE: 87.00\n",
      "\n",
      "Early stopping triggered after 75 epochs.\n",
      "Training completed.\n",
      "Loading best model with validation loss: 78.911621\n",
      "Phase 3 completed. Model saved.\n",
      "\n",
      "--- PHASE 4: SeqLen 300 ---\n",
      "-> Using TBPTT Sliding Window (k=50, w=20)\n",
      "INFO: Detected from model attribute that it returns covariance: False\n",
      "INFO: Starting training with TBPTT(k=100, w=100)\n",
      "Epoch [1/200] | Train Loss: 124.5972 | Val Loss: 125.5877 -> New Best!\n",
      "Epoch [2/200] | Train Loss: 121.0536 | Val Loss: 122.5952 -> New Best!\n",
      "Epoch [3/200] | Train Loss: 119.0385 | Val Loss: 121.7126 -> New Best!\n",
      "Epoch [4/200] | Train Loss: 118.0201 | Val Loss: 121.3406 -> New Best!\n",
      "Epoch [5/200] | Train Loss: 116.7859 | Val Loss: 120.7886 | Val MSE: 120.79 -> New Best!\n",
      "Epoch [6/200] | Train Loss: 115.8149 | Val Loss: 120.6351 -> New Best!\n",
      "Epoch [7/200] | Train Loss: 113.4496 | Val Loss: 120.2067 -> New Best!\n",
      "Epoch [10/200] | Train Loss: 111.6864 | Val Loss: 119.8771 | Val MSE: 119.88 -> New Best!\n",
      "Epoch [11/200] | Train Loss: 109.7553 | Val Loss: 119.5745 -> New Best!\n",
      "Epoch [15/200] | Train Loss: 106.9709 | Val Loss: 117.9422 | Val MSE: 117.94 -> New Best!\n",
      "Epoch [17/200] | Train Loss: 106.6268 | Val Loss: 117.5269 -> New Best!\n",
      "Epoch [20/200] | Train Loss: 103.7283 | Val Loss: 119.7126 | Val MSE: 119.71\n",
      "Epoch [22/200] | Train Loss: 103.3464 | Val Loss: 117.0524 -> New Best!\n",
      "Epoch [25/200] | Train Loss: 101.6889 | Val Loss: 117.1869 | Val MSE: 117.19\n",
      "Epoch [27/200] | Train Loss: 100.8326 | Val Loss: 116.5859 -> New Best!\n",
      "Epoch [29/200] | Train Loss: 99.4825 | Val Loss: 115.9647 -> New Best!\n",
      "Epoch [30/200] | Train Loss: 100.0768 | Val Loss: 116.6941 | Val MSE: 116.69\n",
      "Epoch [33/200] | Train Loss: 98.5982 | Val Loss: 115.7337 -> New Best!\n",
      "Epoch [35/200] | Train Loss: 97.8443 | Val Loss: 117.3662 | Val MSE: 117.37\n",
      "Epoch [40/200] | Train Loss: 96.1823 | Val Loss: 115.9702 | Val MSE: 115.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# SeqLen 300\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# F√ÅZE 3: TBPTT Long\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-> Using TBPTT Sliding Window (k=50, w=20)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state_KalmanNet_sliding_windowTAN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtbptt_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# !!! ZV√ù≈†ENO Z 2 NA 50 !!!\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtbptt_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Velmi opatrn√© √∫pravy vah\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknet_robust_len\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     79\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(state_knet2\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "File \u001b[0;32m~/skola/KalmanNet-for-state-estimation/utils/trainer.py:2172\u001b[0m, in \u001b[0;36mtrain_state_KalmanNet_sliding_windowTAN\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr, clip_grad, early_stopping_patience, tbptt_k, tbptt_w, optimizer_, weight_decay_, criterion_)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;66;03m# 5. Backward pass and weight update (per window)\u001b[39;00m\n\u001b[1;32m   2171\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 2172\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_grad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2175\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- A) KONFIGURACE S√çTƒö ---\n",
    "print(\"=== INICIALIZACE NOV√âHO MODELU ===\")\n",
    "state_knet2 = StateKalmanNet(\n",
    "        system_model=system_model, \n",
    "        device=device,\n",
    "        hidden_size_multiplier=10,       \n",
    "        output_layer_multiplier=4,\n",
    "        num_gru_layers=1,\n",
    "        gru_hidden_dim_multiplier=4      \n",
    ").to(device)\n",
    "\n",
    "print(f\"Model inicializov√°n. Poƒçet parametr≈Ø: {sum(p.numel() for p in state_knet2.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- B) SPU≈†TƒöN√ç TR√âNINKU ---\n",
    "print(\"=== SPU≈†TƒöN√ç TR√âNINKU ===\")\n",
    "\n",
    "for phase in curriculum_schedule:\n",
    "    phase_id = phase['phase_id']\n",
    "    seq_len = phase['seq_len']\n",
    "    \n",
    "    print(f\"\\n--- PHASE {phase_id}: SeqLen {seq_len} ---\")\n",
    "    \n",
    "    # 1. Naƒçten√≠ dat\n",
    "    if phase_id not in datasets_cache:\n",
    "        raise ValueError(\"Data nejsou vygenerov√°na! Spus≈• prvn√≠ bu≈àku.\")\n",
    "        \n",
    "    train_loader, val_loader = datasets_cache[phase_id]\n",
    "    \n",
    "    # 2. Z√°kladn√≠ argumenty spoleƒçn√© pro obƒõ metody\n",
    "    common_args = {\n",
    "        'model': state_knet2,\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'device': device,\n",
    "        'epochs': phase['epochs'],\n",
    "        'lr': phase['lr'],\n",
    "    }\n",
    "\n",
    "    # 3. Rozvƒõtven√≠ logiky podle d√©lky sekvence\n",
    "    if seq_len <= 20:\n",
    "        # === F√ÅZE 1 & 2: Kr√°tk√© sekvence (Standardn√≠ tr√©nink) ===\n",
    "        # Zde chceme init_noise pro robustnost startu\n",
    "        print(\"-> Using Standard Training (with Init Noise)\")\n",
    "        trainer.train_state_KalmanNetTAN(\n",
    "            **common_args,\n",
    "            optimizer_type=torch.optim.AdamW,\n",
    "            weight_decay=1e-3,\n",
    "            clip_grad=1.0,\n",
    "            early_stopping_patience=30\n",
    "        )\n",
    "    elif seq_len == 100 or seq_len == 200:\n",
    "        # F√ÅZE 2: TBPTT (Opraveno z '110' na '100')\n",
    "        # S procesn√≠m ≈°umem u≈æ nem≈Ø≈æeme tr√©novat 100 krok≈Ø v kuse (explodovaly by gradienty)\n",
    "        # Pou≈æijeme TBPTT s oknem 30-40 krok≈Ø.\n",
    "        print(f\"-> Using TBPTT Sliding Window (k=40, w=20)\")\n",
    "        trainer.train_state_KalmanNet_sliding_windowTAN(\n",
    "            **common_args,\n",
    "            weight_decay_=1e-4,\n",
    "            early_stopping_patience=30,\n",
    "            tbptt_k=40,   # !!! ZV√ù≈†ENO: Mus√≠ vidƒõt dozadu, aby poznal drift\n",
    "            tbptt_w=40,   # Posun o 20\n",
    "            optimizer_=torch.optim.AdamW,\n",
    "            clip_grad=0.25\n",
    "        )\n",
    "        \n",
    "    else: # SeqLen 300\n",
    "        # F√ÅZE 3: TBPTT Long\n",
    "        print(f\"-> Using TBPTT Sliding Window (k=50, w=20)\")\n",
    "        trainer.train_state_KalmanNet_sliding_windowTAN(\n",
    "            **common_args,\n",
    "            weight_decay_=1e-5,\n",
    "            early_stopping_patience=20,\n",
    "            tbptt_k=100,   # !!! ZV√ù≈†ENO Z 2 NA 50 !!!\n",
    "            tbptt_w=100,   \n",
    "            optimizer_=torch.optim.AdamW,\n",
    "            clip_grad=0.1 # Velmi opatrn√© √∫pravy vah\n",
    "        )\n",
    "    save_path = f'knet_robust_len{seq_len}.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Phase {phase_id} completed. Model saved.\")\n",
    "\n",
    "print(\"üéâ Tr√©nink dokonƒçen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d15a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 1. Naƒçteme podez≈ôel√Ω dataset\n",
    "# suspicious_path = './generated_data_synthetic_controlled/len_300/train.pt'\n",
    "# data = torch.load(suspicious_path)\n",
    "# x_gt_all = data['x'].to(device)\n",
    "# y_meas_all = data['y'].to(device)\n",
    "\n",
    "# print(f\"üîé Analyzuji dataset: {suspicious_path}\")\n",
    "# print(f\"   Poƒçet trajektori√≠: {len(x_gt_all)}\")\n",
    "\n",
    "# # 2. Spust√≠me v√°≈° aktu√°ln√≠ model (StateKalmanNet) na v≈°ech datech\n",
    "# state_knet2.eval()\n",
    "# losses = []\n",
    "\n",
    "# bad_indices = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(x_gt_all)):\n",
    "#         # P≈ô√≠prava jedn√© trajektorie\n",
    "#         x_gt = x_gt_all[i].unsqueeze(0) # [1, Seq, 4]\n",
    "#         y_meas = y_meas_all[i].unsqueeze(0) # [1, Seq, 3]\n",
    "        \n",
    "#         # Reset modelu\n",
    "#         state_knet2.reset(batch_size=1, initial_state=x_gt[:, 0, :])\n",
    "        \n",
    "#         # Inference\n",
    "#         preds = []\n",
    "#         for t in range(1, x_gt.shape[1]):\n",
    "#             pred = state_knet2.step(y_meas[:, t, :])\n",
    "#             preds.append(pred)\n",
    "            \n",
    "#         preds = torch.stack(preds, dim=1) # [1, Seq-1, 4]\n",
    "        \n",
    "#         # V√Ωpoƒçet MSE pro tuto jednu trajektorii\n",
    "#         loss = torch.nn.functional.mse_loss(preds, x_gt[:, 1:, :]).item()\n",
    "#         losses.append(loss)\n",
    "        \n",
    "#         # Hled√°me extr√©my (nap≈ô. Loss > 500)\n",
    "#         if loss > 1000:\n",
    "#             bad_indices.append((i, loss))\n",
    "\n",
    "# # 3. V√Ωpis v√Ωsledk≈Ø\n",
    "# losses = np.array(losses)\n",
    "# print(f\"\\nüìä Statistika Loss:\")\n",
    "# print(f\"   Pr≈Ømƒõr: {np.mean(losses):.2f}\")\n",
    "# print(f\"   Medi√°n: {np.median(losses):.2f}\") # Medi√°n nebude ovlivnƒõn outlierem!\n",
    "# print(f\"   Max:    {np.max(losses):.2f}\")\n",
    "# print(f\"   Min:    {np.min(losses):.2f}\")\n",
    "\n",
    "# print(f\"\\n‚ö†Ô∏è Nalezen√≠ vin√≠ci (Loss > 1000):\")\n",
    "# for idx, val in bad_indices:\n",
    "#     print(f\"   Index {idx}: Loss = {val:.2f}\")\n",
    "\n",
    "# # 4. Vizualizace nejhor≈°√≠ho p≈ô√≠padu\n",
    "# if bad_indices:\n",
    "#     worst_idx =  max(bad_indices, key=lambda item: item[1])[0]\n",
    "#     print(f\"\\nüìà Vykresluji nejhor≈°√≠ trajektorii (Index {worst_idx})...\")\n",
    "    \n",
    "#     # Znovu spust√≠me pro vykreslen√≠\n",
    "#     x_gt = x_gt_all[worst_idx].cpu().numpy()\n",
    "#     y_meas = y_meas_all[worst_idx].cpu().numpy()\n",
    "    \n",
    "#     # ... (zde by byl k√≥d pro plot, pokud ho chce≈°) ...\n",
    "#     # Ale u≈æ ta ƒç√≠sla naho≈ôe ti ≈ôeknou v≈°e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # Cesta k po≈°kozen√©mu souboru\n",
    "# bad_file = './generated_data_synthetic_controlled/len_300/train.pt'\n",
    "# backup_file = bad_file + '.bak'\n",
    "\n",
    "# # 1. Naƒçten√≠\n",
    "# data = torch.load(bad_file)\n",
    "# x_all = data['x']\n",
    "# y_all = data['y']\n",
    "\n",
    "# print(f\"P≈Øvodn√≠ poƒçet: {len(x_all)}\")\n",
    "\n",
    "# # 2. Definice index≈Ø k odstranƒõn√≠ (z va≈°√≠ diagnostiky)\n",
    "# bad_indices = [327,1521] \n",
    "\n",
    "# # 3. Vytvo≈ôen√≠ masky pro zachov√°n√≠ dobr√Ωch dat\n",
    "# mask = torch.ones(len(x_all), dtype=torch.bool)\n",
    "# mask[bad_indices] = False\n",
    "\n",
    "# # 4. Filtrace\n",
    "# x_clean = x_all[mask]\n",
    "# y_clean = y_all[mask]\n",
    "\n",
    "# print(f\"Nov√Ω poƒçet: {len(x_clean)}\")\n",
    "\n",
    "# # 5. Ulo≈æen√≠ (s z√°lohou)\n",
    "# if not os.path.exists(backup_file):\n",
    "#     os.rename(bad_file, backup_file)\n",
    "#     print(f\"Z√°loha ulo≈æena do: {backup_file}\")\n",
    "\n",
    "# torch.save({'x': x_clean, 'y': y_clean}, bad_file)\n",
    "# print(f\"‚úÖ Vyƒçi≈°tƒõn√Ω dataset ulo≈æen do: {bad_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save model.\n",
    "    save_path = f'knet_curriculum_model_1GRU_relativne_ok_vysledky.pth'\n",
    "    torch.save(state_knet2.state_dict(), save_path)\n",
    "    print(f\"Model saved to '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670fcc",
   "metadata": {},
   "source": [
    "# Test na synteticke trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VYHODNOCEN√ç NA TESTOVAC√ç SADƒö ===\n",
      "Naƒç√≠t√°m data z: ./generated_data_synthetic_controlled/test_set/test.pt\n",
      "Poƒçet testovac√≠ch trajektori√≠: 20\n",
      "D√©lka sekvence: 1000\n",
      "Modely: KalmanNet vs. UKF vs. PF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluace:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluace:   0%|          | 0/20 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# PF (Sn√≠≈æil jsem poƒçet ƒç√°stic na 2000 pro rychlost, pro fin√°ln√≠ diplomku dejte v√≠ce)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m pf \u001b[38;5;241m=\u001b[39m Filters\u001b[38;5;241m.\u001b[39mParticleFilter(system_model, num_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80000\u001b[39m) \n\u001b[0;32m---> 94\u001b[0m pf_res \u001b[38;5;241m=\u001b[39m \u001b[43mpf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_obs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_init_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP0\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m x_est_pf \u001b[38;5;241m=\u001b[39m pf_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# D) V√Ωpoƒçet chyb\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# O≈ôe≈æeme na d√©lku min(odhad, gt) pro jistotu\u001b[39;00m\n",
      "File \u001b[0;32m~/skola/KalmanNet-for-state-estimation/Filters/ParticleFilter.py:119\u001b[0m, in \u001b[0;36mParticleFilter.process_sequence\u001b[0;34m(self, y_seq, u_sequence, Ex0, P0, Q, R, resampling_threshold)\u001b[0m\n\u001b[1;32m    117\u001b[0m effective_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(current_weights\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_N \u001b[38;5;241m<\u001b[39m n_threshold:\n\u001b[0;32m--> 119\u001b[0m     current_particles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_systematic_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_particles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     current_weights\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m current_particles\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChyba na konci kroku \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/skola/KalmanNet-for-state-estimation/Filters/ParticleFilter.py:54\u001b[0m, in \u001b[0;36mParticleFilter._systematic_resample\u001b[0;34m(self, particles, weights)\u001b[0m\n\u001b[1;32m     52\u001b[0m new_particles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(particles)\n\u001b[1;32m     53\u001b[0m n, m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m<\u001b[39m N:\n\u001b[1;32m     55\u001b[0m     u \u001b[38;5;241m=\u001b[39m u0 \u001b[38;5;241m+\u001b[39m n \u001b[38;5;241m/\u001b[39m N\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m u \u001b[38;5;241m>\u001b[39m Q[m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import Filters\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === KONFIGURACE ===\n",
    "TEST_DATA_PATH = './generated_data_synthetic_controlled/test_set/test.pt'\n",
    "PLOT_PER_ITERATION = True  # Vykreslovat graf pro ka≈ædou trajektorii?\n",
    "MAX_TEST_SAMPLES = 20      # Kolik trajektori√≠ z test setu vyhodnotit (max 10, co jsme vygenerovali)\n",
    "\n",
    "print(f\"=== VYHODNOCEN√ç NA TESTOVAC√ç SADƒö ===\")\n",
    "print(f\"Naƒç√≠t√°m data z: {TEST_DATA_PATH}\")\n",
    "\n",
    "# 1. Naƒçten√≠ Testovac√≠ sady\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Soubor {TEST_DATA_PATH} neexistuje! Spus≈•te generov√°n√≠ testovac√≠ sady.\")\n",
    "\n",
    "test_data = torch.load(TEST_DATA_PATH, map_location=device)\n",
    "X_test_all = test_data['x']  # Ground Truth [N, Seq, 4]\n",
    "Y_test_all = test_data['y']  # Measurements [N, Seq, 3]\n",
    "\n",
    "n_samples = min(X_test_all.shape[0], MAX_TEST_SAMPLES)\n",
    "print(f\"Poƒçet testovac√≠ch trajektori√≠: {n_samples}\")\n",
    "print(f\"D√©lka sekvence: {X_test_all.shape[1]}\")\n",
    "print(\"Modely: KalmanNet vs. UKF vs. PF\")\n",
    "\n",
    "# 2. Inicializace pro sbƒõr dat\n",
    "detailed_results = []\n",
    "agg_mse = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "agg_pos = {\"KNet\": [], \"UKF\": [], \"PF\": []}\n",
    "\n",
    "# Ujist√≠me se, ≈æe KNet je v eval m√≥du\n",
    "state_knet2.eval()\n",
    "\n",
    "# --- HLAVN√ç SMYƒåKA (Iterace p≈ôes testovac√≠ trajektorie) ---\n",
    "for i in tqdm(range(n_samples), desc=\"Evaluace\"):\n",
    "    \n",
    "    # A) P≈ô√≠prava dat pro tento bƒõh\n",
    "    x_gt_tensor = X_test_all[i]      # [Seq, 4]\n",
    "    y_obs_tensor = Y_test_all[i]     # [Seq, 3]\n",
    "    \n",
    "    x_gt = x_gt_tensor.cpu().numpy()\n",
    "    seq_len = x_gt.shape[0]\n",
    "    \n",
    "    # Skuteƒçn√Ω startovn√≠ stav (pro inicializaci filtr≈Ø)\n",
    "    true_init_state = x_gt_tensor[0] # [4]\n",
    "    \n",
    "    # B) Inference: KalmanNet\n",
    "    # KNet oƒçek√°v√° [Batch, Seq, Dim], tak≈æe mus√≠me p≈ôidat dimenzi\n",
    "    with torch.no_grad():\n",
    "        initial_state_batch = true_init_state.unsqueeze(0) # [1, 4]\n",
    "        \n",
    "        # Reset stavu s√≠tƒõ\n",
    "        state_knet2.reset(batch_size=1, initial_state=initial_state_batch)\n",
    "        \n",
    "        knet_preds = []\n",
    "        # KNet zpracov√°v√° sekvenci krok po kroku (nebo bychom mohli upravit forward na celou sekvenci)\n",
    "        # Zde zachov√°me logiku step-by-step pro konzistenci\n",
    "        \n",
    "        # Vstup y_obs_tensor m√° tvar [Seq, 3]. Pot≈ôebujeme [1, 3] pro ka≈æd√Ω krok\n",
    "        y_input_batch = y_obs_tensor.unsqueeze(0) # [1, Seq, 3]\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            y_t = y_input_batch[:, t, :] # [1, 3]\n",
    "            x_est = state_knet2.step(y_t)\n",
    "            knet_preds.append(x_est)\n",
    "            \n",
    "        # Slo≈æen√≠ predikce (p≈ôid√°me poƒç√°teƒçn√≠ stav)\n",
    "        if len(knet_preds) > 0:\n",
    "            knet_preds_tensor = torch.stack(knet_preds, dim=1) # [1, Seq-1, 4]\n",
    "            full_knet_est = torch.cat([initial_state_batch.unsqueeze(1), knet_preds_tensor], dim=1)\n",
    "        else:\n",
    "            full_knet_est = initial_state_batch.unsqueeze(1)\n",
    "            \n",
    "        x_est_knet = full_knet_est.squeeze().cpu().numpy()\n",
    "\n",
    "    # C) Inference: UKF & PF\n",
    "    # Filtry oƒçek√°vaj√≠ [Seq, Dim] (bez batch dimenze, pokud tak byly naps√°ny)\n",
    "    \n",
    "    # UKF\n",
    "    ukf_ideal = Filters.UnscentedKalmanFilter(system_model)\n",
    "    ukf_res = ukf_ideal.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_ukf = ukf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    # PF (Sn√≠≈æil jsem poƒçet ƒç√°stic na 2000 pro rychlost, pro fin√°ln√≠ diplomku dejte v√≠ce)\n",
    "    pf = Filters.ParticleFilter(system_model, num_particles=30000) \n",
    "    pf_res = pf.process_sequence(\n",
    "        y_seq=y_obs_tensor,\n",
    "        Ex0=true_init_state, \n",
    "        P0=system_model.P0\n",
    "    )\n",
    "    x_est_pf = pf_res['x_filtered'].cpu().numpy()\n",
    "\n",
    "    \n",
    "    # D) V√Ωpoƒçet chyb\n",
    "    # O≈ôe≈æeme na d√©lku min(odhad, gt) pro jistotu\n",
    "    min_len = min(len(x_gt), len(x_est_knet), len(x_est_ukf))\n",
    "    \n",
    "    # KNet\n",
    "    diff_knet = x_est_knet[:min_len] - x_gt[:min_len]\n",
    "    mse_knet = np.mean(np.sum(diff_knet[:, :2]**2, axis=1)) # Pouze XY chyba\n",
    "    pos_err_knet = np.mean(np.sqrt(diff_knet[:, 0]**2 + diff_knet[:, 1]**2))\n",
    "    \n",
    "    # UKF\n",
    "    diff_ukf = x_est_ukf[:min_len] - x_gt[:min_len]\n",
    "    mse_ukf = np.mean(np.sum(diff_ukf[:, :2]**2, axis=1))\n",
    "    pos_err_ukf = np.mean(np.sqrt(diff_ukf[:, 0]**2 + diff_ukf[:, 1]**2))\n",
    "    \n",
    "    # PF\n",
    "    diff_pf = x_est_pf[:min_len] - x_gt[:min_len]\n",
    "    mse_pf = np.mean(np.sum(diff_pf[:, :2]**2, axis=1))\n",
    "    pos_err_pf = np.mean(np.sqrt(diff_pf[:, 0]**2 + diff_pf[:, 1]**2))\n",
    "    \n",
    "    # Ulo≈æen√≠\n",
    "    agg_mse[\"KNet\"].append(mse_knet)\n",
    "    agg_pos[\"KNet\"].append(pos_err_knet)\n",
    "    agg_mse[\"UKF\"].append(mse_ukf)\n",
    "    agg_pos[\"UKF\"].append(pos_err_ukf)\n",
    "    agg_mse[\"PF\"].append(mse_pf)\n",
    "    agg_pos[\"PF\"].append(pos_err_pf)\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"Run_ID\": i + 1,\n",
    "        \"KNet_MSE\": mse_knet,\n",
    "        \"UKF_MSE\": mse_ukf,\n",
    "        \"PF_MSE\": mse_pf,\n",
    "        \"KNet_PosErr\": pos_err_knet,\n",
    "        \"UKF_PosErr\": pos_err_ukf,\n",
    "        \"PF_PosErr\": pos_err_pf\n",
    "    })\n",
    "    \n",
    "    # E) Vykreslen√≠\n",
    "    if PLOT_PER_ITERATION:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        # Vykresl√≠me jen XY trajektorii\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], 'k-', linewidth=3, alpha=0.3, label='Ground Truth')\n",
    "        plt.plot(x_est_knet[:, 0], x_est_knet[:, 1], 'g-', linewidth=2, label=f'KalmanNet (Err: {pos_err_knet:.1f}m)')\n",
    "        plt.plot(x_est_ukf[:, 0], x_est_ukf[:, 1], 'b--', linewidth=1, label=f'UKF (Err: {pos_err_ukf:.1f}m)')\n",
    "        plt.plot(x_est_pf[:, 0], x_est_pf[:, 1], 'r:', linewidth=1, alpha=0.6, label=f'PF (Err: {pos_err_pf:.1f}m)')\n",
    "        \n",
    "        plt.title(f\"Test Trajectory {i+1} (Length {seq_len})\")\n",
    "        plt.xlabel(\"X [m]\")\n",
    "        plt.ylabel(\"Y [m]\")\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- V√ùPIS V√ùSLEDK≈Æ ---\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILN√ç V√ùSLEDKY PO JEDNOTLIV√ùCH TRAJEKTORI√çCH\")\n",
    "print(\"=\"*80)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(df_results[[\"Run_ID\", \"KNet_MSE\", \"UKF_MSE\", \"PF_MSE\", \"KNet_PosErr\", \"UKF_PosErr\", \"PF_PosErr\"]])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SOUHRNN√Å STATISTIKA ({n_samples} trajektori√≠)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_stats(key):\n",
    "    return np.mean(agg_mse[key]), np.std(agg_mse[key]), np.mean(agg_pos[key]), np.std(agg_pos[key])\n",
    "\n",
    "knet_stats = get_stats(\"KNet\")\n",
    "ukf_stats = get_stats(\"UKF\")\n",
    "pf_stats = get_stats(\"PF\")\n",
    "\n",
    "print(f\"{'Model':<15} | {'MSE (Mean ¬± Std)':<25} | {'Pos Error (Mean ¬± Std)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'KalmanNet':<15} | {knet_stats[0]:.1f} ¬± {knet_stats[1]:.1f} | {knet_stats[2]:.2f} ¬± {knet_stats[3]:.2f} m\")\n",
    "print(f\"{'UKF':<15} | {ukf_stats[0]:.1f} ¬± {ukf_stats[1]:.1f} | {ukf_stats[2]:.2f} ¬± {ukf_stats[3]:.2f} m\")\n",
    "print(f\"{'PF':<15} | {pf_stats[0]:.1f} ¬± {pf_stats[1]:.1f} | {pf_stats[2]:.2f} ¬± {pf_stats[3]:.2f} m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([agg_pos[\"KNet\"], agg_pos[\"UKF\"], agg_pos[\"PF\"]], labels=['KalmanNet', 'UKF', 'PF'], patch_artist=True)\n",
    "plt.title(f\"Position Error Distribution ({n_samples} test trajectories)\")\n",
    "plt.ylabel(\"Avg Position Error [m]\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
