{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ff0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_path = os.getcwd() \n",
    "project_root = os.path.dirname(notebook_path)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dd10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9e0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_models\n",
    "import Filters\n",
    "import utils\n",
    "import Systems\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from state_NN_models.StateBayesianKalmanNet import StateBayesianKalmanNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c41a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Používané zařízení: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Používané zařízení: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1190cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true_nonlinear = lambda x: 0.5 * x\n",
    "f_true_nonlinear = lambda x: 0.9 * x - 0.05 * x**3 \n",
    "\n",
    "Q_true = torch.tensor([[0.1]])\n",
    "R_true = torch.tensor([[0.1]])\n",
    "\n",
    "Ex0_true = torch.tensor([[1.0]])\n",
    "P0_true = torch.tensor([[0.5]])\n",
    "\n",
    "sys_true = Systems.NonlinearSystem(f_true_nonlinear, h_true_nonlinear, Q_true, R_true, Ex0_true, P0_true)\n",
    "\n",
    "#  Nepřesná dynamika (lineární aproximace nelineární funkce f)\n",
    "f_model_nonlinear = lambda x: 0.9 * x \n",
    "h_model_nonlinear = h_true_nonlinear\n",
    "# Nepřesná znalost šumu (podcenění Q)\n",
    "Q_model = torch.tensor([[0.01]])\n",
    "R_model = torch.tensor([[0.2]])\n",
    "# Nepřesný počáteční odhad (pro EKF)\n",
    "Ex0_model = torch.tensor([[0.5]])\n",
    "P0_model = torch.tensor([[0.5]])\n",
    "\n",
    "# Sestavení nepřesného modelu pro filtry\n",
    "# Funkce h, R jsou pro jednoduchost stejné, ale f, Q, Ex0, P0 jsou jiné\n",
    "# sys_model = Systems.NonlinearSystem(f_model_nonlinear, h_model_nonlinear, Q_model, R_model, Ex0_model, P0_model)\n",
    "sys_model = Systems.NonlinearSystem(f_true_nonlinear, h_true_nonlinear, Q_true, R_true, Ex0_model, P0_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85eda8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEQ_LEN = 10\n",
    "VALID_SEQ_LEN = 10  # Změněno z 20\n",
    "TEST_SEQ_LEN = 250   # Změněno z 200\n",
    "\n",
    "# Počty trajektorií\n",
    "NUM_TRAIN_TRAJ = 300 # Změněno z 500\n",
    "NUM_VALID_TRAJ = 5   # Změněno z 250\n",
    "NUM_TEST_TRAJ = 10  # Změněno z 100\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 5         # Zůstává stejné\n",
    "\n",
    "\n",
    "x_train, y_train = utils.generate_data(sys_true, num_trajectories=NUM_TRAIN_TRAJ, seq_len=TRAIN_SEQ_LEN)\n",
    "x_val, y_val = utils.generate_data(sys_true, num_trajectories=NUM_VALID_TRAJ, seq_len=VALID_SEQ_LEN)\n",
    "x_test, y_test = utils.generate_data(sys_true, num_trajectories=1, seq_len=TEST_SEQ_LEN)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a763fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ekf = Filters.ExtendedKalmanFilter(sys_model)\n",
    "y_test_seq = y_test.squeeze(0)\n",
    "ekf_results = ekf.apply_filter(y_test_seq)\n",
    "x_hat_ekf = ekf_results['x_filtered']\n",
    "P_hat_ekf = ekf_results['P_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e42c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iterace [10/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.223700 | Current Beta: 0.0072\n",
      "    ├─ L1 (MSE):   0.164753 (váha: 0.99)\n",
      "    ├─ L2 (Var Sum):7.259990 (váha: 0.0072)\n",
      "    └─ Regularize: 0.007861 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.003898, Min: 0.000029, Max: 0.022785\n",
      "  True Var      -> Avg: 0.164753, Min: 0.000109, Max: 0.802430\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.385049, Min: -0.118264, Max: 1.009708\n",
      "  True Mean     -> Avg: 0.385556, Min: -0.853153, Max: 1.517541\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001719\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000913\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.004942\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000251\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000024\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000116\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000106\n",
      "  Total Grad Norm (before clipping): 0.005320\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6270, p2=0.6128\n",
      "\n",
      "--- Iterace [20/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.452826 | Current Beta: 0.0152\n",
      "    ├─ L1 (MSE):   0.268647 (váha: 0.98)\n",
      "    ├─ L2 (Var Sum):11.902865 (váha: 0.0152)\n",
      "    └─ Regularize: 0.007339 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.004212, Min: 0.000115, Max: 0.017576\n",
      "  True Var      -> Avg: 0.268647, Min: 0.001201, Max: 1.290681\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.509637, Min: 0.034119, Max: 1.277077\n",
      "  True Mean     -> Avg: 0.638256, Min: -0.479889, Max: 1.827505\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001657\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000881\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.004607\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000238\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000021\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000112\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000097\n",
      "  Total Grad Norm (before clipping): 0.004982\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6247, p2=0.6105\n",
      "\n",
      "--- Iterace [30/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.555828 | Current Beta: 0.0232\n",
      "    ├─ L1 (MSE):   0.274386 (váha: 0.98)\n",
      "    ├─ L2 (Var Sum):12.110318 (váha: 0.0232)\n",
      "    └─ Regularize: 0.006848 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.006667, Min: 0.000770, Max: 0.040737\n",
      "  True Var      -> Avg: 0.274386, Min: 0.000567, Max: 1.170768\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.501293, Min: -0.102539, Max: 1.462040\n",
      "  True Mean     -> Avg: 0.526050, Min: -0.875753, Max: 1.406536\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001597\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000850\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.004293\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000226\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000018\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000107\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000088\n",
      "  Total Grad Norm (before clipping): 0.004666\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6224, p2=0.6082\n",
      "\n",
      "--- Iterace [40/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.374499 | Current Beta: 0.0312\n",
      "    ├─ L1 (MSE):   0.156307 (váha: 0.97)\n",
      "    ├─ L2 (Var Sum):6.944870 (váha: 0.0312)\n",
      "    └─ Regularize: 0.006389 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.002100, Min: 0.000000, Max: 0.014607\n",
      "  True Var      -> Avg: 0.156307, Min: 0.000775, Max: 1.030324\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.139574, Min: -0.640196, Max: 1.155695\n",
      "  True Mean     -> Avg: 0.288495, Min: -1.216151, Max: 1.483060\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001539\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000821\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.004001\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000214\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000015\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000103\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000080\n",
      "  Total Grad Norm (before clipping): 0.004372\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6202, p2=0.6060\n",
      "\n",
      "--- Iterace [50/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.418237 | Current Beta: 0.0392\n",
      "    ├─ L1 (MSE):   0.152258 (váha: 0.96)\n",
      "    ├─ L2 (Var Sum):6.785381 (váha: 0.0392)\n",
      "    └─ Regularize: 0.005961 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.001515, Min: 0.000017, Max: 0.006884\n",
      "  True Var      -> Avg: 0.152258, Min: 0.000774, Max: 0.894595\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.534748, Min: 0.232580, Max: 1.445952\n",
      "  True Mean     -> Avg: 0.473476, Min: -0.274321, Max: 1.571157\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001484\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000792\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.003729\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000203\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000013\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000099\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000073\n",
      "  Total Grad Norm (before clipping): 0.004098\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6181, p2=0.6040\n",
      "\n",
      "--- Iterace [60/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.600771 | Current Beta: 0.0472\n",
      "    ├─ L1 (MSE):   0.194087 (váha: 0.95)\n",
      "    ├─ L2 (Var Sum):8.692450 (váha: 0.0472)\n",
      "    └─ Regularize: 0.005561 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.000952, Min: 0.000010, Max: 0.005721\n",
      "  True Var      -> Avg: 0.194087, Min: 0.000079, Max: 1.070703\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.611762, Min: 0.204884, Max: 1.336402\n",
      "  True Mean     -> Avg: 0.847336, Min: 0.071556, Max: 1.870706\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001430\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000765\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.003477\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000192\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000011\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000096\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000066\n",
      "  Total Grad Norm (before clipping): 0.003843\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6161, p2=0.6020\n",
      "\n",
      "--- Iterace [70/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.515331 | Current Beta: 0.0552\n",
      "    ├─ L1 (MSE):   0.149405 (váha: 0.94)\n",
      "    ├─ L2 (Var Sum):6.684511 (váha: 0.0552)\n",
      "    └─ Regularize: 0.005188 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.000928, Min: 0.000029, Max: 0.002750\n",
      "  True Var      -> Avg: 0.149405, Min: 0.000002, Max: 0.644893\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.373382, Min: 0.017452, Max: 1.142454\n",
      "  True Mean     -> Avg: 0.340901, Min: -0.752359, Max: 1.245236\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001379\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000738\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.003242\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000182\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000010\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000092\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000060\n",
      "  Total Grad Norm (before clipping): 0.003606\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6141, p2=0.6001\n",
      "\n",
      "--- Iterace [80/1000] | Fáze: 1 ---\n",
      "--- Loss Stats ---\n",
      "  Total Loss:       0.996859 | Current Beta: 0.0632\n",
      "    ├─ L1 (MSE):   0.262770 (váha: 0.94)\n",
      "    ├─ L2 (Var Sum):11.801521 (váha: 0.0632)\n",
      "    └─ Regularize: 0.004839 (váha: 1)\n",
      "--- Variance Stats ---\n",
      "  Predicted Var -> Avg: 0.000514, Min: 0.000001, Max: 0.002180\n",
      "  True Var      -> Avg: 0.262770, Min: 0.000083, Max: 1.026288\n",
      "--- Mean (State) Stats ---\n",
      "  Predicted Mean-> Avg: 0.335523, Min: 0.002108, Max: 1.056833\n",
      "  True Mean     -> Avg: 0.354095, Min: -0.713305, Max: 1.695384\n",
      "--- Gradient Stats ---\n",
      "  Layer: dnn.input_layer.0.weight       | Grad norm: 0.001330\n",
      "  Layer: dnn.input_layer.0.bias         | Grad norm: 0.000713\n",
      "  Layer: dnn.concrete_dropout1.p_logit  | Grad norm: 0.003024\n",
      "  Layer: dnn.output_layer.0.weight      | Grad norm: 0.000173\n",
      "  Layer: dnn.output_layer.0.bias        | Grad norm: 0.000008\n",
      "  Layer: dnn.output_layer.2.weight      | Grad norm: 0.000089\n",
      "  Layer: dnn.concrete_dropout2.p_logit  | Grad norm: 0.000055\n",
      "  Total Grad Norm (before clipping): 0.003385\n",
      "--- Model Stats ---\n",
      "  Learned p: p1=0.6122, p2=0.5984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m state_knet \u001b[38;5;241m=\u001b[39m StateBayesianKalmanNet(sys_model, device\u001b[38;5;241m=\u001b[39mdevice, hidden_size_multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_gru_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_bkn_with_empirical_averaging\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_knet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_train_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrain_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Prvních 1000 iterací na MSE\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_phase1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Rychlejší učení pro MSE\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_phase2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Pomalejší, jemnější učení pro varianci\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mJ_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/skola/KalmanNet-for-state-estimation/utils/utils.py:1530\u001b[0m, in \u001b[0;36mtrain_bkn_with_empirical_averaging\u001b[0;34m(model, train_loader, val_loader, device, total_train_iter, lr_phase1, lr_phase2, clip_grad, J_samples, reg_weight, validation_period, logging_period, pretrain_iters, final_beta)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1531\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_grad)\n\u001b[1;32m   1532\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_knet = StateBayesianKalmanNet(sys_model, device=device, hidden_size_multiplier=10, num_gru_layers=1).to(device)\n",
    "trained_model = utils.train_bkn_with_empirical_averaging(\n",
    "    model=state_knet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    total_train_iter=700,\n",
    "    pretrain_iters=100,   # Prvních 1000 iterací na MSE\n",
    "    lr_phase1=1e-3,        # Rychlejší učení pro MSE\n",
    "    lr_phase2=1e-3,        # Pomalejší, jemnější učení pro varianci\n",
    "    clip_grad=10,\n",
    "    J_samples=20,\n",
    "    reg_weight=1,\n",
    "    logging_period=10,\n",
    "    final_beta=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# Předpoklady:\n",
    "# Následující proměnné a moduly musí být definovány a naimportovány PŘED \n",
    "# spuštěním tohoto skriptu:\n",
    "#\n",
    "# import utils                  # Váš modul pro generování dat\n",
    "# import Filters                 # Váš modul obsahující EKF\n",
    "# from StateBayesianKalmanNet import StateBayesianKalmanNet # Vaše třída modelu\n",
    "#\n",
    "# device = torch.device(...)   # Např. torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# state_knet = ...             # Instance vaší natrénované třídy StateBayesianKalmanNet\n",
    "# sys_true = ...               # Instance třídy popisující skutečný systém\n",
    "# sys_model = ...              # Instance třídy popisující model systému (pro EKF)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# --- 1. Definice pomocných funkcí a metrik ---\n",
    "\n",
    "def calculate_anees(x_true_list, x_hat_list, P_hat_list):\n",
    "    \"\"\"\n",
    "    Vypočítá Average NEES (ANEES) ze seznamů trajektorií.\n",
    "    \"\"\"\n",
    "    num_runs = len(x_true_list)\n",
    "    if num_runs == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    total_nees = 0.0\n",
    "    for i in range(num_runs):\n",
    "        x_true = x_true_list[i]\n",
    "        x_hat = x_hat_list[i]\n",
    "        P_hat = P_hat_list[i]\n",
    "        \n",
    "        seq_len = x_true.shape[0]\n",
    "        state_dim = x_true.shape[1]\n",
    "        nees_samples_run = torch.zeros(seq_len)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            error = x_true[t] - x_hat[t]\n",
    "            P_t = P_hat[t]\n",
    "            \n",
    "            P_t_matrix = P_t.reshape(state_dim, state_dim)\n",
    "                \n",
    "            P_stable = P_t_matrix + torch.eye(state_dim, device=P_t_matrix.device) * 1e-9\n",
    "            \n",
    "            try:\n",
    "                P_inv = torch.inverse(P_stable)\n",
    "                nees_samples_run[t] = error.unsqueeze(0) @ P_inv @ error.unsqueeze(-1)\n",
    "            except torch.linalg.LinAlgError:\n",
    "                nees_samples_run[t] = float('nan')\n",
    "            \n",
    "        total_nees += torch.nanmean(nees_samples_run).item()\n",
    "        \n",
    "    return total_nees / num_runs\n",
    "\n",
    "# --- 2. Příprava a generování dat ---\n",
    "\n",
    "TEST_SEQ_LEN = 200\n",
    "NUM_TEST_TRAJ = 10\n",
    "J_SAMPLES_TEST = 20\n",
    "\n",
    "print(f\"Generuji {NUM_TEST_TRAJ} testovacích trajektorií...\")\n",
    "x_test, y_test = utils.generate_data(sys_true, num_trajectories=NUM_TEST_TRAJ, seq_len=TEST_SEQ_LEN)\n",
    "\n",
    "# --- 3. Vyhodnocovací smyčka ---\n",
    "\n",
    "state_knet.eval()\n",
    "\n",
    "all_x_true_cpu, all_x_hat_knet_cpu, all_P_hat_knet_cpu = [], [], []\n",
    "all_x_hat_ekf_cpu, all_P_hat_ekf_cpu = [], []\n",
    "\n",
    "print(f\"Vyhodnocuji na {NUM_TEST_TRAJ} testovacích trajektoriích...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_TEST_TRAJ):\n",
    "        # Data pro jednu trajektorii, zůstávají na CPU\n",
    "        y_test_seq_cpu = y_test[i]\n",
    "        x_true_seq_cpu = x_test[i]\n",
    "        \n",
    "        all_x_true_cpu.append(x_true_seq_cpu)\n",
    "        \n",
    "        # --- Vyhodnocení vaší StateBayesianKalmanNet ---\n",
    "        \n",
    "        # Přesun dat na GPU těsně před použitím\n",
    "        y_test_seq_gpu = y_test_seq_cpu.to(device)\n",
    "        x_true_seq_gpu = x_true_seq_cpu.to(device)\n",
    "        \n",
    "        initial_state_gpu = x_true_seq_gpu[0, :].unsqueeze(0)\n",
    "        state_knet.reset(batch_size=1, num_samples=J_SAMPLES_TEST, initial_state=initial_state_gpu)\n",
    "        \n",
    "        knet_predictions_gpu = []\n",
    "        knet_covariances_gpu = []\n",
    "        \n",
    "        for t in range(1, TEST_SEQ_LEN):\n",
    "            y_t_gpu = y_test_seq_gpu[t, :].unsqueeze(0)\n",
    "            x_ensemble_t, _ = state_knet.step(y_t_gpu, num_samples=J_SAMPLES_TEST)\n",
    "            x_filtered_t = x_ensemble_t.mean(dim=0)\n",
    "            diff = x_ensemble_t - x_filtered_t\n",
    "            P_filtered_t = (diff.unsqueeze(-1) * diff.unsqueeze(-2)).mean(dim=0)\n",
    "            knet_predictions_gpu.append(x_filtered_t)\n",
    "            knet_covariances_gpu.append(P_filtered_t)\n",
    "            \n",
    "        x_hat_knet_seq_gpu = torch.cat(knet_predictions_gpu, dim=0)\n",
    "        P_hat_knet_seq_gpu = torch.cat(knet_covariances_gpu, dim=0)\n",
    "        \n",
    "        x_hat_knet_full = torch.cat([initial_state_gpu, x_hat_knet_seq_gpu], dim=0)\n",
    "        P0_knet_gpu = torch.eye(state_knet.state_dim, device=device).unsqueeze(0) * 1e-6\n",
    "        P_hat_knet_full = torch.cat([P0_knet_gpu, P_hat_knet_seq_gpu], dim=0)\n",
    "        \n",
    "        all_x_hat_knet_cpu.append(x_hat_knet_full.cpu())\n",
    "        all_P_hat_knet_cpu.append(P_hat_knet_full.cpu())\n",
    "        \n",
    "        # --- Vyhodnocení ExtendedKalmanFilter ---\n",
    "        ekf_instance = Filters.ExtendedKalmanFilter(sys_model)\n",
    "        \n",
    "        # Nastavíme počáteční stav a kovarianci\n",
    "        ekf_instance.Ex0 = x_true_seq_cpu[0, :].unsqueeze(-1).to(ekf_instance.device)\n",
    "        ekf_instance.P0 = torch.eye(sys_model.state_dim, device=ekf_instance.device) * 1e-6\n",
    "        \n",
    "        # EKF filtruje celou sekvenci měření (interně si nastaví počáteční stav z Ex0)\n",
    "        y_test_seq_for_ekf = y_test_seq_cpu.to(ekf_instance.device)\n",
    "        ekf_results = ekf_instance.apply_filter(y_test_seq_for_ekf)\n",
    "        \n",
    "        x_hat_ekf_cpu = ekf_results['x_filtered'].cpu()\n",
    "        P_hat_ekf_cpu = ekf_results['P_filtered'].cpu()\n",
    "\n",
    "        all_x_hat_ekf_cpu.append(x_hat_ekf_cpu)\n",
    "        all_P_hat_ekf_cpu.append(P_hat_ekf_cpu)\n",
    "        \n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(f\"Dokončeno {i + 1}/{NUM_TEST_TRAJ} běhů...\")\n",
    "\n",
    "# --- 4. Finální výpočet průměrných metrik ---\n",
    "x_true_all = torch.cat(all_x_true_cpu)\n",
    "x_hat_knet_all = torch.cat(all_x_hat_knet_cpu)\n",
    "x_hat_ekf_all = torch.cat(all_x_hat_ekf_cpu)\n",
    "\n",
    "avg_mse_knet = F.mse_loss(x_hat_knet_all, x_true_all).item()\n",
    "avg_mse_ekf = F.mse_loss(x_hat_ekf_all, x_true_all).item()\n",
    "\n",
    "anees_knet = calculate_anees(all_x_true_cpu, all_x_hat_knet_cpu, all_P_hat_knet_cpu)\n",
    "anees_ekf = calculate_anees(all_x_true_cpu, all_x_hat_ekf_cpu, all_P_hat_ekf_cpu)\n",
    "\n",
    "\n",
    "# --- 5. Finální výpis ---\n",
    "# Definujeme `state_dim_for_nees` bezpečně\n",
    "state_dim_for_nees = all_x_true_cpu[0].shape[1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FINÁLNÍ VÝSLEDKY (průměr přes {NUM_TEST_TRAJ} běhů)\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n--- Průměrná MSE (přesnost) ---\")\n",
    "print(f\"EKF:              {avg_mse_ekf:.4f}\")\n",
    "print(f\"Bayesian KNet:    {avg_mse_knet:.4f}\")\n",
    "print(\"\\n--- Průměrný ANEES (kredibilita/kalibrace) ---\")\n",
    "print(f\"Očekávaná hodnota: {state_dim_for_nees:.4f}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"EKF:              {anees_ekf:.4f}\")\n",
    "print(f\"Bayesian KNet:    {anees_knet:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# Předpoklady:\n",
    "# Následující proměnné a moduly musí být definovány a naimportovány PŘED \n",
    "# spuštěním tohoto skriptu:\n",
    "#\n",
    "# import utils\n",
    "# from StateBayesianKalmanNet import StateBayesianKalmanNet\n",
    "#\n",
    "# device = torch.device(...)\n",
    "# state_knet = ... (instance vaší natrénované třídy)\n",
    "# sys_true = ...\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# --- 1. Definice pomocných funkcí a metrik ---\n",
    "\n",
    "def calculate_nees(x_true, x_hat, P_hat):\n",
    "    \"\"\"\n",
    "    Vypočítá Normalized Estimation Error Squared (NEES) pro jednu trajektorii.\n",
    "    \"\"\"\n",
    "    seq_len = x_true.shape[0]\n",
    "    state_dim = x_true.shape[1]\n",
    "    nees_samples = torch.zeros(seq_len)\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        error = x_true[t] - x_hat[t]\n",
    "        P_t = P_hat[t]\n",
    "            \n",
    "        P_t_matrix = P_t.reshape(state_dim, state_dim)\n",
    "        P_stable = P_t_matrix + torch.eye(state_dim, device=P_t_matrix.device) * 1e-9\n",
    "        \n",
    "        try:\n",
    "            P_inv = torch.inverse(P_stable)\n",
    "            nees_samples[t] = error.unsqueeze(0) @ P_inv @ error.unsqueeze(-1)\n",
    "        except torch.linalg.LinAlgError:\n",
    "            nees_samples[t] = float('nan')\n",
    "            \n",
    "    avg_nees = torch.nanmean(nees_samples).item()\n",
    "    return avg_nees\n",
    "\n",
    "# --- 2. Příprava ---\n",
    "\n",
    "TEST_SEQ_LEN = 200\n",
    "J_SAMPLES_TEST = 20\n",
    "\n",
    "print(\"Připravuji data pro online simulaci...\")\n",
    "x_online_test, y_online_test = utils.generate_data(sys_true, num_trajectories=1, seq_len=TEST_SEQ_LEN)\n",
    "\n",
    "y_online_seq_gpu = y_online_test.squeeze(0).to(device)\n",
    "x_true_seq_cpu = x_online_test.squeeze(0).cpu()\n",
    "\n",
    "# --- 3. Simulace online filtrace ---\n",
    "\n",
    "print(\"Zahajuji simulaci online filtrace...\")\n",
    "\n",
    "state_knet.eval()\n",
    "\n",
    "# Počáteční stav pro oba filtry\n",
    "initial_state_gpu = x_true_seq_cpu[0, :].unsqueeze(0).to(device)\n",
    "state_knet.reset(batch_size=1, num_samples=J_SAMPLES_TEST, initial_state=initial_state_gpu)\n",
    "\n",
    "online_predictions = []\n",
    "online_covariances = []\n",
    "step_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(1, TEST_SEQ_LEN): # Začínáme od 1, protože t=0 je daný\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        y_t_gpu = y_online_seq_gpu[t, :].unsqueeze(0)\n",
    "        \n",
    "        # --- ZMĚNA ZDE ---\n",
    "        # 1. Získáme celý ansámbl odhadů z `.step()` metody\n",
    "        x_ensemble_t, _ = state_knet.step(y_t_gpu, num_samples=J_SAMPLES_TEST)\n",
    "        \n",
    "        # 2. Vypočítáme průměr (odhad stavu) a kovarianční matici\n",
    "        x_filtered_t = x_ensemble_t.mean(dim=0)\n",
    "        diff = x_ensemble_t - x_filtered_t\n",
    "        P_filtered_t = (diff.unsqueeze(-1) * diff.unsqueeze(-2)).mean(dim=0)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        step_times.append(end_time - start_time)\n",
    "        \n",
    "        online_predictions.append(x_filtered_t.cpu())\n",
    "        online_covariances.append(P_filtered_t.cpu())\n",
    "\n",
    "print(\"Online simulace dokončena.\")\n",
    "\n",
    "# --- 4. Zpracování a vyhodnocení výsledků ---\n",
    "\n",
    "# Spojíme výsledky a odstraníme přebytečnou `batch_size=1` dimenzi\n",
    "x_hat_online_seq = torch.cat(online_predictions, dim=0)\n",
    "P_hat_online_seq = torch.cat(online_covariances, dim=0)\n",
    "\n",
    "# Přidáme počáteční stav, aby se srovnávaly stejně dlouhé sekvence\n",
    "x_hat_online = torch.cat([x_true_seq_cpu[0:1], x_hat_online_seq], dim=0)\n",
    "P0_online = torch.eye(state_knet.state_dim).unsqueeze(0) * 1e-6\n",
    "P_hat_online = torch.cat([P0_online, P_hat_online_seq], dim=0)\n",
    "\n",
    "mse_online = F.mse_loss(x_hat_online, x_true_seq_cpu)\n",
    "avg_step_time_ms = np.mean(step_times) * 1000\n",
    "\n",
    "nees_online = calculate_nees(x_true_seq_cpu, x_hat_online, P_hat_online)\n",
    "\n",
    "print(\"\\n--- Výsledky online simulace ---\")\n",
    "print(f\"  Celková MSE: {mse_online.item():.4f}\")\n",
    "print(f\"  Průměrný NEES: {nees_online:.4f}\")\n",
    "print(f\"  Průměrný čas na jeden krok filtrace: {avg_step_time_ms:.4f} ms\")\n",
    "\n",
    "\n",
    "# --- 5. Vizualizace ---\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.title(\"Simulace online predikce pomocí stavového Bayesian KalmanNetu\", fontsize=16)\n",
    "\n",
    "# Extrakce dat pro plotování\n",
    "x_true_plot = x_true_seq_cpu.numpy().squeeze()\n",
    "y_meas_plot = y_online_seq_gpu.cpu().numpy().squeeze() # Přesunuto z GPU pro plot\n",
    "bkn_mean = x_hat_online.numpy().squeeze()\n",
    "bkn_std = torch.sqrt(torch.diagonal(P_hat_online, dim1=-2, dim2=-1)).numpy().squeeze()\n",
    "time_steps = range(len(x_true_plot))\n",
    "\n",
    "plt.plot(time_steps, x_true_plot, 'k-', linewidth=3, label=\"Skutečný stav (Ground Truth)\")\n",
    "plt.plot(time_steps, y_meas_plot, 'r.', markersize=6, alpha=0.6, label=\"Přicházející měření\")\n",
    "plt.plot(time_steps, bkn_mean, 'c--', linewidth=2.5, label=f\"Online odhad BKN (MSE={mse_online.item():.4f})\")\n",
    "plt.fill_between(time_steps, bkn_mean - 2 * bkn_std, bkn_mean + 2 * bkn_std, \n",
    "                 color='cyan', alpha=0.2, label='Odhadovaná nejistota BKN (95%)')\n",
    "\n",
    "plt.xlabel(\"Časový krok (t)\", fontsize=12)\n",
    "plt.ylabel(\"Hodnota\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
